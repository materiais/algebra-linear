\documentclass[12pt,a4paper]{article}
\usepackage{cmap} % Makes the PDF copiable. See http://tex.stackexchange.com/a/64198/25761
\usepackage[T1]{fontenc}
\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{textcomp} % \degree
\usepackage{gensymb} % \degree
\usepackage[usenames,svgnames,dvipsnames]{xcolor} % colors
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[top=2cm, bottom=2cm, left=2cm, right=2cm]{geometry}
\usepackage{systeme}

\newcommand{\fixme}{{\color{red}(...)}}
\newcommand*\sen{\operatorname{sen}}
\newcommand*\senh{\operatorname{senh}}
\newcommand\ii{\mathrm{i}} 

\newcommand*\Z{\mathbb{Z}}
\newcommand*\Q{\mathbb{Q}}
\newcommand*\R{\mathbb{R}}
\newcommand*\C{\mathbb{C}}

\newcommand{\IconPc}{\includegraphics[width=1em]{computer.png}}
\newcommand{\IconCalc}{\includegraphics[width=1em]{calculator.png}}
\newcommand{\IconThink}{\includegraphics[width=1em]{pencil.png}}
\newcommand{\IconCheck}{\includegraphics[width=1em]{checkmark.png}}
\newcommand{\IconConcept}{\includegraphics[width=1em]{edit.png}}

\newlength{\SmileysLength}
\setlength{\SmileysLength}{\labelwidth}\addtolength{\SmileysLength}{\labelsep}

\newcommand{\calc}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconCalc}%
   \hspace*{\SmileysLength}}
\newcommand{\software}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconPc}%
   \hspace*{\SmileysLength}}
\newcommand{\teoria}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconThink}%
   \hspace*{\SmileysLength}}
\newcommand{\conceito}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconCheck}%
   \hspace*{\SmileysLength}}
\newcommand{\concept}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconCheck}%
   \hspace*{\SmileysLength}}

% Loop Space / CC BY-SA-3.0 / https://tex.stackexchange.com/a/2238/25761
\newenvironment{amatrix}[1]{%
  \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right]
}

% Loop Space / CC BY-SA-3.0 / https://tex.stackexchange.com/a/3164/25761
%--------grstep
% For denoting a Gauss' reduction step.
% Use as: \grstep{\rho_1+\rho_3} or \grstep[2\rho_5 \\ 3\rho_6]{\rho_1+\rho_3}
\newcommand{\grstep}[2][\relax]{%
   \ensuremath{\mathrel{
       {\mathop{\longrightarrow}\limits^{#2\mathstrut}_{
                                     \begin{subarray}{l} #1 \end{subarray}}}}}}
\newcommand{\swap}{\leftrightarrow}

\author{Helder Geovane Gomes de Lima}
\title{1ª Lista de exercícios - ALI0001/ALG2001}
\date{}

\begin{document}
\thispagestyle{empty}

\begin{center}
\includegraphics[width=9.0cm]{marca.jpg}
\noindent\begin{tabular}{l c c r}
  \textbf{1ª Lista de Exercícios} & \textbf{(ALI0001/ALG2001)}\\
\end{tabular}
\\ Prof. Helder Geovane Gomes de Lima
\end{center}

\section*{Legenda}
\begin{multicols}{4}
\begin{itemize}
\item[] \hspace*{\SmileysLength} \calc \hspace*{-\SmileysLength} Cálculos
\item[] \hspace*{\SmileysLength} \conceito \hspace*{-\SmileysLength} Conceitos
\item[] \hspace*{\SmileysLength} \teoria \hspace*{-\SmileysLength} Teoria
\item[] \hspace*{\SmileysLength} \software \hspace*{-\SmileysLength} Software
\end{itemize}
\end{multicols}

\section*{Questões}
\begin{enumerate}

\item \conceito Considere $K=\{ P, I \}$, isto é, o conjunto formado pelas iniciais das palavras ``par'' e ``ímpar''. Defina neste conjunto as operações $\oplus$ e $\otimes$, inspiradas pelo que ocorre com a adição e a multiplicação de números pares e ímpares (por exemplo, a adição de dois números ímpares é sempre um número par, então $I\oplus I=P$). As tabelas a seguir mostram os resultados de cada operação:
\begin{multicols}{2}
\begin{tabular}{|c||c|c|}
\hline
$\oplus$ & $P$ & $I$ \\
\hline\hline
$P$ & $P$ & $I$ \\
\hline
$I$ & $I$ & $P$ \\
\hline
\end{tabular}

\begin{tabular}{|c||c|c|}
\hline
$\otimes$ & $P$ & $I$ \\
\hline\hline
$P$ & $P$ & $P$ \\
\hline
$I$ & $P$ & $I$ \\
\hline
\end{tabular}
\end{multicols}
Verifique se este conjunto $K$, com as operações definidas, é um \textit{corpo}, ou seja:
\begin{enumerate}
\item Ambas as operações são comutativas?
\item Ambas as operações são associativas?
\item Existe um elemento neutro para cada operação?
\item Todo elemento tem um oposto para a adição? E os elementos não nulos, têm um oposto/inverso para a multiplicação?
\item Vale a distributividade da multiplicação em relação à adição?
\end{enumerate}

\item \concept Justifique por que os conjuntos a seguir não são corpos, indicando (pelo menos) uma propriedade que não é válida:
\begin{enumerate}
\item O conjunto das matrizes $2 \times 2$, com as operações usuais de adição e multiplicação
\item O conjunto dos números inteiros pares, com as operações usuais.
\item Os polinômios de grau menor ou igual a 2, isto é, da forma $p(x) = ax^2+bx+c$, com $a,b,c \in \R$.
\end{enumerate}

\item \teoria Seja $K$ um corpo e $a,b \in K$. Use uma ou mais das 9 propriedades de $K$ para justificar por que a seguinte afirmação é verdadeira: ``se $a \cdot b = 0$ então $a=0$ ou $b=0$''.

\item \conceito Dê um contra-exemplo para a afirmação do item anterior no caso em que o corpo $K$ é trocado pelo conjunto $M_{2 \times 2} (\R)$ das matrizes $2 \times 2$ cujas entradas são números reais, e ``$0$'' representa a matriz nula, em que todas as entradas são iguais a zero. Qual das propriedades utilizadas não é válida neste conjunto?

\item \conceito Seja $\Q(\sqrt{2})$ o conjunto $\{ a + b\sqrt{2} \mid a,b \in \Q \}$ e defina as operações $\heartsuit$ e $\bigstar$ em $\Q(\sqrt{2})$ por:
\[
\begin{cases}
(a+b\sqrt{2}) \heartsuit (c+d\sqrt{2}) = (a+c) + (b+d)\sqrt{2} \in \Q(\sqrt{2})\\
(a+b\sqrt{2}) \bigstar (c+d\sqrt{2}) = (ac+2bd) + (ad+bc)\sqrt{2} \in \Q(\sqrt{2})
\end{cases}
\]
\begin{enumerate}
\item Que elemento de $\Q(\sqrt{2})$ é neutro para a operação $\heartsuit$?
\item Determine $c,d \in \Q$ tais que $(3+5\sqrt{2}) \bigstar (c+d\sqrt{2}) = 3 + 5\sqrt{2}$.
\item Se $3+5\sqrt{2}$ for trocado por algum outro $a+b\sqrt{2} \in \Q(\sqrt{2})$, os valores $c$ e $d$ mudam? O que isso diz a respeito da operação $\bigstar$ e do elemento $c+d\sqrt{2}$ encontrado acima?
\item Que elemento de $\Q(\sqrt{2})$ é o oposto de $a+b\sqrt{2}$ em relação a $\heartsuit$? E em relação a $\bigstar$?
\end{enumerate}

\item \software Considere o sistema de equações lineares
\[
\systeme*{
2x - 3y = -4,
5x +  y = 7
}
\]
e utilize o GeoGebra\footnote{\url{https://www.geogebra.org/download/}} para:
\begin{enumerate}
\item Plotar o conjunto $A$ formado pelos pontos $(x, y)$ cujas coordenadas satisfazem a primeira equação e o conjunto $B$ dos que verificam a segunda equação.

\textbf{Dica}: Não é preciso um comando especial para representar equações polinomiais no GeoGebra. Basta digitá-las diretamente (mesmo se forem como \texttt{5xy\^{}2+2y\^{}3x\^{}2=1}).
\item Alterar algumas vezes os números do segundo membro, e perceber o tipo de mudança que ocorre na representação gráfica de $A$ e $B$.
\item Verificar se com alguma escolha de valores os conjuntos se intersectam. Parece ser possível que isso não aconteça dependendo dos valores escolhidos?

\textbf{Dica}: O comando \texttt{Interseção[p, q]} gera a interseção dos objetos \texttt{p} e \texttt{q}.
\end{enumerate}

\item \software Repita o exercício anterior para o seguinte sistema, em uma janela de visualização 3D:
\[
\systeme*{
 x - y + z = 1,
2x + y + z = 4,
 x + y + 5z= 7
}
\]
\item Considere os seguintes sistemas lineares nas variáveis $x,y \in \R$:
\begin{multicols}{2}
\begin{itemize}
\item[] \begin{equation}
\begin{cases}
 x + 2y &= 6 \\
 2x - c y &= 0
\end{cases}\end{equation}

\item[] \begin{equation}
\systeme*{
   x + 2y = 6,
-c x +  y = 1 - 4c
} \end{equation}
\end{itemize}
\end{multicols}
\begin{enumerate}
\item \calc Determinar para quais valores de $c$ os sistemas lineares têm uma, nenhuma ou infinitas soluções. 
\item \software Obtenha as mesmas conclusões sobre $c$ experimentalmente, usando o GeoGebra. \textbf{Dica}: defina por exemplo \texttt{c=10} e use o botão direito do mouse para tornar o número visível como um ``controle deslizante''.
\end{enumerate}

\newpage
\item \calc Utilize o método de eliminação de Gauss–Jordan para (i) obter a forma escalonada reduzida por linhas da matriz de coeficientes de cada um dos sistemas lineares a seguir, e partir dela (ii) determinar as soluções dos sistemas, nos conjuntos indicados:
\begin{enumerate}
\item $\left\{
\begin{aligned}
5s & {}-{} &  5\pi t & {}={} & -5\pi^2\\
-s & {}+{} & (\pi+3)t & {}={} & \pi(\pi+6)
\end{aligned}
\right., \text{ sendo } s,t \in \R$

\item $\systeme{
4x_1+4x_2 = 16,
5x_2 -15x_4=2,
2x_1+2x_2+x_3=12,
-x_2+8x_4=3/5
}, \text{ sendo } x_i \in \Q$

\item $\systeme{
          3x_1-12x_2 -6x_3              +9x_5=-21,
          -x_1 +4x_2 +2x_3              -3x_5=7,
\frac{1}{2}x_1 -2x_2  -x_3+x_4-\frac{3}{2}x_5=-\frac{5}{2},
         -7x_1+28x_2+15x_3             -23x_5=53
}, \text{ sendo } x_i \in \R$

\item $\begin{cases}
 3x        -6y & =      15\ii\\
 4x-(8 + \ii)y & = -1 + 20\ii\\
-2x         -y & =      -5\ii
\end{cases}, \text{ sendo } x, y \in \C$

\item $\systeme{
     b+6c= 6,
  a+6b-5c=-3,
3a+20b-3c= 1
}, \text{ sendo } a, b, c \in \R$
\end{enumerate}
\item \calc Utilize matrizes inversas para resolver os sistemas anteriores, quando for possível.

\item \calc Resolva os seguintes sistemas lineares sobre $\R$, usando matrizes inversas:
\begin{multicols}{3}
\begin{enumerate}
\item $\systeme{
   -y+5z=2,
 x+2y+3z=7,
2x+4y+5z=13
}$
\item $\systeme{
   -v+5w=0,
 u+2v+3w=0,
2u+4v+5w=0
}$
\item $\systeme{
   -q+5r=-2,
 p+2q+3r=3,
2p+4q+5r=1
}$
\end{enumerate}
\end{multicols}

\item \conceito Se $A$ é uma matriz $p \times q$, $B$ uma matriz $q \times r$ e $C$ uma matriz $r \times q$, qual é o tamanho da matriz $M = (B + C^T) ((AB)^T + CA^T)$?

\item \teoria Seja $X \in M_{m \times n}(\R)$. Para que valores de $m$ e $n$ é possível realizar as seguintes operações?
\begin{multicols}{5}
\begin{enumerate}
\item $X X^T$
\item $X^T X$
\item $X + X^T$
\item $X^T + X$
\item $X - X^T$
\end{enumerate}
\end{multicols}
Quais os tamanhos destas matrizes? Quais delas são simétricas? Justifique.

\item \teoria Seja $M = \begin{bmatrix}a & b \\ c & d\end{bmatrix} \in M_{2 \times 2} (\R)$ a matriz associada a um sistema linear homogêneo. Utilize a eliminação de Gauss-Jordan para provar que se $ad-bc \neq 0$ então o sistema possui somente a solução trivial.

\newpage
\item \concept Dar exemplos de matrizes $A, B, C \in M_{2 \times 2} (\Q)$ tais que:
\begin{enumerate}
\item $(A+B)^2 \neq A^2 + 2AB + B^2$
\item $(A+B)^2 = A^2 + 2AB + B^2$
\item $AB =AC$, mas $B \neq C$ e $A \neq 0$
\item $BA =CA$, mas $B \neq C$ e $A \neq 0$
\item $C^2=0$ mas $C \neq 0$
\end{enumerate}

\item \calc Calcule o produto $C = A \times B$ das seguintes matrizes (utilize mais de um método):
\begin{enumerate}
\item $A = 
\begin{bmatrix}
  1 & 1/2 \\
  3 & 0 \\
7/3 & -1
\end{bmatrix}
\in M_{3 \times 2} (\Q)$
e
$B = 
\begin{bmatrix}
3 & 0 \\
2 & 4
\end{bmatrix}
\in M_{2 \times 2} (\Q)$
\item $A = 
\begin{bmatrix}
1+i & 1-i \\
 2i & 3+2i
\end{bmatrix}
\in M_{2 \times 2} (\C)$
e
$B = 
\begin{bmatrix}
3 & 0 & -i \\
 2i & 4 & 5
\end{bmatrix}
\in M_{2 \times 3} (\C)$,
sendo $i^2 = -1$
\item $A = 
\begin{bmatrix}
\pi & 0 & 0 \\
  2 & 3 & 0 \\
  1 & -2 & 5
\end{bmatrix}
\in M_{3 \times 3} (\R)$
e
$B = 
\begin{bmatrix}
\pi & 2 & 1 \\
 0 & 3 & -2 \\
 0 & 0 & 5
\end{bmatrix}
\in M_{3 \times 3} (\R)$
\item $A = 
\begin{bmatrix}
       -1 & 1 \\
        3 & 2 \\
 \sqrt{2} & -1 \\
 \sqrt{3} & -1 
\end{bmatrix}
\in M_{4 \times 2} (\R)$
e
$B = 
\begin{bmatrix}
        1 & \sqrt{2} & 0\\
 \sqrt{3} &        0 & 2
\end{bmatrix}
\in M_{2 \times 3} (\R)$
\item $A = 
\begin{bmatrix}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 
\end{bmatrix}
\in M_{4 \times 4} (\Q)$
e
$B = 
\begin{bmatrix}
 1 & 2 & 3 & 4 \\
 5 & 6 & 7 & 8 \\
 9 & 10 & 11 & 12 \\
13 & 14 & 15 & 16 
\end{bmatrix}
\in M_{4 \times 4} (\Q)$
\end{enumerate}

\item \calc Sejam $U,I,J,K \in M_{2 \times 2} (\C)$ as matrizes complexas definidas por:
\[
U = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix},
I = \begin{bmatrix} i & 0 \\ 0 & -i \end{bmatrix},
J = \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix},
K = \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix}.
\]
\begin{enumerate}
\item Mostre que $I^2 = J^2 = K^2 = I J K = -U$.
\item Mostre que \begin{align*}
IJ = -JI = K, \\
JK = -KJ = I, \\
KI = -IK = J.
\end{align*}
\item Compare o item anterior com as propriedades do produto vetorial em $\R^3$, para os vetores da base canônica $\vec{i} = (1,0,0)$, $\vec{j} = (0,1,0)$, $\vec{k} = (0,0,1)$.
\item Determine as matrizes inversas de $U$, $I$, $J$ e $K$.
\end{enumerate}

\item \calc Em relação à multiplicação de matrizes quadradas $A$ e $B$:
\begin{enumerate}
\item Quantas multiplicações e adições (de números) você usa ao calcular $AB$ se ambas forem de ordem $2 \times 2$? E se forem de ordem $3 \times 3$? E caso sejam $n \times n$?
\item Pesquise (em livros, ou na internet) se existem métodos capazes de multiplicar matrizes fazendo \emph{menos} multiplicações (e, em compensação, \emph{mais} adições/subtrações)?
\end{enumerate}

\item Em um software de computação numérica (GNU Octave\footnote{\url{https://www.gnu.org/software/octave/download.html}}, o Scilab\footnote{\url{http://www.scilab.org/download/latest}}, MatLab, etc):
\begin{enumerate}
\item \software Sortear ao acaso 10 matrizes de ordem $7\times 7$ e verificar quantas delas são inversíveis. \textbf{Dica}: o comando \texttt{rand(m,n)} gera aleatoriamente uma matriz de ordem $m \times n$, e o comando \texttt{det(A)} calcula o determinante da matriz $A$.
\item \software Repetir o experimento anterior com matrizes quadradas de algum outro tamanho. O que ocorre com a maioria das matrizes em cada uma das dimensões consideradas?
\item Escolher matrizes triangulares superiores $A_2$, $A_3$ e $A_4$ de ordens $2 \times 2$, $3 \times 3$ e $4 \times 4$ respectivamente, todas com zeros na diagonal e então:
\begin{enumerate}
\item \software Calcular as potências $A_2^2$, $A_3^3$ e $A_4^4$.
\item \teoria Com base nos resultados obtidos, formule uma conjectura a respeito da $n$-ésima potência das matrizes triangulares superiores $n\times n$, com zeros na diagonal.
\item \teoria Prove que o seu palpite é realmente válido para \textbf{qualquer} matriz nas condições acima (pelo menos nos casos $2 \times 2$ e $3 \times 3$). 
\end{enumerate}
\end{enumerate}

\item \calc Suponha que $M = \begin{bmatrix} -1 & 2 & 3 \\ 2 &-4 & 5 \\ -1 &1 &7\end{bmatrix}$ e $X = \begin{bmatrix} a & b & c \\ d & e& f \\ g &h &i\end{bmatrix} \in M_{3 \times 3}(\R)$ são tais que $M X = I_{3 \times 3}$. Determine $X$, por meio da comparação das entradas de $MX$ e $I$, e depois calcule $XM$.

\item \calc Para que valores de $t \in \R$ a matriz $T = \begin{bmatrix} -1 & 9 & 1 \\ -1 &t & 3 \\ -1 & 9 &t+1\end{bmatrix}$ é inversível?

\item \calc Existe algum $t \in \R$ para o qual $N = \begin{bmatrix} 2-t & 0 & -4 \\ 6 & 1-t & -15 \\ 2 & 0 & -4-t\end{bmatrix} \in M_{3\times 3}(\R)$ não é inversível?

\item \conceito Dê um contra-exemplo para as afirmações falsas, e justifique de forma convincente (prove!) as que forem verdadeiras:
\begin{enumerate}
\item A matriz nula é uma matriz na forma escalonada reduzida por linhas.
\item A matriz identidade $4 \times 4$ está na forma escalonada reduzida por linhas.
\item Toda matriz escalonada por linhas está na forma escalonada reduzida.
\item Se uma matriz triangular superior é simétrica então ela é uma matriz diagonal.
\item Se $U$ e $V$ são matrizes diagonais, então $UV = VU$.
\item Se $A$ é uma matriz antissimétrica, isto é, se $A^T = -A$, então $A^T$ é antissimétrica.
\item Se $A \in M_{n \times n}(\R)$ é uma matriz antissimétrica, então sua diagonal é igual a zero.
\item Nenhuma matriz $A \in M_{n \times n}(\R)$ pode ser simétrica e antissimétrica simultaneamente.
\item Para toda matriz quadrada $Q$, a matriz $\frac{1}{2}(Q-Q^T)$ é antissimétrica.
\item Seja $T \in M_{n\times n}(\R)$ e $x \in M_{n\times 1}(\R)$. O sistema linear $Tx = 2x$ tem uma única solução se, e somente se, $T-2I$ for uma matriz inversível.
\end{enumerate}

\item \calc Quantas matrizes diagonais $D \in M_{2 \times 2}(\R)$ satisfazem $D^2 = I$, isto é, quantas matrizes diagonais são ``raizes quadradas'' da matriz identidade de ordem 2? E se $D \in M_{3 \times 3}(\R)$?
\item \calc Encontre todas as matrizes diagonais $D \in M_{3 \times 3}(\R)$ tais que $D^2 - 7D + 10I = 0$.

\newpage


\item \teoria Mostre que se $S$ é uma matriz simétrica então $S^2$ também é simétrica. Vale o mesmo para $S^n$, qualquer que seja $n \in \mathbb{N}$? Explique.
\item \teoria Se $M$ é uma matriz quadrada $n \times n$, a soma das entradas da diagonal de $M$ é chamada de \textbf{traço} de $M$, e denotada por $tr(M) = m_{11} + m_{22} + \ldots + m_{nn}$. Mostre que:
\begin{enumerate}
\item $tr(A+B) = tr(A)+tr(B)$
\item $tr(c \cdot A) = c \cdot tr(A)$, para todo escalar $c$
\item $tr(A^T) = tr(A)$
\end{enumerate}
\item \conceito Dê exemplos de matrizes $A$ e $B$ tais que
\begin{enumerate}
\item $A + B$ seja inversível, mas $A$ e $B$ não sejam
\item $A$ e $B$ sejam inversíveis, mas $A+B$ não seja
\item $A$, $B$ e $A+B$ sejam inversíveis
\end{enumerate}
\item \calc Determine se existe alguma função polinômial de grau três $p(x) = ax^3+bx^2+cx+d$, com $a,b,c,d \in \R$, cujo gráfico passa pelos pontos $(-1,0)$, $(0,2)$, $(1,0)$ e $(2,6)$.

\item \calc Seja $\theta \in \R$. Determine a matriz inversa de $R = \begin{bmatrix}
1 & 0 & 0 \\
0 & \cos(\theta) & \sen(\theta) \\
0 & -\sen(\theta) & \cos(\theta)
\end{bmatrix}$.
\item \calc Seja $x \in \R$. Determine a matriz inversa da matriz
\[
H = \begin{bmatrix}
\frac{1}{2}(e^x+e^{-x}) & \frac{1}{2}(e^x-e^{-x}) \\
\frac{1}{2}(e^x-e^{-x}) & \frac{1}{2}(e^x+e^{-x})
\end{bmatrix}.
\]
\item \calc Determine se a matriz $C = \begin{bmatrix}
1 & 2\\
0 & -1\\
2 & 0
\end{bmatrix}$ possui:
\begin{enumerate}
\item Alguma inversa à esquerda ($XC = I$)
\item Alguma inversa à direita ($CX = I$)
\end{enumerate}

\end{enumerate}

\newpage
\section*{Respostas}
\begin{enumerate}
\item
\begin{enumerate}
\item Sim, pois valem as igualdades
\[ I \oplus P = I = P \oplus I,\]
\[I \otimes P = P = P \otimes I.\]

\item Sim, pois ao considerar todas as possíveis combinações de valores para $a \oplus (b \oplus c)$ e $(a \oplus b) \oplus c$, com $a,b,c \in K$, resulta o seguinte:
\begin{enumerate}
\item $P \oplus (P \oplus P) = P \oplus P = (P \oplus P) \oplus P$
\item $P \oplus (P \oplus I) = P \oplus I = (P \oplus P) \oplus I$
\item $P \oplus (I \oplus P) = P \oplus I = I \oplus P = (P \oplus I) \oplus P$
\item $P \oplus (I \oplus I) = P \oplus P = P = I \oplus I = (P \oplus I) \oplus I$
\item $I \oplus (P \oplus P) = I \oplus P = (I \oplus P) \oplus P$
\item $I \oplus (P \oplus I) = I \oplus I = (I \oplus P) \oplus I$
\item $I \oplus (I \oplus P) = I \oplus I = P = P \oplus P = (I \oplus I) \oplus P$
\item $I \oplus (I \oplus I) = I \oplus P = P \oplus I = (I \oplus I) \oplus I$
\end{enumerate}
\item Sim. O elemento neutro para $\oplus$ é $P$, pois $P\oplus P = P$ e $P\oplus I = I$. O elemento neutro para $\otimes$ é $I$, pois $I\otimes P = P$ e $I\otimes I = I$.
\item Sim. O oposto de $P$ para $\oplus$ é $P$, pois $P\oplus P = P$ e o oposto de $I$ para $\oplus$ é $I$ pois $I\oplus I = P$. Já o oposto/inverso de $I$ (o único elemento não nulo, isto é, diferente de $P$) para $\otimes$ é $I$, pois $I\otimes I = I$.
\item Sim, pois ao considerar todas as possíveis combinações de valores para $a \otimes (b \oplus c)$ e $(a \otimes b) \oplus (a \otimes c)$, com $a,b,c \in K$, resulta o seguinte:
\begin{enumerate}
\item $P \otimes (P \oplus P) = P \otimes P = (P \otimes P) \oplus (P \otimes P)$
\item $P \otimes (P \oplus I) = P \otimes I = P = P \oplus P = (P \otimes P) \oplus (P \otimes I)$
\item $P \otimes (I \oplus P) = P \otimes I = P = P \oplus P = (P \otimes I) \oplus (P \otimes P)$
\item $P \otimes (I \oplus I) = P \otimes P = P = P \oplus P = (P \otimes I) \oplus (P \otimes I)$
\item $I \otimes (P \oplus P) = I \otimes P = P = P \oplus P = (I \otimes P) \oplus (I \otimes P)$
\item $I \otimes (P \oplus I) = I \otimes I = I = P \oplus I = (I \otimes P) \oplus (I \otimes I)$
\item $I \otimes (I \oplus P) = I \otimes I = I = I \oplus P = (I \otimes I) \oplus (I \otimes P)$
\item $I \otimes (I \oplus I) = I \otimes P = P = I \oplus I = (I \otimes I) \oplus (I \otimes I)$
\end{enumerate}
\end{enumerate}
\item
\begin{enumerate}
\item O produto de matrizes não é comutativo, pois existem matrizes $A$ e $B$ com $AB \neq BA$. Por exemplo, se $A=\begin{bmatrix} 0 & 1 \\ 1 & 0 \end{bmatrix}$ e $B =\begin{bmatrix} 2 & 2 \\ 0 & 0 \end{bmatrix}$ então $AB = \begin{bmatrix} 0 & 0 \\ 2 & 2 \end{bmatrix}$, mas $BA = \begin{bmatrix} 2 & 2 \\ 0 & 0 \end{bmatrix}$.
\item Não há um elemento neutro para a multiplicação (e consequentemente não há inversos multiplicativos). De fato, dados dois números pares $x=2p$ e $y=2q$, com $p,q \in \Z$, tem-se $x \cdot y=x \Leftrightarrow 2p \cdot 2q = 2p \Leftrightarrow 2q = 1$, o que é impossível com $q \in Z$.
\item Nem todo polinômio não nulo tem inverso multiplicativo. Por exemplo, não há um polinômio que ao ser multiplicado por $p(x) = x = 0x^2 + 1x + 0$ resulte no elemento neutro da multiplicação, que é o polinômio constante $q(x) = 1 = 0x^2+0x+1$.
\end{enumerate}

\item \teoria Sejam $a,b \in K$ tais que $a \cdot b = 0$. Há duas possibilidades:
\begin{enumerate}
\item Se $a = 0$, então é verdade que um dos dois elementos é zero.
\item Caso contrário, $a \neq 0$ e existe $a^{-1}$ tal que $a \cdot a^{-1} = 1$, e este elemento pode ser usado para obter as seguintes conclusões:
\begin{align*}
a \cdot b & = 0\\
\Rightarrow
a^{-1} \cdot (a \cdot b) & = a^{-1} \cdot 0\ (\text{pois } a \text{ possui um inverso})\\
\Rightarrow
(a^{-1} \cdot a) \cdot b & = 0\ (\text{pois a multiplicação é associativa})\\
\Rightarrow
1 \cdot b & = 0\ (\text{pela definição de inverso})\\
\Rightarrow
b & = 0\ (\text{pela definição de elemento neutro})
\end{align*}
Assim, deduz-se novamente que um dos dois elementos tem que ser zero.
\end{enumerate}

\item Considere, por exemplo,
$a = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix} \in M_{2 \times 2} (\R)$ e
$b = \begin{bmatrix} 0 & 2 \\ 0 & 0 \end{bmatrix} \in M_{2 \times 2} (\R)$. Então
\[
\begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}
\cdot
\begin{bmatrix} 0 & 2 \\ 0 & 0 \end{bmatrix}
=
\begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}
= 0.
\]
No entanto, nem $a$ nem $b$ são iguais à matriz nula $2 \times 2$. Este exemplo evidencia que não basta que uma matriz seja não nula para que ela possua um inverso multiplicativo.

\item
\begin{enumerate}
\item Para que um certo $n = a + b\sqrt{2} \in \Q(\sqrt{2})$ seja o elemento neutro de $\heartsuit$, é preciso que $n \heartsuit x = x$, para qualquer $x = c+d\sqrt{2} \in \Q(\sqrt{2})$. Mas
\[
n \heartsuit x
= (a+b\sqrt{2}) \heartsuit (c+d\sqrt{2})
= (a+c) + (b+d)\sqrt{2},
\]
então deve ocorrer que
\[
(a+c) + (b+d)\sqrt{2} = c + d\sqrt{2}.
\]
ou seja, $a+c = c$ e $b+d = d$. Como $a,b,c$ e $d$ são números racionais, resulta destas equações que $a=0$ e $b=0$. Assim, elemento neutro deve ser $n = a + b\sqrt{2} = 0 + 0\sqrt{2}$. Isto pode ser verificado diretamente pela definição de $\heartsuit$:
\[
(0+0\sqrt{2}) \heartsuit (c+d\sqrt{2})
= (0+c) + (0+d)\sqrt{2}
= c + d\sqrt{2}.
\]
\item Pela definição de $\bigstar$, tem-se
\[
(3+5\sqrt{2}) \bigstar (c+d\sqrt{2})
= (3c+10d) + (3d+5c)\sqrt{2},
\]
que será igual a $3 + 5\sqrt{2}$ se $c$ e $d$ forem soluções (racionais) do sistema linear
\[
\begin{cases}
3c + 10d & = 3 \\
5c + 3d  & = 5.
\end{cases}
\]
Realizando uma série de operações elementares sobre as linhas da matriz aumentada deste sistema, resulta o seguinte:
\begin{align*}
\begin{amatrix}{2}
3 & 10 & 3 \\
5 & 3 & 5
\end{amatrix}
&
\grstep{ \frac{1}{5} L_1 }
\begin{amatrix}{2}
1 & 10/3 & 1 \\
5 & 3 & 5
\end{amatrix}
\grstep{ L_2 - 5 L_1 }
\begin{amatrix}{2}
1 & 10/3 & 1 \\
0 & -41/3 & 0
\end{amatrix} \\
&
\grstep{ \frac{-3}{41} L_2 }
\begin{amatrix}{2}
1 & 10/3 & 1 \\
0 & 1 & 0
\end{amatrix}
\grstep{ L_1 - \frac{10}{3} L_2 }
\begin{amatrix}{2}
1 & 0 & 1 \\
0 & 1 & 0
\end{amatrix}
\end{align*}
Portanto, $c=1$ e $d=0$, isto é, o elemento procurado é $1 + 0 \sqrt{2}$.
\item Não mudam, pois continua sendo verdade que
\[
  (a+b\sqrt{2}) \bigstar (1+0\sqrt{2})
= (a \cdot 1 +2b \cdot 0) + (a\cdot 0+b\cdot 1)\sqrt{2}
= a + b \sqrt{2}.
\]
Isso significa que $1+0\sqrt{2}$ atua como um elemento neutro para a operação $\bigstar$.

\textbf{Observação:} Os valores de $c$ e $d$ poderiam ser obtidos por meio de um sistema análogo ao anterior, mas com os valores arbitrários $a$ e $b$:
\begin{align*}
\begin{amatrix}{2}
a & 2b & a \\
b & a & b
\end{amatrix}
&
\grstep{ \frac{1}{a} L_1 }
\begin{amatrix}{2}
1 & 2b/a & 1 \\
b & a & b
\end{amatrix}
\grstep{ L_2 - b L_1 }
\begin{amatrix}{2}
1 & 2b/a & 1 \\
0 & (a-2b^2/a) & 0
\end{amatrix} \\
& 
\grstep{ \frac{1}{(a-2b^2/a)} L_2 }
\begin{amatrix}{2}
1 & 2b/a & 1 \\
0 & 1 & 0
\end{amatrix}
\grstep{ L_1 - \frac{2b}{a}L_2 }
\begin{amatrix}{2}
1 & 0 & 1 \\
0 & 1 & 0
\end{amatrix}.
\end{align*}
Na primeira operação elementar, supõe-se que $a \neq 0$ (caso contrário poderia ser feita uma troca de linhas, ou uma verificação direta se $b$ também fosse zero). A terceira operação não requer hipóteses extras, pois $a-2b^2/a$ não tem como ser zero quando $a$ e $b$ são números racionais. De fato, para que isso ocorresse seria preciso que
\[
a^2-2b^2 = 0
\Rightarrow
a^2 = 2b^2\Rightarrow
(a/b)^2 = 2\Rightarrow
a/b = \pm \sqrt{2},
\]
o que é impossível já que $\sqrt{2}$ é irracional.

\item \begin{enumerate}
\item 
Se $x+y\sqrt{2}$ é o oposto de $a+b\sqrt{2}$ em relação a $\heartsuit$ então
\[
(a+b\sqrt{2}) \heartsuit (x+y\sqrt{2})
= (a+x) + (b+y)\sqrt{2}
= 0 + 0\sqrt{2},
\]
ou seja, $a+x = 0$ e $b+y = 0$. Assim, $x = -a$ e $y = -b$ e o oposto de $a+b\sqrt{2}$ é $(-a)+(-b)\sqrt{2}$.

\item Se $a+b\sqrt{2} \neq 0$ e $x+y\sqrt{2}$ for o oposto de $a+b\sqrt{2}$ em relação a $\bigstar$ então
\[
  (a+b\sqrt{2}) \bigstar (x+y\sqrt{2})
= (ax+2by) + (ay+bx)\sqrt{2}
= 1 + 0\sqrt{2}.
\]
Em outras palavras, $x,y$ devem ser soluções (racionais) do sistema linear
\[
\begin{cases}
ax + 2by & = 1 \\
bx +  ay & = 0.
\end{cases}
\]
Como acima,
\begin{align*}
\begin{amatrix}{2} a & 2b & 1 \\
b & a & 0
\end{amatrix}
&
\grstep{ \frac{1}{a} L_1 }
\begin{amatrix}{2}
1 & 2b/a & 1/a \\
b & a & 0
\end{amatrix}
\grstep{ L_2 - b L_1 }
\begin{amatrix}{2}
1 & 2b/a & 1/a \\
0 & \frac{a^2-2b^2}{a} & -b/a
\end{amatrix} \\
& 
\grstep{ \frac{a}{a^2-2b^2} L_2 }
\begin{amatrix}{2}
1 & 2b/a & 1/a \\
0 & 1& \frac{-b}{a^2 -2b^2 }
\end{amatrix}
\grstep{ L_1 - \frac{2b}{a} L_2 }
\begin{amatrix}{2}
1 & 0 & \frac{a}{a^2 -2b^2 } \\
0 & 1 & \frac{-b}{a^2 -2b^2 }
\end{amatrix}
\end{align*}
Logo, $\frac{a}{a^2 -2b^2 } + \frac{-b}{a^2 -2b^2 } \sqrt{2}$ é o inverso de $a+b\sqrt{2}$ em relação a $\bigstar$.
\end{enumerate}
\end{enumerate}

\item 
\begin{enumerate}
\item Digite \texttt{2x-3y=-4} para que o GeoGebra mostre a reta formada pelos pontos que satisfazem a primeira equação, e \texttt{5x+y=7} para representar a segunda reta.
\item Ao trocar o $-4$ por um número maior, a reta correspondente se desloca para baixo, mantendo-se paralela à reta original. Ao diminuir este valor, a reta se desloca paralelamente para cima. Na segunda equação, a troca de $7$ por um número maior resulta em um deslocamento para a direita, e a diminuição deste valor desloca a reta para a esquerda.

\item As retas, que inicialmente se intersectam em $(1,2)$, têm sempre um ponto em comum, independentemente dos valores atribuidos ao segundo membro das equações. Isso reflete o fato de que as duas equações correspondem a retas que não são paralelas entre si, e sua direção permanece inalterada mesmo quando o segundo membro é modificado.
\end{enumerate}


\item 
\begin{enumerate}
\item Digite \texttt{x-y+z=1} para que o GeoGebra mostre o plano formado pelos pontos $(x,y,z)$ que satisfazem a primeira equação, e então \texttt{2x+y+z=4} e \texttt{x+y+5z=7} para representar os planos correspondentes às demais equações.
\item A trocar os valores do segundo membro de cada equação, o plano correspondente desloca-se no espaço mantendo-se paralelo à sua posição original.

\item Como os planos se intersectam inicialmente no ponto $(1,1,1)$, e sempre permanecem paralelos às suas posições iniciais, continua existindo um único ponto de interseção, quaisquer que sejam os valores do segundo membro do sistema.
\end{enumerate}

\item
\begin{enumerate}
\item
\begin{enumerate}
\item A matriz aumentada associada ao primeiro sistema pode ser levada à sua forma escalonada reduzida por linhas por meio das seguintes operações elementares:
\[
\begin{amatrix}{2}
1 & 2 & 6 \\
2 & -c & 0
\end{amatrix}
\grstep{ L_2 - 2 L_1 }
\begin{amatrix}{2}
1 & 2 & 6 \\
0 & -c-4 & -12
\end{amatrix}
\grstep{ \frac{-1}{c+4} L_2 }
\begin{amatrix}{2}
1 & 2 & 6 \\
0 & 1 & \frac{12}{c+4}
\end{amatrix}
\grstep{ L_1 - 2 L_2 }
\begin{amatrix}{2}
1 & 0 & \frac{6c}{c+4} \\
0 & 1 & \frac{12}{c+4}
\end{amatrix}
\]

Se $c = -4$ a segunda operação deixa de ser possível, e o sistema não tem solução. Por outro lado, se $c \neq -4$, todos os passos podem ser realizados e conclui-se que o sistema é possível e determinado, tendo como única solução o ponto $\left(\frac{6c}{c+4}, \frac{12}{c+4}\right)$.
\item A matriz aumentada associada ao segundo sistema pode ser levada à sua forma escalonada reduzida por linhas por meio das seguintes operações elementares:
\begin{align*}
\begin{amatrix}{2}
1 & 2 & 6 \\
-c & 1 & 1-4c
\end{amatrix}
\grstep{ L_2 + c L_1 }
\begin{amatrix}{2}
1 & 2 & 6 \\
0 & 1+2c & 1+2c
\end{amatrix}
&
\grstep{ \frac{1}{1+2c} L_2 }
\begin{amatrix}{2}
1 & 2 & 6 \\
0 & 1 & 1
\end{amatrix}
\grstep{ L_1 - 2 L_2 }
\begin{amatrix}{2}
1 & 0 & 4 \\
0 & 1 & 1
\end{amatrix}
\end{align*}

Desta vez, se $c = -1/2$ a segunda linha zera após a primeira operação elementar, e o sistema tem mais de uma solução. De fato, o escalonamento mostra que o sistema original é equivalente a um sistema formado pela primeira equação e por uma equação do tipo $0=0$, que não impõe qualquer restrição sobre os valores de $x$ e $y$. Assim, todo par da forma $(6-2y, y)$, com $y \in \R$, é solução deste sistema possível e indeterminado.

Por outro lado, nos casos em que $c \neq -1/2$, os três passos da eliminação de Gauss-Jordan podem ser realizados, e a conclusão é de que o sistema possui como única solução o ponto $(4,1)$, sendo então possível e determinado.
\end{enumerate}

\item \begin{enumerate}
\item Geometricamente, nota-se que conforme o valor de $c$ vai se aproximando de $c=-4$ a reta que corresponde à segunda equação gira em torno da origem até ficar paralela à reta da primeira equação. Quando isso ocorre, não há um ponto de interseção. Nos demais casos, as retas se intersectam em um único ponto.
\item Geometricamente, ao variar o valor de $c$, uma das retas gira em torno do ponto $(4,1)$, em que elas se intersectam, e em um caso específico (quando $c=-1/2$) as duas retas coincidem, fazendo com que todos os seus pontos sejam pontos de interseção.
\end{enumerate}
\end{enumerate}

\item \begin{enumerate}
\item A matriz aumentada associada ao sistema dado é $A = \begin{amatrix}{2}
 5 & -5\pi & -5\pi^2 \\
-1 & \pi+3 & \pi(\pi + 6)
\end{amatrix}$
e sua forma escalonada reduzida é obtida por meio das seguintes operações elementares sobre as linhas:
\begin{align*}
A
&
\grstep{ \frac{1}{5} L_1 }
\begin{amatrix}{2}
 1 & -\pi & -\pi^2 \\
-1 & \pi+3 & \pi(\pi + 6)
\end{amatrix}
\grstep{ L_2 + L_1 }
\begin{amatrix}{2}
 1 & -\pi & -\pi^2 \\
 0 &    3 & 6\pi
\end{amatrix} \\
&
\grstep{ \frac{1}{3} L_2 }
\begin{amatrix}{2}
 1 & -\pi & -\pi^2 \\
 0 &    1 & 2\pi
\end{amatrix}
\grstep{ L_1 + \pi L_2 }
\begin{amatrix}{2}
 1 & 0 & \pi^2 \\
 0 & 1 & 2\pi
\end{amatrix}
\end{align*}
Esta última matriz está associada às equações
\[
\left\{
\begin{aligned}
s & = \pi^2 \\
t & = 2\pi
\end{aligned}
\right.,
\]
e, portanto, $S = \{ ( \pi^2, 2\pi ) \}$ é o conjunto das soluções do sistema proposto.

\item A redução à forma escalonada reduzida da matriz associada ao sistema é obtida através das seguintes operações elementares:
\begin{align*}
\begin{amatrix}{4}
4 &  4 & 0 &   0 & 16 \\
0 &  5 & 0 & -15 & 2 \\
2 &  2 & 1 &   0 & 12 \\
0 & -1 & 0 &   8 & 3/5
\end{amatrix}
&
\grstep{ \frac{1}{4} L_1 }
\begin{amatrix}{4}
1 &  1 & 0 &   0 & 4 \\
0 &  5 & 0 & -15 & 2 \\
2 &  2 & 1 &   0 & 12 \\
0 & -1 & 0 &   8 & 3/5
\end{amatrix}
\grstep{ L_3 - 2 L_1 }
\begin{amatrix}{4}
1 &  1 & 0 &   0 & 4 \\
0 &  5 & 0 & -15 & 2 \\
0 &  0 & 1 &   0 & 4 \\
0 & -1 & 0 &   8 & 3/5
\end{amatrix} \\
\grstep{ \frac{1}{5} L_2 }
\begin{amatrix}{4}
1 &  1 & 0 &   0 & 4 \\
0 &  1 & 0 & -3 & 2/5 \\
0 &  0 & 1 &   0 & 4 \\
0 & -1 & 0 &   8 & 3/5
\end{amatrix}
&
\grstep{ L_4 + L_2 }
\begin{amatrix}{4}
1 & 1 & 0 &  0 & 4 \\
0 & 1 & 0 & -3 & 2/5 \\
0 & 0 & 1 &  0 & 4 \\
0 & 0 & 0 &  5 & 1
\end{amatrix}
\grstep{ \frac{1}{5} L_4 }
\begin{amatrix}{4}
1 & 1 & 0 &  0 & 4 \\
0 & 1 & 0 & -3 & 2/5 \\
0 & 0 & 1 &  0 & 4 \\
0 & 0 & 0 &  1 & 1/5
\end{amatrix} \\
\grstep{ L_2 + 3L_4 }
\begin{amatrix}{4}
1 & 1 & 0 & 0 & 4 \\
0 & 1 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 4 \\
0 & 0 & 0 & 1 & 1/5
\end{amatrix}
&
\grstep{ L_1 - L_2 }
\begin{amatrix}{4}
1 & 0 & 0 & 0 & 3 \\
0 & 1 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 4 \\
0 & 0 & 0 & 1 & 1/5
\end{amatrix}
\end{align*}
Esta última matriz está associada às equações
\[
\left\{
\begin{aligned}
x_1 & = 3 \\
x_2 & = 1 \\
x_3 & = 4 \\
x_4 & = 1/5 \\
\end{aligned}
\right.,
\]
de modo que $S = \{ ( 3, 1, 4, 1/5 ) \}$ é o conjunto das soluções do sistema proposto.
\item A redução à forma escalonada reduzida da matriz associada ao sistema é obtida através das seguintes operações elementares:\begin{align*}
\begin{amatrix}{5}
  3 & -12 & -6 & 0 &    9 & -21 \\
 -1 &   4 &  2 & 0 &   -3 &  7 \\
1/2 &  -2 & -1 & 1 & -3/2 & -5/2 \\
 -7 &  28 & 15 & 0 & -23  & 53
\end{amatrix}
&
\grstep{ \frac{1}{3} L_1 }
\begin{amatrix}{5}
  1 & -4 & -2 &  0 &   3 & -7 \\
 -1 &  4 &  2 & 0 &  -3 &   7 \\
1/2 & -2 & -1 & 1 & -3/2 & -5/2 \\
 -7 & 28 & 15 & 0 & -23 & 53
\end{amatrix} \\
\grstep[ L_3 - \frac{1}{2} L_1 \\ L_4 + 7 L_1 ]{ L_2 + L_1 }
\begin{amatrix}{5}
1 & -4 & -2 &   0 &  3 & -7 \\
0 &  0 &  0 &   0 &  0 &  0 \\
0 &  0 &  0 &   1 & -3 & 1 \\
0 &  0 &  1 &   0 & -2 & 4
\end{amatrix}
&
\grstep{ L_2 \leftrightarrow L_4 }
\begin{amatrix}{5}
1 & -4 & -2 &   0 &  3 & -7 \\
0 &  0 &  1 &   0 & -2 & 4 \\
0 &  0 &  0 &   1 & -3 & 1 \\
0 &  0 &  0 &   0 &  0 &  0
\end{amatrix} \\
\grstep{ L_1 +2 L_2 }
\begin{amatrix}{5}
1 & -4 & 0 & 0 & -1 & 1 \\
0 &  0 & 1 & 0 & -2 & 4 \\
0 &  0 & 0 & 1 & -3 & 1 \\
0 &  0 & 0 & 0 &  0 &  0
\end{amatrix}
\end{align*}
Esta última matriz está associada às equações
\[
\left\{
\begin{aligned}
x_1 -4x_2-x_5 & =1 \\
x_3 -2x_5 & =4 \\
x_4 -3x_5 & =1 \\
0 & = 0.
\end{aligned}
\right.
\]

Logo, o conjunto das soluções do sistema proposto é
\begin{align*}
S
& = \{ (x_1, x_2, x_3, x_4, x_5 ) \in \R^5 \mid x_1 = 1 + 4x_2 + x_5, x_2 = 4 + 2x_5, x_4 = 1 + 3 x_5 \} \\
& = \{ (1 + 4x_2 + x_5, 4 + 2x_5, x_3, 1 + 3 x_5, x_5 ) \mid x_3,x_5 \in \R \}.
\end{align*}

\item A redução à forma escalonada reduzida da matriz associada ao sistema é obtida através das seguintes operações elementares:
\begin{align*}
\begin{amatrix}{2}
 3 & -6     & 15i \\
 4 & -8-i & -1+20i \\
-2 & -1     & -5i
\end{amatrix}
&
\grstep{ \frac{1}{3} L_1 }
\begin{amatrix}{2}
 1 & -2     & 5i \\
 4 & -8-i & -1+20i \\
-2 & -1     & -5i
\end{amatrix}
\grstep[L_3 + 2 L_1]{ L_2 - 4 L_1 }
\begin{amatrix}{2}
1 & -2 & 5i \\
0 & -i & -1 \\
0 & -5 & 5i
\end{amatrix} \\
\grstep{ i L_2 }
\begin{amatrix}{2}
1 & -2 & 5i \\
0 & 1 & -i \\
0 & -5 & 5i
\end{amatrix}
&
\grstep{ L_3 + 5 L_2 }
\begin{amatrix}{2}
1 & -2 & 5i \\
0 & 1 & -i \\
0 & 0 & 0
\end{amatrix}
\grstep{ L_1 + 2 L_2 }
\begin{amatrix}{2}
1 & 0 & 3i \\
0 & 1 & -i \\
0 & 0 & 0
\end{amatrix}
\end{align*}

Esta última matriz está associada às equações
\[
\left\{
\begin{aligned}
x & =3i \\
y & =-i\\
0 & = 0.
\end{aligned}
\right.
\]

Logo, o conjunto das soluções do sistema proposto é
$S = \{ (3i, -i) \}$.

\item
\begin{align*}
\begin{amatrix}{3}
 0 &  1 &  6 &  6 \\
 1 &  6 & -5 & -3 \\
 3 & 20 & -3 &  1
\end{amatrix}
&
\grstep{ L_1 \leftrightarrow L_2 }
\begin{amatrix}{3}
 1 &  6 & -5 & -3 \\
 0 &  1 &  6 &  6 \\
 3 & 20 & -3 &  1
\end{amatrix}
\grstep{ L_3 - 3 L_1 }
\begin{amatrix}{3}
 1 & 6 & -5 & -3 \\
 0 & 1 &  6 &  6 \\
 0 & 2 & 12 & 10
\end{amatrix} \\
\grstep{ L_3 - 2 L_1 }
\begin{amatrix}{3}
 1 & 6 & -5 & -3 \\
 0 & 1 &  6 &  6 \\
 0 & 0 &  0 & -2
\end{amatrix}
&
\grstep{ \frac{-1}{2} L_3 }
\begin{amatrix}{3}
 1 & 6 & -5 & -3 \\
 0 & 1 &  6 &  6 \\
 0 & 0 &  0 & 1
\end{amatrix}
\grstep[ L_1 + 3 L_3 ]{ L_2 - 6 L_3 }
\begin{amatrix}{3}
 1 & 6 & -5 & 0 \\
 0 & 1 &  6 & 0 \\
 0 & 0 &  0 & 1
\end{amatrix}
\grstep{ L_1 - 6 L_2 }
\begin{amatrix}{3}
 1 & 0 & -41 & 0 \\
 0 & 1 &  6 & 0 \\
 0 & 0 &  0 & 1
\end{amatrix}
\end{align*}
Como a última linha corresponde a uma equação da forma $0 = 1$, o sistema é impossível, ou seja, $S = \emptyset$.

\end{enumerate}

\item
\begin{enumerate}
\item Primeiro é preciso determinar a inversa de $A$, e para isso serão usadas as mesmas operações elementares que produziram a forma escalonada reduzida de $A$:
\begin{align*}
\begin{bmatrix}
 5 & -5\pi & 1 & 0\\
-1 & \pi+3 & 0 & 1
\end{bmatrix}
&
\grstep{ \frac{1}{5} L_1 }
\begin{bmatrix}
 1 & -\pi & 1/5 & 0 \\
-1 & \pi+3 & 0 & 1
\end{bmatrix}
\grstep{ L_2 + L_1 }
\begin{bmatrix}
1 & -\pi & 1/5 & 0 \\
0 &    3 & 1/5 & 1
\end{bmatrix} \\
&
\grstep{ \frac{1}{3} L_2 }
\begin{bmatrix}
1 & -\pi & 1/5 & 0 \\
0 &    1 & 1/15 & 1/3
\end{bmatrix}
\grstep{ L_1 + \pi L_2 }
\begin{bmatrix}
1 & 0 & 1/5 + \pi/15 & \pi/3 \\
0 & 1 &         1/15 & 1/3
\end{bmatrix}
\end{align*}
Assim, $A^{-1} =
\begin{bmatrix}
1/5 + \pi/15 & \pi/3 \\
        1/15 & 1/3
\end{bmatrix}
=
\frac{1}{15}
\begin{bmatrix}
3 + \pi & 5\pi \\
1       & 5
\end{bmatrix}.$

Sempre que $A X = B$ e $A$ é inversível, vale $X = A^{-1} B$. Assim, para $B = \begin{bmatrix}
-5\pi^2 \\
\pi(\pi+6)
\end{bmatrix}$, tem-se
\[
X
%= A^{-1} B
=
\frac{1}{15}
\begin{bmatrix}
3 + \pi & 5\pi \\
1       & 5
\end{bmatrix}
\cdot 
\begin{bmatrix}
-5\pi^2 \\
\pi(\pi+6)
\end{bmatrix}
=
\frac{1}{15}
\begin{bmatrix}
-5\pi^2(3 + \pi) + 5\pi^2(\pi+6) \\
-5\pi^2 + 5\pi(\pi+6)
\end{bmatrix}
=
\begin{bmatrix}
\pi^2 \\
2 \pi
\end{bmatrix}.
\]

\item Primeiro, determina-se $A^{-1}$ usando as mesmas operações elementares que produziram a forma escalonada reduzida de $A$:
\begin{align*}
\begin{bmatrix}
4 &  4 & 0 &   0 & 1 & 0 & 0 & 0 \\
0 &  5 & 0 & -15 & 0 & 1 & 0 & 0 \\
2 &  2 & 1 &   0 & 0 & 0 & 1 & 0 \\
0 & -1 & 0 &   8 & 0 & 0 & 0 & 1
\end{bmatrix}
&
\grstep{ \frac{1}{4} L_1 }
\begin{bmatrix}
1 &  1 & 0 &   0 & 1/4 & 0 & 0 & 0 \\
0 &  5 & 0 & -15 & 0 & 1 & 0 & 0 \\
2 &  2 & 1 &   0 & 0 & 0 & 1 & 0 \\
0 & -1 & 0 &   8 & 0 & 0 & 0 & 1
\end{bmatrix} \\
\grstep{ L_3 - 2 L_1 }
\begin{bmatrix}
1 &  1 & 0 &   0 & 1/4 & 0 & 0 & 0 \\
0 &  5 & 0 & -15 & 0 & 1 & 0 & 0 \\
0 &  0 & 1 &   0 & -1/2 & 0 & 1 & 0 \\
0 & -1 & 0 &   8 & 0 & 0 & 0 & 1
\end{bmatrix}
& 
\grstep{ \frac{1}{5} L_2 }
\begin{bmatrix}
1 &  1 & 0 &   0 & 1/4 & 0 & 0 & 0 \\
0 &  1 & 0 & -3 & 0 & 1/5 & 0 & 0 \\
0 &  0 & 1 &   0 & -1/2 & 0 & 1 & 0 \\
0 & -1 & 0 &   8 & 0 & 0 & 0 & 1
\end{bmatrix} \\
\grstep{ L_4 + L_2 }
\begin{bmatrix}
1 & 1 & 0 &  0 & 1/4 & 0 & 0 & 0 \\
0 & 1 & 0 & -3 & 0 & 1/5 & 0 & 0 \\
0 & 0 & 1 &  0 & -1/2 & 0 & 1 & 0 \\
0 & 0 & 0 &  5 & 0 & 1/5 & 0 & 1
\end{bmatrix}
&
\grstep{ \frac{1}{5} L_4 }
\begin{bmatrix}
1 & 1 & 0 &  0 & 1/4 & 0 & 0 & 0 \\
0 & 1 & 0 & -3 & 0 & 1/5 & 0 & 0 \\
0 & 0 & 1 &  0 & -1/2 & 0 & 1 & 0 \\
0 & 0 & 0 &  1 & 0 & 1/25 & 0 & 1/5
\end{bmatrix} \\
\grstep{ L_2 + 3L_4 }
\begin{bmatrix}
1 & 1 & 0 &  0 & 1/4 & 0 & 0 & 0 \\
0 & 1 & 0 &  0 & 0 & 8/25 & 0 & 3/5 \\
0 & 0 & 1 &  0 & -1/2 & 0 & 1 & 0 \\
0 & 0 & 0 &  1 & 0 & 1/25 & 0 & 1/5
\end{bmatrix}
&
\grstep{ L_1 - L_2 }
\begin{bmatrix}
1 & 0 & 0 &  0 & 1/4 & -8/25 & 0 & -3/5 \\
0 & 1 & 0 &  0 & 0 & 8/25 & 0 & 3/5 \\
0 & 0 & 1 &  0 & -1/2 & 0 & 1 & 0 \\
0 & 0 & 0 &  1 & 0 & 1/25 & 0 & 1/5
\end{bmatrix}
\end{align*}

Assim, $A^{-1} =
\begin{bmatrix}
 1/4 & -8/25 & 0 & -3/5 \\
   0 &  8/25 & 0 & 3/5  \\
-1/2 &     0 & 1 & 0    \\
   0 &  1/25 & 0 & 1/5
\end{bmatrix}
=
\frac{1}{100}
\begin{bmatrix}
 25 & -32 &   0 & -60 \\
  0 &  32 &   0 &  60 \\
-50 &   0 & 100 &   0 \\
  0 &   4 &   0 &  20
\end{bmatrix}$ e a solução $X = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
x_4
\end{bmatrix}$ do sistema é obtida através da seguinte multiplicação:

\[
X
= A^{-1} B
=
\frac{1}{100}
\begin{bmatrix}
 25 & -32 &   0 & -60 \\
  0 &  32 &   0 &  60 \\
-50 &   0 & 100 &   0 \\
  0 &   4 &   0 &  20
\end{bmatrix}
\cdot
\begin{bmatrix}
16 \\
 2 \\
12 \\
3/5
\end{bmatrix}
=
\frac{1}{100}
\begin{bmatrix}
300 \\
100 \\
400 \\
20
\end{bmatrix}
=
\begin{bmatrix}
3 \\
1 \\
4 \\
1/5
\end{bmatrix}
\]

\item A matriz (não aumentada) associada ao sistema não é quadrada.
\item A matriz (não aumentada) associada ao sistema não é quadrada.
\item A matriz associada ao sistema não é inversível, pois sua forma escalonada reduzida não é a matriz identidade.
\end{enumerate}

\item Os três sistemas podem ser escritos na forma $AX=B$ com uma mesma matriz $A = \begin{bmatrix}
0 & -1 & 5 \\
1 & 2 & 3 \\
2 & 4 & 5
\end{bmatrix}$, então será preciso calcular apenas uma matriz inversa:
\begin{align*}
\begin{bmatrix}
0 & -1 & 5 & 1 & 0 & 0\\
1 & 2 & 3 & 0 & 1 & 0\\
2 & 4 & 5 & 0 & 0 & 1
\end{bmatrix}
&
\grstep{ L_1 \swap L2 }
\begin{bmatrix}
1 & 2 & 3 & 0 & 1 & 0\\
0 & -1 & 5 & 1 & 0 & 0\\
2 & 4 & 5 & 0 & 0 & 1
\end{bmatrix}
\grstep{ L_3 - 2 L1 }
\begin{bmatrix}
1 & 2 & 3 & 0 & 1 & 0\\
0 & -1 & 5 & 1 & 0 & 0\\
0 & 0 & -1 & 0 & -2 & 1
\end{bmatrix} \\
&
\grstep{ -L_2 }
\begin{bmatrix}
1 & 2 & 3 & 0 & 1 & 0\\
0 & 1 & -5 & -1 & 0 & 0\\
0 & 0 & -1 & 0 & -2 & 1
\end{bmatrix}
\grstep{ -L_3 }
\begin{bmatrix}
1 & 2 & 3 & 0 & 1 & 0\\
0 & 1 & -5 & -1 & 0 & 0\\
0 & 0 & 1 & 0 & 2 & -1
\end{bmatrix} \\
&
\grstep{ L2+5L_3 }
\begin{bmatrix}
1 & 2 & 3 & 0 & 1 & 0\\
0 & 1 & 0 & -1 & 10 & -5\\
0 & 0 & 1 & 0 & 2 & -1
\end{bmatrix}
\grstep{ L_1-3L_3 }
\begin{bmatrix}
1 & 2 & 0 & 0 & -5 & 3\\
0 & 1 & 0 & -1 & 10 & -5\\
0 & 0 & 1 & 0 & 2 & -1
\end{bmatrix} \\
&
\grstep{ L_1-2L_2 }
\begin{bmatrix}
1 & 0 & 0 & 2 & -25 & 13\\
0 & 1 & 0 & -1 & 10 & -5\\
0 & 0 & 1 & 0 & 2 & -1
\end{bmatrix}.
\end{align*}
Disto resulta que $A^{-1} = \begin{bmatrix}
 2 & -25 & 13\\
-1 &  10 & -5\\
 0 & 2 & -1
\end{bmatrix}$.

\begin{enumerate}
\item Se $X = \begin{bmatrix}
x\\
y\\
z
\end{bmatrix}$ e $B = \begin{bmatrix}
2\\
7\\
13
\end{bmatrix}$ então:
$
X
= A^{-1} B
=
\begin{bmatrix}
 2 & -25 & 13\\
-1 &  10 & -5\\
 0 & 2 & -1
\end{bmatrix}
\begin{bmatrix}
2\\
7\\
13
\end{bmatrix}
=
\begin{bmatrix}
-2\\
3\\
1
\end{bmatrix}.
$

\item Se $X = \begin{bmatrix}
u\\
v\\
w
\end{bmatrix}$ e $B = \begin{bmatrix}
0\\
0\\
0
\end{bmatrix}$ então $X = \begin{bmatrix}
0\\
0\\
0
\end{bmatrix}$ pois $A$ é inversível.

\item Se $X = \begin{bmatrix}
p\\
q\\
r
\end{bmatrix}$ e $B = \begin{bmatrix}
-2\\
3\\
1
\end{bmatrix}$ então:
$
X
= A^{-1} B
=
\begin{bmatrix}
 2 & -25 & 13\\
-1 &  10 & -5\\
 0 & 2 & -1
\end{bmatrix}
\begin{bmatrix}
-2\\
3\\
1
\end{bmatrix}
=
\begin{bmatrix}
-66\\
27\\
5
\end{bmatrix}.
$
\end{enumerate}

\item A matriz $(B + C^T) ((AB)^T + CA^T)$ tem tamanho $q \times p$, pois
\begin{itemize}
\item $B$ e $C^T$ têm tamanho $q \times r$, de modo que $B + C^T$ também é $q \times r$.
\item $AB$ têm tamanho $p \times r$, de modo que $(AB)^T$ é $r \times p$.
\item $A^T$ têm tamanho $q \times p$, de modo que $CA^T$ é $r \times p$.
\item O produto de qualquer matriz $q \times r$ por uma matriz $r \times p$ tem tamanho $q \times p$.
\end{itemize}

\item
\begin{enumerate}
\item Para quaisquer $m$ e $n$, se $X$ é $m \times n$ então sua transposta $X^T$ é $n \times m$. Em particular, o número de colunas de $X$ é sempre igual ao número de linhas de $X^T$, e estas matrizes podem ser multiplicadas (nesta ordem), gerando um produto que é $m \times m$. Além disso, $X X^T$ é simétrica pois
\[
(X X^T)^T = (X^T)^T X^T = X X^T.
\]
\item De forma análoga ao item anterior, o número de colunas de $X^T$ é sempre igual ao número de linhas de $X$, e estas matrizes podem ser multiplicadas (nesta ordem), desta vez gerando um produto que é $n \times n$. Além disso, $X^T X$ também é simétrica:
\[
(X^T X)^T = X^T (X^T)^T = X^T X.
\]
\item Para que seja possível calcular $X + X^T$, é necessário que $X$ e $X^T$ tenham o mesmo tamanho. Como uma delas é $m \times n$ e a outra é $n \times m$, a adição só será possível se $m = n$. Neste caso, a soma será uma matriz simétrica, pois
\[
(X + X^T)^T = X^T + (X^T)^T = X^T + X = X + X^T.
\]
\item Como no item anterior, para que $X^T + X$ faça sentido é preciso que $X$ e $X^T$ tenham o mesmo tamanho, isto é, que $m = n$. Neste caso, a soma também será uma matriz simétrica, já que
\[
(X^T + X)^T = (X^T)^T + X^T = X + X^T = X^T + X.
\]
\item Novamente, é preciso que $m=n$ para que a operação $X - X^T$ seja possível. No entanto, neste caso
\[
(X - X^T)^T = X^T - (X^T)^T = X^T - X = -(X - X^T).
\]
No entanto, $D = X - X^T$ só será igual a $-(X - X^T)$ se $d_{ij} = -d_{ij}$, para cada $i,j$, e isso só é possível se todos os $d_{ij}$ forem nulos. Em outras palavras, $X - X^T$ só é uma matriz simétrica se $X - X^T = 0$.
\end{enumerate}

\item Há duas possibilidades, dependendo das entradas da primeira coluna:
\begin{enumerate}
\item Se $a \neq 0$ então a redução à forma escalonada reduzida começa com as seguintes operações elementares:
\begin{align*}
\begin{amatrix}{2}
a & b & 0 \\
c & d & 0
\end{amatrix}
&
\grstep{ \frac{1}{a} L_1 }
\begin{amatrix}{2}
1 & \frac{b}{a} & 0 \\
c & d & 0
\end{amatrix}
\grstep{ L_2 - c L_1 }
\begin{amatrix}{2}
1 & \frac{b}{a} & 0 \\
0 & d-c\frac{b}{a} & 0
\end{amatrix}
=
\begin{amatrix}{2}
1 & \frac{b}{a} & 0 \\
0 & \frac{ad-bc}{a} & 0
\end{amatrix}
\end{align*}
Neste ponto, a hipótese de que $ad-bc \neq 0$ pode ser usada para concluir a eliminação:
\begin{align*}
\begin{amatrix}{2}
1 & \frac{b}{a} & 0 \\
0 & \frac{ad-bc}{a} & 0
\end{amatrix}
&
\grstep{ \frac{a}{ad-bc} L_2 }
\begin{amatrix}{2}
1 & \frac{b}{a} & 0 \\
0 & 1 & 0
\end{amatrix}
\grstep{ L_1 - \frac{b}{a} L_2 }
\begin{amatrix}{2}
1 & 0 & 0 \\
0 & 1 & 0
\end{amatrix}.
\end{align*}
\item Se $a = 0$ então uma troca da primeira linha com a segunda faz com que o problema recaia no caso anterior, em que a primeira entrada da primeira linha não é zero. Note que neste caso $c$ não será zero, pois senão ocorreria $ad-bc = 0 \cdot d-b \cdot 0 = 0$.
\end{enumerate}
\textbf{Observação:} Note que $ad-bc$ é justamente a fórmula do determinante da matriz $\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}$.

\item 
\begin{enumerate}
\item Se $A =
\begin{bmatrix}
1/2 & 3 \\
0 & 1
\end{bmatrix}$
e
$B =
\begin{bmatrix}
-1/2 & 3 \\
2 & 0
\end{bmatrix}$
então $(A + B)^2 \neq A^2 + 2AB + B^2$ pois
\[
(A+B)^2 =
\begin{bmatrix}
0 & 6 \\
2 & 1
\end{bmatrix}^2
=
\begin{bmatrix}
12 & 6 \\
2 & 13
\end{bmatrix}
\]
e
\[
A^2 + 2AB + B^2 =
\begin{bmatrix}
1/4 & 9/2 \\
0 & 1
\end{bmatrix}
+
\begin{bmatrix}
23/2 & 3 \\
4 & 0
\end{bmatrix}
+
\begin{bmatrix}
25/4 & -3/2 \\
-1 & 6
\end{bmatrix}
=
\begin{bmatrix}
18 & 6 \\
3 & 7
\end{bmatrix}.
\]

\item Se $A =
\begin{bmatrix}
3 & 7 \\
0 & 2
\end{bmatrix}$
e
$B =
\begin{bmatrix}
2 & -7 \\
0 & 3
\end{bmatrix}$
então $(A + B)^2 = A^2 + 2AB + B^2$ pois
\[
(A+B)^2 =
\begin{bmatrix}
5 & 0 \\
0 & 5
\end{bmatrix}^2
=
\begin{bmatrix}
25 & 0 \\
0 & 25
\end{bmatrix}
\]
e
\[
A^2 + 2AB + B^2 =
\begin{bmatrix}
9 & 35 \\
0 & 4
\end{bmatrix}
+
\begin{bmatrix}
12 & 0 \\
0 & 12
\end{bmatrix}
+
\begin{bmatrix}
4 & -35 \\
0 & 9
\end{bmatrix}
=
\begin{bmatrix}
25 & 0 \\
0 & 25
\end{bmatrix}.
\]

\item $AB =AC$, mas $B \neq C$ e $A \neq 0$
Se $A =
\begin{bmatrix}
1 & 2 \\
3 & 6
\end{bmatrix}$,
$B =
\begin{bmatrix}
 1 & 2 \\
-2 & 0
\end{bmatrix}$
e
$C =
\begin{bmatrix}
-3 & 0 \\
0 & 1
\end{bmatrix}$
então $A \neq 0$, $B \neq C$ e apesar disso
\[
A B =
\begin{bmatrix}
-3 & 2 \\
-9 & 6
\end{bmatrix}
= A C.
\]
\textbf{Observação:} Perceba que $AB = AC$ é equivalente a $AB - AC = 0$, ou ainda, $A(B-C) = 0$. Então o problema poderia ser resolvido procurando matrizes não nulas $A$ e $B-C$ cujo produto fosse a matriz nula (ver exercício (4)).

\item Se $A = B =
\begin{bmatrix}
0 & 2 \\
0 & 0
\end{bmatrix}$
e
$C =
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix}$,
então $B \neq C$ e $A \neq 0$. No entanto, $BA = 0 = CA$.


\item A matriz $C =
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix} \neq 0$
do exemplo anterior satisfaz $C^2=0$.
\end{enumerate}




\item
\begin{enumerate}
\item Em termos das colunas, as colunas do produto $AB$ são combinações lineares das colunas de $A$:
\begin{align*}
\begin{bmatrix}
  1 & 1/2 \\
  3 & 0 \\
7/3 & -1
\end{bmatrix}
\cdot
\begin{bmatrix}
3 & 0 \\
2 & 4
\end{bmatrix}
& =
\begin{bmatrix}
A \cdot \begin{bmatrix}
3 \\
2
\end{bmatrix}
 & A \cdot \begin{bmatrix}
0 \\
4
\end{bmatrix}
\end{bmatrix}
=
\begin{bmatrix}
3
\begin{bmatrix}
  1 \\
  3 \\
7/3
\end{bmatrix}
+ 2
\begin{bmatrix}
1/2 \\
0 \\
-1
\end{bmatrix}
&
0
\begin{bmatrix}
  1 \\
  3 \\
7/3
\end{bmatrix}
+ 4
\begin{bmatrix}
1/2 \\
0 \\
-1
\end{bmatrix}
\end{bmatrix} \\
& =
\begin{bmatrix}
\begin{bmatrix}
3 \\
9 \\
7
\end{bmatrix}
+
\begin{bmatrix}
1 \\
0 \\
-2
\end{bmatrix}
&
\begin{bmatrix}
0 \\
0 \\
0
\end{bmatrix}
+
\begin{bmatrix}
2 \\
0 \\
-4
\end{bmatrix}
\end{bmatrix}
=
\begin{bmatrix}
4 & 2 \\
9 & 0 \\
5 & -4
\end{bmatrix}.
\end{align*}
Já as linhas de $AB$ são combinações lineares das linhas de $B$:
\begin{align*}
\begin{bmatrix}
  1 & 1/2 \\
  3 & 0 \\
7/3 & -1
\end{bmatrix}
\cdot
\begin{bmatrix}
3 & 0 \\
2 & 4
\end{bmatrix}
& =
\begin{bmatrix}
1
\begin{bmatrix}
3 & 0
\end{bmatrix}
+\frac{1}{2}
\begin{bmatrix}
2 & 4
\end{bmatrix}
\\
3
\begin{bmatrix}
3 & 0
\end{bmatrix}
+0\begin{bmatrix}
2 & 4
\end{bmatrix}\\
\frac{7}{3}
\begin{bmatrix}
3 & 0
\end{bmatrix}
-1\begin{bmatrix}
2 & 4
\end{bmatrix}
\end{bmatrix}
=
\begin{bmatrix}
\begin{bmatrix}
3 & 0
\end{bmatrix}
+
\begin{bmatrix}
1 & 2
\end{bmatrix}
\\
\begin{bmatrix}
9 & 0
\end{bmatrix}
+
\begin{bmatrix}
0 & 0
\end{bmatrix}\\
\begin{bmatrix}
7 & 0
\end{bmatrix}
-
\begin{bmatrix}
2 & 4
\end{bmatrix}
\end{bmatrix}
=
\begin{bmatrix}
4 & 2 \\
9 & 0 \\
5 & -4
\end{bmatrix}.
\end{align*}

Outra alternativa é considerar os produtos das colunas de $A$ por colunas de $B$:
\begin{align*}
\begin{bmatrix}
  1 & 1/2 \\
  3 & 0 \\
7/3 & -1
\end{bmatrix}
\cdot
\begin{bmatrix}
3 & 0 \\
2 & 4
\end{bmatrix}
& =
\begin{bmatrix}
1 \\
3 \\
7/3
\end{bmatrix}
\begin{bmatrix}
3 & 0
\end{bmatrix}
+
\begin{bmatrix}
1/2 \\
0 \\
-1
\end{bmatrix}
\begin{bmatrix}
2 & 4
\end{bmatrix} \\
& =
\begin{bmatrix}
3 & 0 \\
9 & 0 \\
7 & 0
\end{bmatrix}
+
\begin{bmatrix}
 1 & 2\\
 0 & 0\\
-2 & -4
\end{bmatrix}
=
\begin{bmatrix}
4 & 2 \\
9 & 0 \\
5 & -4
\end{bmatrix}.
\end{align*}

\item As colunas de $AB$ são combinações lineares das colunas de $A$:
\begin{align*}
\begin{bmatrix}
1+i & 1-i \\
 2i & 3+2i
\end{bmatrix}
\cdot
\begin{bmatrix}
3 & 0 & -i \\
 2i & 4 & 5
\end{bmatrix}
& = %{\scriptsize
%\begin{bmatrix}
\left[\begin{smallmatrix}
3
\begin{bmatrix}
1+i \\
 2i
\end{bmatrix}
+2i
\begin{bmatrix}
1-i \\
3+2i
\end{bmatrix},
0
\begin{bmatrix}
1+i \\
 2i
\end{bmatrix}
+4
\begin{bmatrix}
1-i \\
3+2i
\end{bmatrix},
-i
\begin{bmatrix}
1+i \\
 2i
\end{bmatrix}
+5
\begin{bmatrix}
1-i \\
3+2i
\end{bmatrix}
%\end{bmatrix}
%}% \scriptsize
\end{smallmatrix}\right]
\\
& = % \footnotesize
%\begin{bmatrix}
\left[\begin{smallmatrix}
\begin{bmatrix}
3+3i \\
6i
\end{bmatrix}
+
\begin{bmatrix}
2+2i \\
-4+6i
\end{bmatrix},
\begin{bmatrix}
0 \\
0
\end{bmatrix}
+
\begin{bmatrix}
4-4i \\
12+8i
\end{bmatrix},
\begin{bmatrix}
1-i \\
2
\end{bmatrix}
+
\begin{bmatrix}
5-5i \\
15+10i
\end{bmatrix}
%\end{bmatrix}
\end{smallmatrix}\right]
\\
& =
\begin{bmatrix}
5+5i & 4-4i & 6-6i\\
-4+12i & 12+8i & 17+10i
\end{bmatrix}.
\end{align*}

Já as linhas de $AB$ são combinações lineares das linhas de $B$:
\begin{align*}
\begin{bmatrix}
1+i & 1-i \\
 2i & 3+2i
\end{bmatrix}
\cdot
\begin{bmatrix}
3 & 0 & -i \\
 2i & 4 & 5
\end{bmatrix}
& =
\begin{bmatrix}
(1+i)
& \begin{bmatrix}
3 & 0 & -i
\end{bmatrix}
& +(1-i)
&\begin{bmatrix}
2i & 4 & 5
\end{bmatrix} \\
2i
& \begin{bmatrix}
3 & 0 & -i
\end{bmatrix}
&+(3+2i)
&\begin{bmatrix}
2i & 4 & 5
\end{bmatrix}
\end{bmatrix} \\
& =
\begin{bmatrix}
\begin{bmatrix}
3+3i & 0 & 1-i
\end{bmatrix}
+
\begin{bmatrix}
2+2i & 4-4i & 5-5i
\end{bmatrix}\\
\begin{bmatrix}
6i & 0 & 2
\end{bmatrix}
+
\begin{bmatrix}
-4+6i & 12+8i & 15 + 10i
\end{bmatrix}
\end{bmatrix}\\
& =
\begin{bmatrix}
5+5i & 4-4i & 6-6i\\
-4+12i & 12+8i & 17+10i
\end{bmatrix}.
\end{align*}
O mesmo produto pode ainda ser obtido como uma soma de matrizes $2 \times 3$:
\begin{align*}
\begin{bmatrix}
1+i & 1-i \\
 2i & 3+2i
\end{bmatrix}
\cdot
\begin{bmatrix}
3 & 0 & -i \\
 2i & 4 & 5
\end{bmatrix}
& =
\begin{bmatrix}
1+i \\
 2i
\end{bmatrix}
\begin{bmatrix}
3 & 0 & -i
\end{bmatrix}
+\begin{bmatrix}
1-i \\
3+2i
\end{bmatrix}
\begin{bmatrix}
 2i & 4 & 5
\end{bmatrix}
\\
& =
\begin{bmatrix}
3 +3i & 0 & 1-i \\
6i    & 0 & 2
\end{bmatrix}
+
\begin{bmatrix}
2+2i  & 4-4i & 5-5i \\
-4+6i & 12 + 8i & 15+10i
\end{bmatrix}
\\
& =
\begin{bmatrix}
5+5i & 4-4i & 6-6i\\
-4+12i & 12+8i & 17+10i
\end{bmatrix}.
\end{align*}

\item As colunas de $AB$ podem ser obtidas como combinações lineares das colunas de $A$:
\begin{align*}
\begin{bmatrix}
\pi & 0 & 0 \\
  2 & 3 & 0 \\
  1 & -2 & 5
\end{bmatrix}
\cdot
\begin{bmatrix}
\pi & 2 & 1 \\
 0 & 3 & -2 \\
 0 & 0 & 5
\end{bmatrix}
& =
\left[\begin{smallmatrix}
\pi
\begin{bmatrix}
\pi \\
  2 \\
  1
\end{bmatrix}
+0
\begin{bmatrix}
0 \\
3 \\
-2
\end{bmatrix}
+0
\begin{bmatrix}
0 \\
0 \\
5
\end{bmatrix},\quad
&
2
\begin{bmatrix}
\pi \\
  2 \\
  1
\end{bmatrix}
+3
\begin{bmatrix}
0 \\
3 \\
-2
\end{bmatrix}
+0
\begin{bmatrix}
0 \\
0 \\
5
\end{bmatrix},\quad
&
1
\begin{bmatrix}
\pi \\
  2 \\
  1
\end{bmatrix}
-2
\begin{bmatrix}
0 \\
3 \\
-2
\end{bmatrix}
+5
\begin{bmatrix}
0 \\
0 \\
5
\end{bmatrix}
\end{smallmatrix}\right] \\
& =
\begin{bmatrix}
\begin{bmatrix}
\pi^2 \\
2\pi \\
\pi
\end{bmatrix},\quad
\begin{bmatrix}
2\pi \\
4 \\
2
\end{bmatrix}
+
\begin{bmatrix}
0 \\
9 \\
-6
\end{bmatrix},\quad
\begin{bmatrix}
\pi \\
2 \\
1
\end{bmatrix}
+
\begin{bmatrix}
0 \\
-6 \\
4
\end{bmatrix}
+
\begin{bmatrix}
0 \\
0 \\
25
\end{bmatrix}
\end{bmatrix} \\
& =\begin{bmatrix}
\pi^2 & 2\pi & \pi\\
2\pi & 13 & -4\\
\pi & -4 & 30
\end{bmatrix}.
\end{align*}

Alternativamente, as linhas de $AB$ podem ser obtidas como combinações lineares das linhas de $B$:
\begin{align*}
\begin{bmatrix}
\pi & 0 & 0 \\
  2 & 3 & 0 \\
  1 & -2 & 5
\end{bmatrix}
\cdot
\begin{bmatrix}
\pi & 2 & 1 \\
 0 & 3 & -2 \\
 0 & 0 & 5
\end{bmatrix}
& =
\begin{bmatrix}
\pi
\begin{bmatrix}
\pi & 2 & 1
\end{bmatrix}
+0
\begin{bmatrix}
 0 & 3 & -2
\end{bmatrix}
+0
\begin{bmatrix}
 0 & 0 & 5
\end{bmatrix} \\
2
\begin{bmatrix}
\pi & 2 & 1
\end{bmatrix}
+3
\begin{bmatrix}
 0 & 3 & -2
\end{bmatrix}
+0
\begin{bmatrix}
 0 & 0 & 5
\end{bmatrix}\\
\begin{bmatrix}
\pi & 2 & 1
\end{bmatrix}
-2
\begin{bmatrix}
 0 & 3 & -2
\end{bmatrix}
5
\begin{bmatrix}
 0 & 0 & 5
\end{bmatrix}
\end{bmatrix} \\
& =
\begin{bmatrix}
\begin{bmatrix}
\pi^2 & 2\pi & \pi
\end{bmatrix}
+
\begin{bmatrix}
 0 & 0 & 0
\end{bmatrix}
+
\begin{bmatrix}
 0 & 0 & 0
\end{bmatrix}
\\
\begin{bmatrix}
2\pi & 4 & 2
\end{bmatrix}
+
\begin{bmatrix}
0 & 9 & -6
\end{bmatrix}
+
\begin{bmatrix}
 0 & 0 & 0
\end{bmatrix}
\\
\begin{bmatrix}
\pi & 2 & 1
\end{bmatrix}
+
\begin{bmatrix}
0 & -6 & 4
\end{bmatrix}
+
\begin{bmatrix}
0 & 0 & 25
\end{bmatrix}
\end{bmatrix} \\
& =
\begin{bmatrix}
\pi^2 & 2\pi & \pi \\
2\pi & 13 & -4 \\
\pi & -4 & 30 
\end{bmatrix}.
\end{align*}

Outra opção é somar os produtos de linhas de $A$ por colunas de $B$:
\begin{align*}
\begin{bmatrix}
\pi & 0 & 0 \\
  2 & 3 & 0 \\
  1 & -2 & 5
\end{bmatrix}
\cdot
\begin{bmatrix}
\pi & 2 & 1 \\
 0 & 3 & -2 \\
 0 & 0 & 5
\end{bmatrix}
& =
\begin{bmatrix}
\pi \\
  2\\
  1
\end{bmatrix}
\begin{bmatrix}
\pi & 2 & 1
\end{bmatrix}
+
\begin{bmatrix}
0\\
3\\
-2
\end{bmatrix}
\begin{bmatrix}
  0 & 3 & -2
\end{bmatrix}
+
\begin{bmatrix}
0 \\
0 \\
5
\end{bmatrix}
\begin{bmatrix}
0 & 0 & 5
\end{bmatrix} \\
& =
\begin{bmatrix}
\pi^2 & 2\pi & \pi \\
 2\pi & 4 & 2 \\
  \pi & 2 & 1
\end{bmatrix}
+
\begin{bmatrix}
0 & 0 & 0 \\
0 & 9 & -6 \\
0 & -6 & 4
\end{bmatrix}
+
\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 25
\end{bmatrix}\\
& =
\begin{bmatrix}
\pi^2 & 2\pi & \pi \\
2\pi & 13 & -4 \\
\pi & -4 & 30 
\end{bmatrix}.
\end{align*}

\item As colunas do produto são obtidas assim:
\begin{align*}
\begin{bmatrix}
       -1 & 1 \\
        3 & 2 \\
 \sqrt{2} & -1 \\
 \sqrt{3} & -1 
\end{bmatrix}
\cdot
\begin{bmatrix}
        1 & \sqrt{2} & 0\\
 \sqrt{3} &        0 & 2
\end{bmatrix}
& =
\begin{bmatrix}
\begin{bmatrix}
       -1 \\
        3 \\
 \sqrt{2} \\
 \sqrt{3} 
\end{bmatrix}
+\sqrt{3}
\begin{bmatrix}
1 \\
2 \\
-1 \\
-1
\end{bmatrix},
&
\sqrt{2}
\begin{bmatrix}
       -1 \\
        3 \\
 \sqrt{2} \\
 \sqrt{3} 
\end{bmatrix},
&
2
\begin{bmatrix}
1 \\
2 \\
-1 \\
-1
\end{bmatrix}
\end{bmatrix} \\
& =
\begin{bmatrix}
-1 + \sqrt{3} & -\sqrt{2} & 2 \\
3 + 2\sqrt{3} & 3\sqrt{2} & 4 \\
\sqrt{2} - \sqrt{3} & 2 & -2 \\
0 & \sqrt{6} & -2
\end{bmatrix}
\end{align*}

\item Considerando as submatrizes (os blocos)
{\footnotesize
\[
0 = \begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix},
I = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix},
B_{11} = \begin{bmatrix}
1 & 2 \\
5 & 6
\end{bmatrix},
B_{12} = \begin{bmatrix}
3 & 4 \\
7 & 8
\end{bmatrix}, 
B_{21} = \begin{bmatrix}
9 & 10 \\
13 & 14
\end{bmatrix} \text{ e }
B_{22} = \begin{bmatrix}
11 & 12 \\
15 & 16
\end{bmatrix}
\]
}
e usando a multiplicação por blocos, resulta que:
\begin{align*}
A \cdot B
& =
\begin{bmatrix}
0
&
I \\
I
&
0
\end{bmatrix}
\cdot
\begin{bmatrix}
B_{11} & B_{12}\\
B_{21} & B_{22}
\end{bmatrix}
=
\begin{bmatrix}
  0 B_{11} + I B_{21}
& 0 B_{12} + I B_{22}\\
  I B_{11} + 0 B_{21}
& I B_{12} + 0 B_{22}
\end{bmatrix} \\
& =
\begin{bmatrix}
  B_{21} & B_{22}\\
  B_{11} & B_{12}
\end{bmatrix}
= 
\begin{bmatrix}
 9 & 10 & 11 & 12 \\
13 & 14 & 15 & 16 \\
 1 & 2 & 3 & 4 \\
 5 & 6 & 7 & 8
\end{bmatrix}.
\end{align*}
\end{enumerate}

\item 
\begin{enumerate}
\item Tem-se
\begin{align*}
I^2
& =   \begin{bmatrix} \ii & 0 \\ 0 & -\ii \end{bmatrix}
\cdot \begin{bmatrix} \ii & 0 \\ 0 & -\ii \end{bmatrix}
= \begin{bmatrix} \ii^2 & 0 \\ 0 & (-\ii)^2 \end{bmatrix}
= \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} \\
J^2
& =   \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}
\cdot \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}
= \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} \\
K^2
& =   \begin{bmatrix} 0 & \ii \\ \ii & 0 \end{bmatrix}
\cdot \begin{bmatrix} 0 & \ii \\ \ii & 0 \end{bmatrix}
= \begin{bmatrix} \ii^2 & 0 \\ 0 & \ii^2 \end{bmatrix}
= \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix} \\
IJK
& =   \left( \begin{bmatrix} \ii & 0 \\ 0 & -\ii \end{bmatrix}
\cdot \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix} \right)
\cdot \begin{bmatrix} 0 & \ii \\ \ii & 0 \end{bmatrix}
=     \begin{bmatrix} 0 & \ii \\ \ii & 0 \end{bmatrix}
\cdot \begin{bmatrix} 0 & \ii \\ \ii & 0 \end{bmatrix}
= \begin{bmatrix} -1 & 0 \\ 0 & -1 \end{bmatrix}
\end{align*}

Portanto, $I^2 = J^2 = K^2 = I J K = -U$.


\item Tem-se
\begin{align*}
IJ
& =   \begin{bmatrix} \ii & 0 \\ 0 & -\ii \end{bmatrix}
\cdot \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}
= \begin{bmatrix} 0 & i \\ i & 0 \end{bmatrix}
= K, \\
-JI
& =  -\begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}
\cdot \begin{bmatrix} \ii & 0 \\ 0 & -\ii \end{bmatrix}
= - \begin{bmatrix} 0 & -\ii \\ -\ii & 0 \end{bmatrix}
= K, \\
JK
& =   \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}
\cdot \begin{bmatrix} 0 & \ii \\ \ii & 0 \end{bmatrix}
= \begin{bmatrix} \ii & 0 \\ 0 & -\ii \end{bmatrix}
= I, \\
-KJ
& =  -\begin{bmatrix} 0 & \ii \\ \ii & 0 \end{bmatrix}
\cdot \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}
=    -\begin{bmatrix} -\ii & 0 \\ 0 & \ii \end{bmatrix}
= I, \\
KI
& =   \begin{bmatrix} 0 & \ii \\ \ii & 0 \end{bmatrix}
\cdot \begin{bmatrix} \ii & 0 \\ 0 & -\ii \end{bmatrix}
= \begin{bmatrix} 0 & 1 \\ -1 & 0 \end{bmatrix}
= J, \\
-IK
& =  -\begin{bmatrix} \ii & 0 \\ 0 & -\ii \end{bmatrix}
\cdot \begin{bmatrix} 0 & \ii \\ \ii & 0 \end{bmatrix}
=    -\begin{bmatrix} 0 & -1 \\ 1 & 0 \end{bmatrix}
= J.
\end{align*}

\item Em $\R^3$, os vetores $\vec{i}$, $\vec{j}$ e $\vec{k}$ satisfazem as seguintes propriedades em relação ao produto vetorial:
\[
  \vec{i} \times \vec{j}
= - \vec{j} \times \vec{i}
= \vec{k},
\]
\[
  \vec{j} \times \vec{k}
= - \vec{k} \times \vec{j}
= \vec{i},
\]
\[
  \vec{k} \times \vec{i}
= - \vec{i} \times \vec{k}
= \vec{j}.
\]
Percebe-se que, exceto pela mudança de notação, as propriedades são as mesmas das matrizes $I$, $J$ e $K$.

\item Como $U$ é a matriz identidade de tamanho $2 \times 2$, e pelo primeiro item
\[
I^2 = J^2 = K^2 = -U,
\]
resulta que $-I \cdot I = -J \cdot J = -K \cdot K = U$, isto é,
\[
I^{-1} = -I, \quad
J^{-1} = -J, \quad
K^{-1} = -K.
\]

\end{enumerate}

\item 
\begin{enumerate}
\item 
\begin{enumerate}
\item Sendo $A$ e $B$ matrizes $2 \times 2$, o produto $C = AB$ também é $2 \times 2$ e tem-se:
\[
C = AB =
\begin{bmatrix}
a_{11} & a_{12}\\
a_{21} & a_{22}
\end{bmatrix}
\cdot
\begin{bmatrix}
b_{11} & b_{12}\\
b_{21} & b_{22}
\end{bmatrix}
=
\begin{bmatrix}
a_{11}b_{11} + a_{12}b_{21} & a_{11}b_{12} + a_{12}b_{22}\\
a_{21}b_{11} + a_{22}b_{21} & a_{11}b_{12} + a_{12}b_{22}
\end{bmatrix}.
\]
Nesta matriz, há $4 = 2^2$ entradas $c_{ij}$, e cada uma delas é calculada por meio de $1$ adição e $2$ multiplicações. Assim, são necessárias $4 = 2^2 \times 1$ adições e $8 = 2^2 \times 2 = 2^3$ multiplicações.
\item No caso de matrizes $3 \times 3$, serão $9 = 3^2$ entradas do tipo
\[
c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + a_{i3}b_{3j},
\]
cada uma sendo obtida por meio de $3$ multiplicações e $2$ adições. Assim, o produto de matrizes $3 \times 3$ exige $27 = 9 \times 3 = 3^3$ multiplicações e $18 = 9 \times 2 = 3^2 \cdot 2$ adições.
\item O caso $n \times n$ é análogo: há $n^2$ entradas $c_{ij}$ e cada uma delas é calculada por meio de $n$ produtos (o número de colunas de $A$) e $n-1$ adições. Portanto, são utilizadas $n^3 = n^2 n$ multiplicações e $n^3 - n^2 = n^2 (n-1)$ adições.
\end{enumerate}

\item No \href{https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm#Sub-cubic_algorithms}{artigo da Wikipédia (em inglês) sobre algoritmos para a multiplicação de matrizes} são mencionados alguns métodos alternativos que exigem menos do que $n^3$ multiplicações para calcular o produto de matrizes $n\times n$ (e também de matrizes não quadradas).
Um dos primeiros métodos deste tipo foi \href{https://dx.doi.org/10.1007\%2FBF02165411}{publicado em 1969 por Volker Strassen}. Atualmente, ainda é um problema em aberto encontrar o método mais rápido para a multiplicação de matrizes.
\end{enumerate}



\item 
\begin{enumerate}
\item Ao sortear 10 matrizes $7 \times 7$ \textit{aleatoriamente}, é bem provavel que \textbf{todas} as matrizes obtidas sejam inversíveis (execute o comando mais de 10 vezes se não estiver convencido).
\item Repetindo o experimento com matrizes quadradas de qualquer outro tamanho, há grandes chances de não encontrar uma única matriz que não seja inversível. De fato, ao sortear \textit{aleatoriamente} uma matriz quadrada, há \textbf{probabilidade zero} (não é só pequena, é zero!) de ser escolhida uma matriz não inversível. Elas são raras, mas pode se deparar com elas se estiver com sorte (ou se o sorteio não for realmente aleatório).

\item Sejam $A_2 =
\begin{bmatrix}
0 & 5 \\
0 & 0
\end{bmatrix}$, $A_3 =
\begin{bmatrix}
0 & -2 & 1 \\
0 &  0 & 5 \\
0 &  0 & 0
\end{bmatrix}$ e $A_4 =
\begin{bmatrix}
0 & -1 & 2 & 3 \\
0 &  0 & 1 & 5 \\
0 &  0 & 0 & 7 \\
0 &  0 & 0 & 0
\end{bmatrix}$. Então:
\begin{enumerate}
\item
\begin{align*}
A_2^2 & =
\begin{bmatrix}
0 & 5 \\
0 & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
0 & 5 \\
0 & 0
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix}, \\
A_3^3 & =
\left(
\begin{bmatrix}
0 & -2 & 1 \\
0 &  0 & 5 \\
0 &  0 & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
0 & -2 & 1 \\
0 &  0 & 5 \\
0 &  0 & 0
\end{bmatrix}
\right)
\cdot
\begin{bmatrix}
0 & -2 & 1 \\
0 &  0 & 5 \\
0 &  0 & 0
\end{bmatrix} \\
& =
\begin{bmatrix}
0 & 0 & -10 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
0 & -2 & 1 \\
0 &  0 & 5 \\
0 &  0 & 0
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix},\\
A_4^4 & =
\begin{bmatrix}
0 & -1 & 2 & 3 \\
0 &  0 & 1 & 5 \\
0 &  0 & 0 & 7 \\
0 &  0 & 0 & 0
\end{bmatrix}^2
\cdot
\begin{bmatrix}
0 & -1 & 2 & 3 \\
0 &  0 & 1 & 5 \\
0 &  0 & 0 & 7 \\
0 &  0 & 0 & 0
\end{bmatrix}^2
=
\begin{bmatrix}
0 & 0 & -1 & 9 \\
0 & 0 & 0 & 7 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}^2
=
\begin{bmatrix}
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}.
\end{align*}

\item Com base nos exemplos anteriores, é natural suspeitar que a $n$-ésima potência de uma matriz triangular superior $n\times n$ qualquer, com zeros na diagonal, é sempre a matriz nula $n\times n$.
\item As matrizes triangulares superiores de tamanho $2 \times 2$, com zeros na diagonal, têm a forma $A_2 =
\begin{bmatrix}
0 & c\\
0 & 0
\end{bmatrix}$, em que $c$ pode ser qualquer escalar. Então:
\[
A_2^2 =
\begin{bmatrix}
0 & c\\
0 & 0
\end{bmatrix}
\begin{bmatrix}
0 & c\\
0 & 0
\end{bmatrix}
=
\begin{bmatrix}
0 & 0\\
0 & 0
\end{bmatrix}
\]
Já no caso $3 \times 3$, tem-se $A_3=
\begin{bmatrix}
0 & a & b \\
0 & 0 & c \\
0 & 0 & 0
\end{bmatrix}$ e então:
\[
A_3^3 =
\begin{bmatrix}
0 & a & b \\
0 & 0 & c \\
0 & 0 & 0
\end{bmatrix}^2
\cdot
\begin{bmatrix}
0 & a & b \\
0 & 0 & c \\
0 & 0 & 0
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & ac \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
0 & a & b \\
0 & 0 & c \\
0 & 0 & 0
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}.
\]
Mais geralmente, se $A = (a_{ij})$ for uma matriz triangular superior de tamanho $n \times n$ com diagonal nula, então as primeiras $i$ entradas da linha $i$ são todas nulas. Ao elevar $A$ ao quadrado, a matriz obtida terá as primeiras $i+1$ entradas da linha $i$ iguais a zero. Analogamente, ao calcular $A^3$, a matriz resultante terá $i+2$ entradas da linha $i$ igual a zero. Como a matriz tem $n$ colunas, procedendo desta maneira até obter $A^n$ o resultado final será uma matriz com zeros em todas as $n$ colunas de cada linha.
\[
\overset{A}{
\overbrace{
\begin{bmatrix}
\textbf{0} &     a_{12} & a_{13} & \ldots & a_{1n} \\
         0 & \textbf{0} & a_{23} & \ldots & a_{2n} \\
    \vdots &     \vdots & \ddots & \ddots & \vdots \\
         0 &          0 &      0 & \ddots & a_{n-1,n} \\
         0 &          0 &      0 & \ldots & \textbf{0}
\end{bmatrix}
}}
\rightarrow
\overset{A^2}{
\overbrace{
\begin{bmatrix}
         0 & \textbf{0} & a_{13} & \ldots & a_{1n} \\
         0 &          0 & \textbf{0} & \ldots & a_{2n} \\
    \vdots &     \vdots &     \ddots & \ddots & \vdots \\
         0 &          0 &          0 & \ddots & \textbf{0} \\
         0 &          0 &          0 & \ldots & 0
\end{bmatrix}
}}
%\rightarrow
\ldots \rightarrow
\overset{A^n}{
\overbrace{
\begin{bmatrix}
         0 &          0 &          0 & \ldots & \textbf{0} \\
         0 &          0 &          0 & \ldots & 0 \\
    \vdots &     \vdots &     \ddots & \ddots & \vdots \\
         0 &          0 &          0 & \ddots & 0 \\
         0 &          0 &          0 & \ldots & 0
\end{bmatrix}
}}
\]

O padrão acima também pode ser percebido ao calcular explicitamente as entradas dos produtos. Como a matriz $A$ é triangular superior e tem zeros na diagonal, tem-se $a_{ij} = 0$ sempre que $i \geq j$. Consequentemente, se $i \geq j-1$ a entrada $ij$ de $A^2$ é dada por
\begin{align*}
[A^2]_{ij}
  = [A \cdot A]_{ij}
& = (a_{i1} a_{1j}       + \ldots + a_{ii} a_{ij})
  + (a_{i,i+1} a_{i+1,j} + \ldots + a_{in} a_{nj}) \\
& = (0 a_{1j}       + \ldots + 0 a_{ij})
  + (a_{i,i+1} 0 + \ldots + a_{in} 0)
  = 0.
\end{align*}
Do mesmo modo, $[A^3]_{ij} = 0$ para $i \geq j-2$:
\begin{align*}
[A^3]_{ij}
  = [A^2 \cdot A]_{ij}
& = ([A^2]_{i1} a_{1j} + \ldots + [A^2]_{i,i+1} a_{i+1,j}) \\
& \quad + ([A^2]_{i,i+2} a_{i+2,j} + \ldots + [A^2]_{in} a_{nj}) \\
& = (0 a_{1j}       + \ldots + 0 a_{i+1,j})
  + ([A^2]_{i+2,j+1} 0 + \ldots + [A^2]_{in} 0)
  = 0.
\end{align*}
Procedendo da mesma maneira até a $n$-ésima potência de $A$, consegue-se todas as entradas iguais a zero.
\end{enumerate}
\end{enumerate}

\item Como
\[
MX = \begin{bmatrix}
-1 &  2 & 3 \\
 2 & -4 & 5 \\
-1 &  1 & 7
\end{bmatrix}
\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{bmatrix}
=
\begin{bmatrix}
-a + 2d + 3g & -b + 2e + 3h & -c + 2f + 3i \\
2a - 4d + 5g & 2b - 4e + 5h & 2c - 4f + 5i \\
-a +  d + 7g & -b +  e + 7h & -c +  f + 7i
\end{bmatrix}
\]
e por hipótese $MX = I$, uma comparação das entradas de $MX$ com as de $I$ mostra que as incógnitas que formam as colunas de $X$ devem ser soluções dos sistemas lineares
\[
\systeme[adg]{
-a + 2d + 3g = 1,
2a - 4d + 5g = 0,
-a +  d + 7g = 0
}, 
\systeme[adg]{
-a + 2d + 3g = 0,
2a - 4d + 5g = 1,
-a +  d + 7g = 0
} \text{ e }
\systeme[adg]{
-a + 2d + 3g = 0,
2a - 4d + 5g = 1,
-a +  d + 7g = 0
}.
\]

Como todos os sistemas têm a mesma matriz de coeficientes, os três podem ser escalonados simultaneamente como segue:

\begin{align*}
&
\begin{bmatrix}
-1 &  2 & 3 & 1 & 0 & 0\\
 2 & -4 & 5 & 0 & 1 & 0\\
-1 &  1 & 7 & 0 & 0 & 1
\end{bmatrix}
\grstep{ -L_1 }
\begin{bmatrix}
 1 & -2 & -3 & -1 & 0 & 0\\
 2 & -4 &  5 &  0 & 1 & 0\\
-1 &  1 &  7 &  0 & 0 & 1
\end{bmatrix} \\
\grstep{ L_2 - 2 L_1 }
& \begin{bmatrix}
 1 & -2 & -3 & -1 & 0 & 0\\
 0 &  0 & 11 &  2 & 1 & 0\\
-1 &  1 &  7 &  0 & 0 & 1
\end{bmatrix}
\grstep{ L_3 + L_1 }
\begin{bmatrix}
 1 & -2 & -3 & -1 & 0 & 0\\
 0 &  0 & 11 &  2 & 1 & 0\\
 0 & -1 &  4 & -1 & 0 & 1
\end{bmatrix} \\
\grstep{ L_2 \swap L_3 }
& \begin{bmatrix}
 1 & -2 & -3 & -1 & 0 & 0\\
 0 & -1 &  4 & -1 & 0 & 1\\
 0 &  0 & 11 &  2 & 1 & 0
\end{bmatrix}
\grstep{ -L_2 }
\begin{bmatrix}
 1 & -2 & -3 & -1 & 0 &  0\\
 0 &  1 & -4 &  1 & 0 & -1\\
 0 &  0 & 11 &  2 & 1 &  0
\end{bmatrix} \\
\grstep{ \frac{1}{11} L_3 }
&
\begin{bmatrix}
 1 & -2 & -3 & -1 & 0 &  0\\
 0 &  1 & -4 &  1 & 0 & -1\\
 0 &  0 &  1 & 2/11 & 1/11 &  0
\end{bmatrix}
\grstep{ L_2 +4 L_3 }
\begin{bmatrix}
 1 & -2 & -3 & -1 & 0 &  0\\
 0 &  1 &  0 & 19/11 & 4/11 & -1\\
 0 &  0 &  1 & 2/11 & 1/11 &  0
\end{bmatrix} \\
\grstep{ L_1 + 3 L_3 }
&
\begin{bmatrix}
 1 & -2 & 0 & -5/11 & 3/11 &  0\\
 0 &  1 & 0 & 19/11 & 4/11 & -1\\
 0 &  0 & 1 &  2/11 & 1/11 &  0
\end{bmatrix}
\grstep{ L_1 + 2 L_2 }
\begin{bmatrix}
 1 & 0 & 0 & 3 & 1 & -2\\
 0 & 1 & 0 & 19/11 & 4/11 & -1\\
 0 & 0 & 1 &  2/11 & 1/11 &  0
\end{bmatrix}
\end{align*}
Assim,
$X = \begin{bmatrix}
    3 &    1 & -2\\
19/11 & 4/11 & -1\\
 2/11 & 1/11 &  0
\end{bmatrix}$. Multiplicando esta matriz à esquerda de $M$, obtém-se:
\[
XM =
\begin{bmatrix}
    3 &    1 & -2\\
19/11 & 4/11 & -1\\
 2/11 & 1/11 &  0
\end{bmatrix}
\begin{bmatrix}
-1 &  2 & 3 \\
 2 & -4 & 5 \\
-1 &  1 & 7
\end{bmatrix}
=\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}.
\]
Isto quer dizer que a matrix $X$ que atua como inversa à direita de $A$ também é uma inversa à esquerda de $A$, pois ambos os produtos ($AX$ e $XA$) resultam na matriz identidade.


\item Para que a matriz $T =
\begin{bmatrix}
-1 & 9 & 1 \\
-1 & t & 3 \\
-1 & 9 & t + 1
\end{bmatrix}$
seja inversível, sua forma escalonada reduzida por linhas deve ser a matriz identidade. Procedendo com a eliminação de Gauss-Jordan, seriam realizadas as seguintes operações elementares sobre as linhas:
\begin{align*}
\begin{bmatrix}
-1 & 9 & 1 \\
-1 & t & 3 \\
-1 & 9 & t + 1
\end{bmatrix}
\grstep{ -L_1 }
\begin{bmatrix}
1 & -9 & -1 \\
-1 & t & 3 \\
-1 & 9 & t + 1
\end{bmatrix}
\grstep{ L_2 + L_1 }
\begin{bmatrix}
1 & -9 & -1 \\
0 & t-9 & 2 \\
-1 & 9 & t + 1
\end{bmatrix}
\grstep{ L_3 + L_1 }
\begin{bmatrix}
1 & -9 & -1 \\
0 & t-9 & 2 \\
0 & 0 & t
\end{bmatrix}
\end{align*}
Neste ponto, para conseguir um pivô igual a $1$ na segunda coluna da segunda linha, seria necessária uma divisão da segunda linha por $t - 9$, e isso significa que se $t=9$ a matriz não será inversível. Além disso, no passo seguinte, será necessário dividir a terceira linha por $t$, de modo que para $t = 0$ a matriz também não será inversível. Supondo que $t \neq 0$ e $t \neq 9$, basta realizar mais algumas operações elementares e obtém-se a identidade:
\begin{align*}
\begin{bmatrix}
1 & -9 & -1 \\
0 & t-9 & 2 \\
0 & 0 & t
\end{bmatrix}
\grstep{ \frac{1}{t-9} L_2 }
&\begin{bmatrix}
1 & -9 & -1 \\
0 & 1 & \frac{2}{t-9} \\
0 & 0 & t
\end{bmatrix}
\grstep{ \frac{1}{t} L_3 }
\begin{bmatrix}
1 & -9 & -1 \\
0 & 1 & \frac{2}{t-9} \\
0 & 0 & 1
\end{bmatrix}
\grstep{ L_2 -\frac{2}{t-9} L_3 }
\begin{bmatrix}
1 & -9 & -1 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}\\
\grstep{ L_1 + L_3 }
&\begin{bmatrix}
1 & -9 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\grstep{ L_1 + 9L_2 }
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}.
\end{align*}
Portanto, $T$ é inversível se, e somente se, $t \not \in \{0, 9\}$.

\item As operações elementares a seguir mostram que $N$ é equivalente por linhas à identidade, desde que seja possível dividir por $1-t$ e depois por $t(t+2)$. Isto significa que para $t \in\{ -2, 0, 1 \}$ a matriz $N$ não é inversível, pois apareceria uma linha nula em um dos passos da eliminação de Gauss-Jordan.
\begin{align*}
N= & \begin{bmatrix}
2-t & 0 & -4 \\
6 & 1-t & -15 \\
2 & 0 & -4-t
\end{bmatrix}
\grstep{ L_1 \swap L_3 }
\begin{bmatrix}
2 & 0 & -4-t \\
6 & 1-t & -15 \\
2-t & 0 & -4
\end{bmatrix}
\grstep{ \frac{1}{2} L_1 }
\begin{bmatrix}
1 & 0 & -2-\frac{t}{2} \\
6 & 1-t & -15 \\
2-t & 0 & -4
\end{bmatrix} \\
\grstep{ L_2 - 6 L_1 }
& \begin{bmatrix}
1 & 0 & -2-\frac{t}{2} \\
0 & 1-t & 3(t-1) \\
2-t & 0 & -4
\end{bmatrix}
\grstep{ L_3 - (2-t) L_1 }
\begin{bmatrix}
1 & 0 & \frac{-4-t}{2} \\
0 & 1-t & 3(t-1) \\
0 & 0 & -\frac{t(t+2)}{2}
\end{bmatrix}
\grstep{ \frac{1}{1-t} L_2 }
\begin{bmatrix}
1 & 0 & \frac{-4-t}{2} \\
0 & 1 & -3 \\
0 & 0 & -\frac{t(t+2)}{2}
\end{bmatrix}\\
\grstep{ -\frac{2}{t(t+2)} L_2 }
& \begin{bmatrix}
1 & 0 & \frac{-4-t}{2} \\
0 & 1 & -3 \\
0 & 0 & 1
\end{bmatrix}
\grstep{ L_2 + 3 L_3}
\begin{bmatrix}
1 & 0 & \frac{-4-t}{2} \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
\grstep{ L_1 -\frac{4+t}{2} L_3}
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}.
\end{align*}

\item
\begin{enumerate}
\item A matriz nula é uma matriz na forma escalonada reduzida por linhas, pois 
\begin{itemize}
\item Não nenhuma linha não nula em que o primeiro elemento não nulo seja diferente de 1 (nem sequer existem linhas não nulas);
\item Todas as linhas nulas estão na parte inferior
\item Não há pivôs mais a esquerda dos pivôs de linhas anteriores (já que não há pivôs)
\item Não há elementos não nulos acima ou abaixo de nenhum pivô
\end{itemize}
\item A matriz identidade $I_{4 \times 4}$ está na forma escalonada reduzida por linhas pois
\begin{itemize}
\item Em todas as linhas o o primeiro elemento não nulo é 1;
\item Não há linhas nulas
\item Todos os pivôs estão na diagonal
\item Exceto pelos pivôs que estão na diagonal, as colunas só contém zeros
\end{itemize}
\item Há matrizes escalonadas por linhas que não estão na forma escalonada reduzida. Por exemplo, se $A = \begin{bmatrix}
1 & 3 \\ 0 & 1
\end{bmatrix}$ então a segunda coluna contém um pivô, mas tem o elemento não nulo 3.
\item Em uma matriz triangular superior $S \in M_{n \times n}(K)$, todos os elementos abaixo da diagonal principal são nulos, ou seja, $s_{ij} = 0$ sempre que $i > j$. Se $S$ é simétrica, então $s_{ij} = s_{ji}$, sendo $1 \leq i,j \leq n$. Em particular, se $i < j$ então $s_{ij} = s_{ji} = 0$, pois $j > i$. Logo, $T$ é uma matriz diagonal, já que $s_{ij} = 0$ sempre que que $i > j$ ou $i < j$, isto é, para $i \neq j$.

\item Se $U, V \in M_{m \times m} (K)$ são matrizes diagonais, então $UV = VU$. De fato, se $i \neq j$ então $u_{ij} = v_{ij} = 0$ e além disso
\[
\left[UV\right]_{ij}
= \sum_{k=1}^m u_{ik} v_{kj}
= u_{i1} v_{1j} + u_{i2} v_{2j} + \ldots + u_{im} v_{mj}.
\]
Nesta soma, tem-se $u_{ik} = 0$, exceto possivelmente quando $k = i$. Mesmo assim, a parcela $u_{ii}v_{ij}$ será nula, pois $k = i \neq j \Rightarrow v_{kj} = v_{ij} = 0$. Assim, todos os termos da soma são nulos, e as entradas $\left[UV\right]_{ij}$ são nulas sempre que $i \neq j$. De forma análoga, tem-se $\left[VU\right]_{ij} = 0 $ para $i \neq j$, ou seja, $UV$ e $VU$ coincidem fora da diagonal principal. Por outro lado, na diagonal principal tem-se $i = j$ e então
\[
\left[UV\right]_{ij} = u_{ii} v_{ii} = v_{ii} u_{ii} = \left[VU\right]_{ij}.
\]

\item Seja $A$ antissimétrica. Então $A^T = -A$ e resulta que $
\left( A^T \right)^T = A = -A^T$, ou seja, $A^T$ também é antissimétrica.

\item Dada uma matriz antissimétrica $A \in M_{n \times n}(\R)$, tem-se $[A]_{ij} = [A^T]_{ji} = -[A]_{ji}$. Em particular, se $i = j$, vale $[A]_{ii} = -[A]_{ii}$, o que implica que  $2[A]_{ii} = 0$, isto é,  $[A]_{ii} = 0$. Assim, todas as entradas da diagonal de $A$ são nulas.

\item A matriz nula $0 \in M_{n \times n}(\R)$ é simétrica e antissimétrica simultaneamente.

\item A matriz $\frac{1}{2}(Q-Q^T)$ é antissimétrica porque
{\footnotesize
\[
\left[ \frac{1}{2}(Q-Q^T) \right]^T
= \frac{1}{2} \left[ (Q-Q^T) \right]^T
= \frac{1}{2} (Q^T-(Q^T)^T)
= \frac{1}{2} (Q^T-Q)
= - \left[ \frac{1}{2} (Q - Q^T)\right].
\]
}

\item O sistema linear $Tx = 2x$ tem uma única solução se, e somente se, $Tx - 2x = (T - 2I)x = 0$ tem uma única solução, o e isto ocorre se, e somente se, $T-2I$ for uma matriz inversível.
\end{enumerate}


\item Seja $D \in M_{2\times 2} (\R)$ uma matriz diagonal. Então
$
D = \begin{bmatrix}
x_1 & 0\\
0 & x_2
\end{bmatrix},
$
com $x_1, x_2 \in \R$
e tem-se
\begin{align*}
D^2
& =
\begin{bmatrix}
x_1 & 0 \\
0 & x_2
\end{bmatrix}^2
=
\begin{bmatrix}
x_1^2 & 0 \\
0 & x_2^2
\end{bmatrix}
=
\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}.
\end{align*}
Assim, os escalares $x_1$ e $x_2$ satisfazem $x_i^2 = 1$, ou seja, $x_i = 1$ ou $x_i = -1$. Logo, $D$ pode ser uma destas 4 matrizes:
\[
\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix},
\begin{bmatrix}
1 & 0\\
0 & -1
\end{bmatrix},
\begin{bmatrix}
-1 & 0\\
0 & 1
\end{bmatrix}
\text{ e }
\begin{bmatrix}
-1 & 0\\
0 & -1
\end{bmatrix}
\]
No caso de matrizes $3 \times 3$, cada uma das três entradas da diagonal pode ser igual a $1$ ou a $-1$, e consequentemente $I = I_3$ tem 8 raízes quadradas distintas.

\item Seja $D \in M_{3 \times 3} (\R)$ uma matriz diagonal. Então
$
D = \begin{bmatrix}
x_1 & 0 & 0\\
0 & x_2 & 0\\
0 & 0 & x_3
\end{bmatrix},
$
com $x_1, x_2, x_3 \in \R$
e tem-se
\begin{align*}
D^2 - 7D + 10I
& =
\begin{bmatrix}
x_1 & 0 & 0\\
0 & x_2 & 0\\
0 & 0 & x_3
\end{bmatrix}^2
-7
\begin{bmatrix}
x_1 & 0 & 0\\
0 & x_2 & 0\\
0 & 0 & x_3
\end{bmatrix}
+10
\begin{bmatrix}
1 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 1
\end{bmatrix} \\
& =
\begin{bmatrix}
x_1^2 - 7x_1 + 10& 0 & 0\\
0 & x_2^2 - 7x_2 + 10 & 0\\
0 & 0 & x_3^2 - 7x_3 + 10
\end{bmatrix}
=
\begin{bmatrix}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{bmatrix}.
\end{align*}
Assim, se $D^2 - 7D + 10I = 0$ os escalares $x_1$, $x_2$ e $x_3$ são soluções de $x_i^2 - 7x_i + 10 = 0$, ou seja, de $(x_i-2)(x_i-5)=0$. Portanto, cada $x_i$ pode assumir os valores $2$ ou $5$, e há as seguintes possibilidades para $D$:
{\footnotesize
\[
\begin{bmatrix}
2 & 0 & 0\\
0 & 2 & 0\\
0 & 0 & 2
\end{bmatrix},
\begin{bmatrix}
2 & 0 & 0\\
0 & 2 & 0\\
0 & 0 & 5
\end{bmatrix},
\begin{bmatrix}
2 & 0 & 0\\
0 & 5 & 0\\
0 & 0 & 2
\end{bmatrix},
\begin{bmatrix}
2 & 0 & 0\\
0 & 5 & 0\\
0 & 0 & 5
\end{bmatrix},
\begin{bmatrix}
5 & 0 & 0\\
0 & 2 & 0\\
0 & 0 & 2
\end{bmatrix},
\begin{bmatrix}
5 & 0 & 0\\
0 & 2 & 0\\
0 & 0 & 5
\end{bmatrix},
\begin{bmatrix}
5 & 0 & 0\\
0 & 5 & 0\\
0 & 0 & 2
\end{bmatrix}
\text{ e }
\begin{bmatrix}
5 & 0 & 0\\
0 & 5 & 0\\
0 & 0 & 5
\end{bmatrix}.
\]
}


\item \teoria Seja $S$ uma matriz simétrica $n \times n$, isto é, $S^T = S$. As entradas de $S^2$ e de $(S^2)^T$, são dadas por
\begin{equation}\label{eq:S-quadrado}
[S^2]_{ij}
= \sum_{k=1}^n s_{ik} s_{kj}
= s_{i1} s_{1j} + s_{i2} s_{2j} + \ldots + s_{in} s_{nj}
\end{equation}
e
\[
[(S^2)^T]_{ij}
= [S^2]_{ji}
= \sum_{k=1}^n s_{jk} s_{ki}
= s_{j1} s_{1i} + s_{j2} s_{2i} + \ldots + s_{jn} s_{ni}
\]
respectivamente. Mas as entradas de $S$ satisfazem a igualdade $s_{ij} = s_{ji}$, então resulta desta última equação, permutando os índices de cada termo, que
\begin{align*}
[(S^2)^T]_{ij}
& = s_{j1} s_{1i} + s_{j2} s_{2i} + \ldots + s_{jn} s_{ni} \\
& = s_{1j} s_{i1} + s_{2j} s_{i2} + \ldots + s_{nj} s_{in}\\
& = s_{i1} s_{1j} + s_{i2} s_{2j} + \ldots + s_{in} s_{nj},
\end{align*}
onde a última igualdade deve-se à propriedade comutativa dos escalares $s_{ij}$. Comparando com \eqref{eq:S-quadrado}, conclui-se que $[(S^2)^T]_{ij} = [S^2]_{ij}$, ou seja, que $(S^2)^T = S^2$, o que significa que $S^2$ é simétrica.

\textbf{Observação:} Para uma verificação mais direta, sem comparar entradas individuais das matrizes, poderia ser usada o fato de que $(AB)^T = B^T A^T$:
\[
(S^2)^T = (S S)^T = S^T S^T = S S = S^2.
\]

Por este raciocínio fica fácil ver que as potências de uma matriz simétrica são simétricas:
\[
(S^n)^T
= (S \cdot \ldots \cdot S)^T
= S^T \cdot \ldots \cdot S^T
= S \cdot \ldots \cdot S = S^n.
\]

\item
\begin{enumerate}
\item Usando a definição de traço e as propriedades da adição, resulta que:
\begin{align*}
tr(A+B)
& = [A+B]_{11} + [A+B]_{22} + \ldots + [A+B]_{nn} \\
& = ([A]_{11} + [A]_{11}) + ([A]_{22} + [B]_{22}) + \ldots + ([A]_{nn} + [B]_{nn}) \\
& = ([A]_{11} + \dots + [A]_{nn}) + ([B]_{11} + \ldots + [B]_{nn})\\
& = tr(A)+tr(B).
\end{align*}
\item Segue da definição de traço e das propriedades da multiplicação por escalar que:
\begin{align*}
tr(cB)
& = [cA]_{11} + [cA]_{22} + \ldots + [cA]_{nn} \\
& = c[A]_{11} + c[A]_{22} + \ldots + c[A]_{nn} \\
& = c([A]_{11} + \dots + [A]_{nn}) \\
& = c \cdot tr(A).
\end{align*}
\item Como a diagonal principal não é alterada pela transposição de matrizes, e o traço só depende destas entradas, tem-se:
\begin{align*}
tr(A^T)
& = [A^T]_{11} + [A^T]_{22} + \ldots + [A^T]_{nn} \\
& = [A]_{11} + [A]_{22} + \ldots + [A]_{nn} \\
& = tr(A).
\end{align*}
\end{enumerate}
\item
\begin{enumerate}
\item Se
$A = \begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix}$
e 
$B = \begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix}$, então
$A + B
= \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix} = I$, que é inversível. Porém, $A$ e $B$ não são inversíveis, já que possuem uma coluna de zeros.

\item Se
$A = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}$
e 
$B = \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix}$, então
$A + B
= \begin{bmatrix}
1 & 1 \\
1 & 1
\end{bmatrix}$ não é inversível, pois $AX = 0$ tem uma solução não nula $X = \begin{bmatrix}
1\\-1
\end{bmatrix}$. Porém, $A = A^{-1}$ e $B = B^{-1}$ são inversíveis.

\item Se
$A = B = \begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}$, então
$A + B
= \begin{bmatrix}
2 & 0 \\
0 & 2
\end{bmatrix}$ e as matrizes $A$, $B$ e $A+B$ são inversíveis, sendo  $A^{-1} = B^{-1} = I$ e $(A+B)^{-1} = (2I)^{-1} = \frac{1}{2} I$.
\end{enumerate}
\item Para que o gráfico de $p(x) = ax^3+bx^2+cx+d$ passe pelos pontos $(-1,0)$, $(0,2)$, $(1,0)$ e $(2,6)$ é preciso que $p(-1)=0$, $p(0)=2$, $p(1)=0$ e $p(2)=6$, isto é, os coeficientes devem satisfazer
\[
\left\{
\begin{alignedat}{4}
(-1)^3a & {}+{} & (-1)^2b & {}+{} & (-1)c & {}+{} & d & = 0\\
   0^3a & {}+{} &    0^2b & {}+{} &    0c & {}+{} & d & = 2\\
   1^3a & {}+{} &    1^2b & {}+{} &    1c & {}+{} & d & = 0\\
   2^3a & {}+{} &    2^2b & {}+{} &    2c & {}+{} & d & = 6,
\end{alignedat}
\right.
\]
ou equivalentemente,
\[
\left\{
\begin{alignedat}{4}
-a & {}+{} &  b & {}-{} &  c & {}+{} & d & = 0\\
   &       &    &       &    &       & d & = 2\\
 a & {}+{} &  b & {}+{} &  c & {}+{} & d & = 0\\
8a & {}+{} & 4b & {}+{} & 2c & {}+{} & d & = 6.
\end{alignedat}
\right.
\]
Procedendo com a eliminação de Gauss-Jordan, obtem-se

\begin{align*}
&
\begin{amatrix}{4}
-1 & 1 & -1 & 1 & 0 \\
 0 & 0 &  0 & 1 & 2 \\
 1 & 1 &  1 & 1 & 0 \\
 8 & 4 &  2 & 1 & 6
\end{amatrix}
\grstep{ -L_1 }
\begin{amatrix}{4}
 1 &-1 &  1 & -1 & 0 \\
 0 & 0 &  0 &  1 & 2 \\
 1 & 1 &  1 &  1 & 0 \\
 8 & 4 &  2 &  1 & 6
\end{amatrix}
\grstep{ L_3 - L_1 }
\begin{amatrix}{4}
 1 &-1 &  1 & -1 & 0 \\
 0 & 0 &  0 &  1 & 2 \\
 0 & 2 &  0 &  2 & 0 \\
 8 & 4 &  2 &  1 & 6
\end{amatrix}\\
&
\grstep{ L_4 - 8L_1 }
\begin{amatrix}{4}
 1 & -1 & 1 & -1 & 0 \\
 0 &  0 & 0 &  1 & 2 \\
 0 &  2 & 0 &  2 & 0 \\
 0 & 12 & -6 & 9 & 6
\end{amatrix}
\grstep{ L_2 \swap L_3 }
\begin{amatrix}{4}
 1 & -1 & 1 & -1 & 0 \\
 0 &  2 & 0 &  2 & 0 \\
 0 &  0 & 0 &  1 & 2 \\
 0 & 12 & -6 & 9 & 6
\end{amatrix}
\grstep{ \frac{1}{2}L_2 }
\begin{amatrix}{4}
 1 & -1 & 1 & -1 & 0 \\
 0 &  1 & 0 &  1 & 0 \\
 0 &  0 & 0 &  1 & 2 \\
 0 & 12 & -6 & 9 & 6
\end{amatrix}\\
&
\grstep{ L_4 - 12 L_2 }
\begin{amatrix}{4}
 1 & -1 & 1 & -1 & 0 \\
 0 &  1 & 0 &  1 & 0 \\
 0 &  0 & 0 &  1 & 2 \\
 0 &  0 & -6 & -3 & 6
\end{amatrix}
\grstep{ L_3 \swap L_4 }
\begin{amatrix}{4}
 1 & -1 & 1 & -1 & 0 \\
 0 &  1 & 0 &  1 & 0 \\
 0 &  0 & -6 & -3 & 6 \\
 0 &  0 & 0 &  1 & 2
\end{amatrix}
\grstep{ \frac{-1}{6}L_3 }
\begin{amatrix}{4}
 1 & -1 & 1 & -1 & 0 \\
 0 &  1 & 0 &  1 & 0 \\
 0 &  0 & 1 & 1/2 & -1 \\
 0 &  0 & 0 &  1 & 2
\end{amatrix}\\
&
\grstep{ L_3 - \frac{1}{2} L_4 }
\begin{amatrix}{4}
 1 & -1 & 1 & -1 & 0 \\
 0 &  1 & 0 &  1 & 0 \\
 0 &  0 & 1 &  0 & -2 \\
 0 &  0 & 0 &  1 & 2
\end{amatrix}
\grstep{ L_2 - L_4 }
\begin{amatrix}{4}
 1 & -1 & 1 & -1 & 0 \\
 0 &  1 & 0 &  0 & -2 \\
 0 &  0 & 1 &  0 & -2 \\
 0 &  0 & 0 &  1 & 2
\end{amatrix}
\grstep{ L_1 + L_4 }
\begin{amatrix}{4}
 1 & -1 & 1 & 0 & 2 \\
 0 &  1 & 0 & 0 & -2 \\
 0 &  0 & 1 & 0 & -2 \\
 0 &  0 & 0 & 1 & 2
\end{amatrix}\\
&
\grstep{ L_1 - L_3 }
\begin{amatrix}{4}
 1 & -1 & 0 & 0 & 4 \\
 0 &  1 & 0 & 0 & -2 \\
 0 &  0 & 1 & 0 & -2 \\
 0 &  0 & 0 & 1 & 2
\end{amatrix}
\grstep{ L_1 + L_2 }
\begin{amatrix}{4}
 1 & 0 & 0 & 0 & 2 \\
 0 & 1 & 0 & 0 & -2 \\
 0 & 0 & 1 & 0 & -2 \\
 0 & 0 & 0 & 1 & 2
\end{amatrix}.
\end{align*}


Assim, a única solução deste sistema é $a=2$, $b=-2$, $c=-2$ e $d=2$, e portanto $p(x) = 2x^3-2x^2-2x+2$.

\item Para calcular a inversa de $R$, procede-se com a redução de $[R|I]$ à forma escada reduzida por linhas. Assumindo que $\cos(\theta) \neq 0$, tem-se:
\begin{align*}
[R|I] = &
\begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & \cos(\theta) & \sen(\theta) & 0 & 1 & 0 \\
0 & -\sen(\theta) & \cos(\theta) & 0 & 0 & 1
\end{bmatrix}
\grstep{ \frac{1}{\cos(\theta)} L_2 }
\begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & \tan(\theta) & 0 & \sec(\theta) & 0 \\
0 & -\sen(\theta) & \cos(\theta) & 0 & 0 & 1
\end{bmatrix} \\
&
\grstep{ L_3 + \sen(\theta) L_2 }
\begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & \tan(\theta) & 0 & \sec(\theta) & 0 \\
0 & 0 & \sec(\theta) & 0 & \tan(\theta) & 1
\end{bmatrix}
\grstep{ \cos(\theta) L_3 }
\begin{bmatrix}
1 & 0 & 0  & 1 & 0 & 0 \\
0 & 1 & \tan(\theta) & 0 & \sec(\theta) & 0 \\
0 & 0 & 1 & 0 & \sen(\theta) & \cos(\theta)
\end{bmatrix}\\
&
\grstep{ L_2 - \tan(\theta) L_3 }
\begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & \cos(\theta) & -\sen(\theta) \\
0 & 0 & 1 & 0 & \sen(\theta) & \cos(\theta)
\end{bmatrix}
\Rightarrow
R^{-1} =
\begin{bmatrix}
1 & 0 & 0 \\
0 & \cos(\theta) & -\sen(\theta) \\
0 & \sen(\theta) & \cos(\theta)
\end{bmatrix}.
\end{align*}
Por outro lado, se $\cos(\theta) = 0$, tem-se
\begin{align*}
[R|I] = &
\begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 0 & \sen(\theta) & 0 & 1 & 0 \\
0 & -\sen(\theta) & 0 & 0 & 0 & 1
\end{bmatrix}
\grstep{ L_2 \swap L_3 }
\begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & -\sen(\theta) & 0 & 0 & 0 & 1 \\
0 & 0 & \sen(\theta) & 0 & 1 & 0
\end{bmatrix}\\
&
\grstep{ -\frac{1}{\sen(\theta)} L_2 }
\begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & -\frac{1}{\sen(\theta)} \\
0 & 0 & \sen(\theta) & 0 & 1 & 0
\end{bmatrix}
\grstep{ \frac{1}{\sen(\theta)} L_3 }
\begin{bmatrix}
1 & 0 & 0 & 1 & 0 & 0 \\
0 & 1 & 0 & 0 & 0 & -\frac{1}{\sen(\theta)} \\
0 & 0 & 1 & 0 & \frac{1}{\sen(\theta)} & 0
\end{bmatrix}
\end{align*}
Mas $\cos(\theta) = 0$ e sabe-se que $\cos^2(\theta) + \sen^2(\theta) = 1$, então $\sen^2(\theta) = 1$ e $\sen(\theta) = \frac{1}{\sen(\theta)}$. Logo, 
$
R^{-1}
=
\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & -\frac{1}{\sen(\theta)} \\
0 & \frac{1}{\sen(\theta)} & 0
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
0 & \cos(\theta) & -\sen(\theta) \\
0 & \sen(\theta) & \cos(\theta)
\end{bmatrix}$, como no caso $\cos(\theta) \neq 0$.


\item Sendo $H =
\begin{bmatrix}
a & b \\
c & d
\end{bmatrix}
=
\begin{bmatrix}
\frac{1}{2}(e^x+e^{-x}) & \frac{1}{2}(e^x-e^{-x}) \\
\frac{1}{2}(e^x-e^{-x}) & \frac{1}{2}(e^x+e^{-x})
\end{bmatrix}
=
\frac{1}{2}
\begin{bmatrix}
e^x+e^{-x} & e^x-e^{-x} \\
e^x-e^{-x} & e^x+e^{-x}
\end{bmatrix}
$ uma matriz $2 \times 2$, pode-se obter $H^{-1}$ diretamente (caso seja inversível) por meio da fórmula
\[
\begin{bmatrix}
a & b\\
c& d
\end{bmatrix}^{-1}
=
\frac{1}{ad-bc}
\begin{bmatrix}
d & -b\\
-c& a
\end{bmatrix}.
\]
Observe que
\[
ad - bc
= \frac{1}{4}\left[ (e^x+e^{-x})^2 - (e^x-e^{-x})^2 \right]
= \frac{1}{4}\left[ (2e^x)(2e^{-x}) \right]
= e^x e^{-x}
= 1
\]
e portanto
\[
H^{-1}
=
\frac{1}{2}
\begin{bmatrix}
e^x+e^{-x} & -(e^x-e^{-x}) \\
-(e^x-e^{-x}) & e^x+e^{-x}
\end{bmatrix}
=
\frac{1}{2}
\begin{bmatrix}
e^x+e^{-x} & -e^x+e^{-x} \\
-e^x+e^{-x} & e^x+e^{-x}
\end{bmatrix}.
\]
\textbf{Observação:} Lembrando que $\cosh(x) = \frac{1}{2}(e^x+e^{-x})$ e que $\senh(x) = \frac{1}{2}(e^x+e^{-x})$, o resultado anterior pode ser expresso assim:
\[
\begin{bmatrix}
\cosh(x) & \senh(x) \\
\senh(x) & \cosh(x)
\end{bmatrix}^{-1}
=
\begin{bmatrix}
 \cosh(x) & -\senh(x) \\
-\senh(x) & \cosh(x)
\end{bmatrix}.
\]


\item
\begin{enumerate}
\item Já que $C = \begin{bmatrix}
1 & 2\\
0 & -1\\
2 & 0
\end{bmatrix}$ é uma matriz $3 \times 2$, a equação $XC = I$ só fará sentido se a identidade $I$ for $2 \times 2$ e a matriz $X$ for $2 \times 3$. Considerando $X = 
\begin{bmatrix}
x & y & z\\
u & v & w
\end{bmatrix}$, tem-se
\[
XC = 
\begin{bmatrix}
x & y & z\\
u & v & w
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & 2\\
0 & -1\\
2 & 0
\end{bmatrix}
=
\begin{bmatrix}
x+2z & 2x-y\\
u+2w & 2u-v
\end{bmatrix}
=
\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}.
\]
Esta igualdade equivale aos seguintes sistemas lineares (um para cada linha de $X$):
\[
\systeme[xyz]{
x+2z=1,
2x-y=0
}
\text{ e }
\systeme[uvw]{
u+2w=0,
2u-v=1
}
\]
Procedendo com o escalonamento dos sistemas, obtém-se:
\[
\begin{amatrix}{3}
1 &  0 & 2 & 1\\
2 & -1 & 0 & 0
\end{amatrix}
\grstep{ L_2 - 2 L_1 }
\begin{amatrix}{3}
1 &  0 &  2 &  1\\
0 & -1 & -4 & -2
\end{amatrix}
\grstep{ -L_2 }
\begin{amatrix}{3}
1 &  0 &  2 &  1\\
0 & 1 & 4 & 2
\end{amatrix}
\]
\[
\begin{amatrix}{3}
1 &  0 & 2 & 0\\
2 & -1 & 0 & 1
\end{amatrix}
\grstep{ L_2 - 2 L_1 }
\begin{amatrix}{3}
1 &  0 &  2 & 0\\
0 & -1 & -4 & 1
\end{amatrix}
\grstep{ -L_2 }
\begin{amatrix}{3}
1 &  0 &  2 & 0\\
0 & 1 & 4 & -1
\end{amatrix}
\]
Assim, os sistemas são equivalentes a
\[
\systeme[xyz]{
x = 1 - 2z,
y = 2 - 4z
}
\text{ e }
\systeme[uvw]{
u=-2w,
v=-1-4w
}
\]
o que significa que qualquer matriz $X = \begin{bmatrix}
1 - 2z &  2 - 4z & z\\
   -2w & -1 - 4w & w
\end{bmatrix}$, com $z,w$ arbitrários, é uma inversa à esquerda de $C$.

\item Considerando novamente $X = 
\begin{bmatrix}
x & y & z\\
u & v & w
\end{bmatrix}$, tem-se
\[
CX = 
\begin{bmatrix}
1 & 2\\
0 & -1\\
2 & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
x & y & z\\
u & v & w
\end{bmatrix}
=
\begin{bmatrix}
x+2u & y+2v & z+2w\\
  -u &   -v &   -w\\
  2x &   2y &   2z
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{bmatrix}.
\]
Esta igualdade equivale aos seguintes sistemas lineares (um para cada coluna de $X$):
\[
\systeme[xyz]{
x + 2u = 1,
    -u = 0,
    2x = 0
}
\quad
\systeme[uvw]{
y + 2v = 0,
    -v = 1,
    2y = 0
}
\text{ e }
\systeme[uvw]{
z + 2w = 0,
    -w = 0,
    2z = 1
}
\]
Estes sistemas são incompatíveis, pois por exemplo, as equações $-u = 0$ e $2x = 0$ implicam que $u=x=0$ e portanto $x+2u = 0 \neq 1$.

Portanto, nenhuma matriz $X$ é uma inversa à direita de $C$.
\end{enumerate}

\end{enumerate}
\end{document}

