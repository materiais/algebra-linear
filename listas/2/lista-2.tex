\documentclass[12pt,a4paper]{article}
\usepackage{cmap} % Makes the PDF copiable. See http://tex.stackexchange.com/a/64198/25761
\usepackage[T1]{fontenc}
\usepackage[brazil]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{textcomp} % \degree
\usepackage{gensymb} % \degree
\usepackage[usenames,svgnames,dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{graphicx}
\usepackage[margin=2cm]{geometry}
\usepackage{systeme}

\hypersetup{
    colorlinks = true,
    allcolors = {blue}
}

\newcommand{\fixme}{{\color{red}(...)}}
\newcommand*\ger[1]{\operatorname{ger}\left\{#1\right\}}
\newcommand\ii{\mathrm{i}}

\newcommand*\N{\mathbb{N}}
\newcommand*\R{\mathbb{R}}
\newcommand*\C{\mathbb{C}}

\newcommand{\IconPc}{\includegraphics[width=1em]{computer.png}}
\newcommand{\IconCalc}{\includegraphics[width=1em]{calculator.png}}
\newcommand{\IconThink}{\includegraphics[width=1em]{pencil.png}}
\newcommand{\IconCheck}{\includegraphics[width=1em]{checkmark.png}}
\newcommand{\IconConcept}{\includegraphics[width=1em]{edit.png}}

\newlength{\SmileysLength}
\setlength{\SmileysLength}{\labelwidth}\addtolength{\SmileysLength}{\labelsep}

\newcommand{\calc}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconCalc}%
   \hspace*{\SmileysLength}}
\newcommand{\software}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconPc}%
   \hspace*{\SmileysLength}}
\newcommand{\teoria}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconThink}%
   \hspace*{\SmileysLength}}
\newcommand{\conceito}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconCheck}%
   \hspace*{\SmileysLength}}
\newcommand{\concept}{\hspace*{-\SmileysLength}\makebox[0pt][r]{\IconCheck}%
   \hspace*{\SmileysLength}}

% Loop Space / CC BY-SA-3.0 / https://tex.stackexchange.com/a/2238/25761
\newenvironment{amatrix}[1]{%
  \left[\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right]
}

% Loop Space / CC BY-SA-3.0 / https://tex.stackexchange.com/a/3164/25761
%--------grstep
% For denoting a Gauss' reduction step.
% Use as: \grstep{\rho_1+\rho_3} or \grstep[2\rho_5 \\ 3\rho_6]{\rho_1+\rho_3}
\newcommand{\grstep}[2][\relax]{%
   \ensuremath{\mathrel{
       {\mathop{\longrightarrow}\limits^{#2\mathstrut}_{
                                     \begin{subarray}{l} #1 \end{subarray}}}}}}
\newcommand{\swap}{\leftrightarrow}

\newcommand*\tipo{2ª Lista de Exercícios}
%\newcommand*\turma{...}
\newcommand*\disciplina{ALI0001}
\newcommand*\eu{Helder G. G. de Lima}
\newcommand*\data{\today}

\author{\eu}
\title{\tipo - \disciplina}
\date{\data}

\begin{document}

\begin{center}
\includegraphics[width=9.0cm]{marca} \\
\textbf{\tipo\ (\disciplina)} \\
Prof. \eu\footnote{
Este é um material de acesso livre distribuído sob os termos da licença \href{https://creativecommons.org/licenses/by-sa/4.0/deed.pt_BR}{Creative Commons Atribuição-CompartilhaIgual 4.0 Internacional}}
\end{center}

\section*{Legenda}
\begin{multicols}{4}
\begin{itemize}
\item[] \hspace*{\SmileysLength} \calc \hspace*{-\SmileysLength} Cálculos
\item[] \hspace*{\SmileysLength} \conceito \hspace*{-\SmileysLength} Conceitos
\item[] \hspace*{\SmileysLength} \teoria \hspace*{-\SmileysLength} Teoria
%\item[] \hspace*{\SmileysLength} \software \hspace*{-\SmileysLength} Software
\end{itemize}
\end{multicols}

\section*{Questões}

\begin{enumerate}
\item \conceito Revise todos os axiomas da definição de espaço vetorial $V$ sobre o corpo de escalares $\R$, verificando a validade de cada um deles nos seguintes conjuntos. Em cada item, as operações de adição e multiplicação por escalar que aparecem no lado esquerdo são as do espaço vetorial que está sendo definido, e as do lado direito são operações usuais de $\R$.

\begin{enumerate}
\item $V = \R^n$, formado por todas as sequências de $n$ números reais, e
\begin{align*}
(x_1, \ldots, x_n) + (y_1, \ldots, y_n)
& = (x_1 + y_1, \ldots, x_n + y_n)\\
k \cdot (x_1, \ldots, x_n)
& = (kx_1, \ldots, kx_n)
\end{align*}
\item $V = \R^{m \times n} = M_{m \times n} (\R)$, formado pelas matrizes de ordem $m \times n$, com $m,n$ fixos e
\begin{align*}
(A + B)_{ij}
& = (A)_{ij} + (B)_{ij}, \text{ para } 1 \leq i \leq m \text{ e } 1 \leq j \leq n,\\
(k \cdot A)_{ij}
& = k(A_{ij}), \text{ para } 1 \leq i \leq m \text{ e } 1 \leq j \leq n
\end{align*}
\item $V = \R^{\R} = \mathcal{F}(\R) = \mathcal{F}(-\infty, +\infty)$, formado por todas as funções $f: \R \to \R$ e
\begin{align*}
(f + g)(x)
& = f(x) + g(x), \quad \forall x \in \R\\
(k \cdot f)(x)
& = k (f(x)), \quad \forall x \in \R
\end{align*}
\item $V = \C$, formado por todos os números complexos e
\begin{align*}
(a + b \ii) + (c + d \ii)
& = (a + c) + (b + d) \ii \\
k \cdot (a + b \ii)
& = (ka) + (kb) \ii
\end{align*}
\item $V = \R^\infty$, formado por todas as sequências infinitas de números reais, e
\begin{align*}
(x_1, x_2, \ldots) + (y_1, y_2, \ldots)
& = (x_1 + y_1, x_2 + y_2, \ldots)\\
k \cdot (x_1, \ldots, x_n)
& = (kx_1, kx_2, \ldots)
\end{align*}
\end{enumerate}

\item \conceito Verifique se as operações definidas a seguir fazem com que os conjuntos $V$ indicados sejam espaços vetoriais. Em particular, determine se há algum elemento $z \in V$ que faz o papel de ``vetor nulo'', ou seja, que satisfaz $v + z = v, \forall v \in V$, e para cada vetor $v \in V$, indique qual $x \in V$ é o seu ``oposto'', no sentido de que $x + v = z$.
\begin{enumerate}
\item $V = \R$, sendo
$\begin{cases}
x + y
= x + y + 1, \quad \forall x \in V, y \in V \\
k \cdot x
= k x + k - 1, \quad \forall k \in \R, \forall x \in V
\end{cases}$
\item $V = \R^2$, sendo
$\begin{cases}
(x, y) + (u,v)
= (x+u, y+v), \quad \forall (x, y) \in V, \forall (u, v) \in V \\
k \cdot (x,y)
= ( -3 k x + k y, 5 k x - 2 k y ), \quad \forall k \in \R, \forall (x, y) \in V
\end{cases}$
\item $V = \R^2$, sendo
$\begin{cases}
(x, y) + (u, v)
 = (x + u + 1, y + v - 2), \quad \forall (x, y) \in V, \forall (u, v) \in V \\
k \cdot (x,y)
 = ( k x + k - 1, k y - 2 k + 2 ), \quad \forall k \in \R, \forall (x, y) \in V
\end{cases}$
\end{enumerate}
\item \teoria
Assumindo que $V$ seja um espaço vetorial sobre $\R$ e que $u \in V$, $v \in V$ e $a \in \R$, mostre que as seguintes igualdades decorrem dos axiomas satisfeitos por $V$ e por suas operações:
\begin{multicols}{2}
\begin{enumerate}
\item $-(-u)=u$
\item $-(a u) = (-a) u$
\item $-(a u) = a (-u)$
\item $-(u+v) = (-u) + (-v)$
\end{enumerate}
\end{multicols}

\item \conceito Seja $V = \R^4$, o espaço vetorial das quádruplas de números reais, com as operações usuais. Mostre que os seguintes subconjuntos são subespaços vetoriais de $V$:
\begin{enumerate}
\item $U = \{ (a,b,c,d) \in \R^4 \mid a - b = c + d \}$.
\item $W = \{ (a,b,c,d) \in \R^4 \mid c = d = 0 \}$.
\end{enumerate}

\item \conceito Seja $V = M_{3 \times 3}(\R)$, o espaço vetorial das matrizes quadradas de ordem $3$, com as operações usuais. Mostre que os seguintes subconjuntos são subespaços vetoriais de $V$:
\begin{enumerate}
\item As matrizes simétricas: $S = \{ X \in M_{3 \times 3}(\R) \mid X^T = X \}$.
\item As matrizes antissimétricas: $A = \{ X \in M_{3 \times 3}(\R) \mid X^T = -X \}$.
\item As matrizes que comutam com uma matriz $B$ fixada: $C = \{ X \in V \mid BX = XB \}$.
\item $U = \{ A = (a_{ij}) \in M_{3 \times 3}(\R) \mid a_{ij} = 0 \text{ sempre que } i + j \neq 4 \}$.
\end{enumerate}

\item \conceito Seja $V = \mathcal{F}(\R)$. Mostre que os seguintes subconjuntos são subespaços vetoriais de $V$:
\begin{enumerate}
\item O conjunto $P_\infty$ formado por todas as funções polinomiais (de qualquer grau):
\[
P_\infty = \{ p(x) \in \mathcal{F}(\R) \mid p(x) = a_n x^n + \ldots + a_1 x^1 + a_0, \text{ para algum natural } n, \text{ e } a_i \in \R \}.
\]
\item Dado um natural $n$, o conjunto $P_n$ de todos polinômios de grau menor ou igual a $n$:
\[
P_n = \{ q(x) \in P_\infty \mid \operatorname{grau}(q(x)) \leq n \}.
\]
\item As funções pares, isto é, tais que $f(-x) = f(x)$, para todo $x \in \R$.
\item As funções ímpares, isto é, tais que $f(-x) = -f(x)$, para todo $x \in \R$.
\item O conjunto $C^0(\R)$ das funções contínuas
\item O conjunto $C^1(\R)$ das funções deriváveis cuja derivada é contínua
\end{enumerate}

\item \calc Em cada item, determine se $U$ é ou não um subespaço do espaço vetorial $V$ indicado:
\begin{enumerate}
\item $V = \R^2$ e $U = \{ (x, y) \in \R^2 \mid y+2x = 5 \}$
\item $V = \R^2$ e $U = \{ (x, y) \in \R^2 \mid y = -4x \}$
\item $V = M_{3 \times 1}(\R)$, $U = \{ X \in M_{3 \times 1}(\R) \mid AX = 0 \}$, sendo $A = \begin{bmatrix}
1 & 2 & 3 \\4 & 5 & 6
\end{bmatrix}$.
\item $V = M_{2 \times 2}(\R)$ e $U = \{ Z \in M_{2 \times 2}(\R) \mid Z \text{ tem zeros em sua diagonal} \}$
\item $V = M_{2 \times 2}(\R)$ e $U = \{ B \in M_{2 \times 2}(\R) \mid \det(B) \neq 0 \}$
\item $V = \mathcal{F}(\R)$ e $U = \{ f \in \mathcal{F}(\R) \mid f \text{ é um polinômio de grau exatamente igual a 3} \}$
\item $V = \mathcal{F}(\R)$ e $U = \{ f \in \mathcal{F}(\R) \mid f^\prime(-1) = f^\prime(1) = 0 \}$
\end{enumerate}

\item \calc Sejam $A = \begin{bmatrix}
11 & 15 & -1 \\
 3 &  7 & 16
\end{bmatrix}$, $B = \begin{bmatrix}
2 & 11 & -18  \\
1 & -8 &  1
\end{bmatrix}$ e $C = \begin{bmatrix}
-3 & -6 & 6\\
-2 &  4 & 2
\end{bmatrix}$ vetores de $M_{3 \times 2}(\R)$. Resolva a seguinte equação na variável $X \in M_{3 \times 2}(\R)$: $\displaystyle \frac{A-X}{5}-\frac{X+B}{2} = C$.

\item \calc
Seja $U$ o conjunto de todas as funções $q: \R \setminus \{3\} \to \R$, para as quais existem $a \in \R$ e $b \in \R$ tais que $q(x) = \dfrac{ax+b}{(x-3)^2}$, para todo $x\neq 3$. Demonstre as afirmações a seguir:
\begin{enumerate}
\item O conjunto $U$ é um espaço vetorial.
\item As funções $q_1$ e $q_2$, definidas por $q_1(x) = \dfrac{1}{x-3}$ e $q_2(x) = \dfrac{1}{(x-3)^2}$, pertencem a $U$.
\item A função $q_3$, definida por $q_3(x) = \dfrac{-2x+11}{(x-3)^2}$, pertence ao espaço gerado por $\{q_1, q_2\}$.
\item Toda função pertencente a $U$ também pertence ao espaço gerado por $\{q_1, q_2\}$.
\item Toda função pertencente ao espaço gerado por $\{q_1, q_2\}$ também pertence a $U$.
\item O conjunto de funções $B = \{q_1, q_2\}$ é linearmente independente.
\item O conjunto $B$ é uma base de $U$.
\end{enumerate}

\item \calc
Sejam $b_1 = (2,-3, 1)$ e $b_2 = (4,0,0)$ e $b_3 = (0,2,2)$ vetores de $\R^3$. Determine vetores $u,v,w \in \R^3$ tais que
\systeme{
  u  +  2 v - 2w = b_1,
 2u  +  5 v - 5w = b_2,
-2u  -  5 v + 6w = b_3.
}
(\textbf{Dica:} não resolva um sistema linear $9 \times 9$).

\item \calc
Seja $S$ o conjunto das funções $y = f(x)$, de $\R$ em $\R$, que satisfazem a equação diferencial $y^{\prime\prime} - 4y = 0$. Mostre que $S$ é um subespaço do espaço vetorial das funções $f: \R \to \R$.

\item \teoria Prove que os únicos subespaços de $\R$ (com as operações usuais) são o trivial $\{ 0 \}$ e $\R$. (dica: mostre que se $U$ é um subespaço de $\R$ e $U \neq \{0\}$ então $U = \R$)

\item \teoria Seja $U$ um subespaço de um espaço vetorial $V$. Explique por que $W = \{ x \in V \mid x \not \in U \}$ não pode ser um subespaço de $V$.

\item \calc Para cada um dos subespaços vetoriais $U$ e $V$ a seguir, obtenha o subespaço $U \cap V$:
\begin{enumerate}
\item $U = \{ ax^3 + bx^2 + cx + d \in P_3(\R) \mid 3a-b+c+d = 0 \}$ e $V = \{ p(x) \in P_3(\R) \mid p^\prime(-1) = 0 \}$
\item $U = \{ g \in \mathcal{F}(\R) \mid g \text{ é par} \}$ e $V = \{ g \in \mathcal{F}(\R) \mid g \text{ é ímpar} \}$
\item $U = \{ M \in M_{n \times n}(\R) \mid M \text{ é antissimétrica} \}$ e $V = \{ M \in M_{n \times n}(\R) \mid M \text{ é simétrica} \}$
\end{enumerate}

\item \teoria Seja $V$ um espaço vetorial qualquer e $U$ um subespaço de $V$. Prove que para quaisquer vetores $x,y \in V$ vale o seguinte: se $x\in U$ e $y-x \in U$ então $y \in U$.

\item \conceito Dê exemplos de subespaços vetoriais $V_1$ e $V_2$ de $\R^3$ que satisfaçam as seguintes condições (e explique porque satisfazem):
\begin{enumerate}
\item $V_1 \cup V_2$ não é subespaço vetorial de $\R^3$
\item $V_1 \cup V_2$ é subespaço vetorial de $\R^3$
\end{enumerate}

\item \teoria Sejam $V_1$ e $V_2$ quaisquer subespaços de um espaço vetorial $W$ arbitrário. Prove que para $V_1 \cup V_2$ ser subespaço vetorial de $W$ é preciso que $V_1 \subseteq V_2$ ou que $V_2 \subseteq V_1$.

\item \conceito
Explique todas as afirmações verdadeiras a seguir, e dê contraexemplos para as demais.
\begin{enumerate}
\item A interseção de dois subespaços vetoriais de $V$ nunca é vazia.
\item Se $V= \ger{ u_1, u_2, u_3 }$ então $\{ u_1, u_2, u_3 \}$ é uma base de $V$
\item Se $\{u, v\}$ é linearmente dependente (L.D.) então um dos vetores é múltiplo do outro.
\item Se $u \neq 0$ então os vetores $u$ e $-u$ são linearmente independentes (L.I.).
\item Se $\{ u, v \}$ é L.I. e $\{ v, w \}$ é L.I. então $\{ u, v, w \}$ é L.I..
\item Se $\{ u \}$ é L.I. e $\{ v \}$ é L.I. então $\{ u, v \}$ é L.I..
\item É possível que $u \in \ger{v,w}$ sem que ocorra $u \in \ger{v}$ nem $u \in \ger{w}$.
\end{enumerate}

\item \calc É verdade que $\R^4 = \ger{ (1,-1,0,0), (0,0,1,-1), (0,2,1,0), (0,0,1,1)}$? Por que?

\item \calc Sob quais condições o vetor $v = (a,b,c,d) \in \R^4$ é combinação linear dos vetores $w_1 = (1,-1,1,0)$, $w_2 = (2,-3,2,-1)$ e $w_3 = (0,0,-1,0)$?

\item \calc Encontre um conjunto de vetores geradores de um dos seguintes subespaços de $M_{3 \times 3}(\R)$:
\begin{enumerate}
\item $D = \{ X \in M_{3 \times 3}(\R) \mid X \text{ é diagonal } \}$
\item $W = \{ A \in M_{3 \times 3}(\R) \mid A \text{ é antissimétrica } \}$
\item $U = \{ B \in M_{3 \times 3}(\R) \mid B \text{ é simétrica } \}$
\end{enumerate}

\item \calc Verifique se os conjuntos $\{v_1, \ldots, v_n \}$ a seguir são formados por vetores linearmente independentes (L. I.) ou linearmente dependentes (L. D.):
\begin{enumerate}
\item $v_1=(0,3,-9)$, $v_2=(1,4,-10)$, $v_3=(2,5,-11)$ em $\R^3$.
\item $v_1=(0,3,-9)$, $v_2=(1,4,-10)$ em $\R^3$.
\item $v_1 = \begin{bmatrix}
0 & 2 \\
2 & 0
\end{bmatrix}$, $v_2 = \begin{bmatrix}
1 & 0 \\
0 & 4
\end{bmatrix}$, $v_3 = \begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix}$, $v_4 = \begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix}$, $v_5 = \begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}$ em $M_{2\times 2}(\R)$.
\item $v_1 = x^3 + x^2 + x + 1$, $v_2 = x^2 + x + 1$, $v_3 = x + 1$, $v_4 = 1$ no espaço de polinômios $P_4$.
\end{enumerate}

\item \calc Encontre uma base e a dimensão de cada um dos seguintes espaços vetoriais:
\begin{enumerate}
\item $V = \R^4 = \{ (x,y,z,w) \mid x \in \R, y \in \R, z \in \R, w \in \R \}$.
\item $V = M_{2 \times 3}(\R) = \left\{ \begin{bmatrix}
a & b & c \\p & q & r
\end{bmatrix} \mid a \in \R, b \in \R, c \in \R, p \in \R, q \in \R, r \in \R \right\}$.
\item $V = P_5 = \{ p(x) \in \mathcal{F}(\R) \mid p(x) \text{ é um polinômio de grau} \leq 5 \}$.
\end{enumerate}

\item \calc Encontre uma base e a dimensão do subespaço $W$ de $V$ nos seguintes casos:
\begin{enumerate}
\item $V = \R^3$ e $W = \{ (x,y,z) \mid z = 0 \}$ (o plano horizontal que passa pela origem).
\item $V = M_{2 \times 2}(\R)$ e $W = \{ S \in V \mid S^T = S \}$ (as matrizes simétricas $2 \times 2$)
\item $V = \mathcal{F}(\R)$ e $W = \{ p(x) = ax^2+bx+c \in P_2 \mid p^\prime(6) = 0 \}$ (geometricamente, estas funções correspondem a parábolas com vértice em $(6, f(6))$).
\item $V = \R^4$ e $W = \{ (x,y,z,w) \mid x + 2w = 0, -y = -2w, w=z/2 \}$.
\end{enumerate}

\item \calc Encontre um subconjunto de $S = \{ (1,1,1), (1,1,0), (1,0,1), (0,1,1) \}$ que seja base de $\R^3$.

\item \teoria Mostre que $\{ 0, v_1, \ldots, v_n \}$ é um conjunto de vetores linearmente dependente, isto é, todo conjunto de vetores contendo o vetor nulo é linearmente dependente.

\item \teoria Prove que se $W = \ger{ v_1, \ldots, v_n}$ e $z \in W$ então $W = \ger{ v_1, \ldots, v_n, z }$.

\item \teoria Prove que se $S_1 = \{ u_1, u_2, u_3 \}$ é um conjunto de vetores linearmente independentes, então $S_2 = \{ u_1, u_2 \}$ também é um conjunto de vetores linearmente independentes.

\item \teoria Mostre que se $U$ é um subespaço de $V$ então $U=\ger{U}$.

\item \calc Dada uma matriz $A$, denote por $L(A)$ o espaço linha (subespaço gerado pelas linhas de $A$), por $C(A)$ o espaço coluna (subespaço gerado pelas colunas de $A$) e por $N(A)$ o núcleo de $A$ (o espaço das matrizes coluna $X$ que satisfazem $AX = 0$). Encontre uma base e a dimensão de $L(A)$, $C(A)$ e $N(A)$ para as seguintes matrizes:
\begin{enumerate}
\begin{multicols}{2}
\item $A = \begin{bmatrix}
1 & -1 \\
1 & 1
\end{bmatrix}$

\item $A = \begin{bmatrix}
1 & 2 & 3 & 4
\end{bmatrix}$

\item $A = \begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 0 & -1
\end{bmatrix}$

\item $A = \begin{bmatrix}
1 & 0 & 2 & 0 & 3 \\
2 & 0 & 4 & 0 & 6
\end{bmatrix}^T$

\end{multicols}
\end{enumerate}

\item \calc Se $U$ e $V$ são subespaços de um espaço vetorial $W$, denota-se o conjunto de todas as somas de elementos de $U$ com elementos de $V$ por $ U + V = \{ x + y \in W \mid x \in U \text{ e } y \in V \}$.
Esta soma é chamada de \textbf{soma direta}, e denotada por $U \oplus V$, se ocorrer $U \cap V = \{ 0 \}$.

Encontre $U+V$ para os seguintes subespaços $U$ e $V$ e verifique se é uma soma direta:
\begin{enumerate}
\item Em $W = \R^4$, os subespaços $U = \{ (x,y,z,w) \in \R^4 \mid x + y = 0 = z + w \}$ e $V = \{ (a,b,c,d) \in \R^4 \mid b = 2c \}$.

\item Os subespaços $U = \{ M \in M_{3 \times 3}(\R) \mid M^T = -M \}$ das matrizes antissimétricas e $V = \{ N \in M_{3 \times 3}(\R) \mid N^T = N \}$ das matrizes simétricas de $W = M_{3 \times 3} (\R)$.

\item Os subespaços $U = \{ q(x)=b_0 + b_1x+b_2x^2+b_3x^3 \mid q(-x) = -q(x), \forall x \in \R \}$ e $V = \{ p(x)=a_0 + a_1x+a_2x^2+a_3x^3 \mid p(-x) = p(x), \forall x \in \R \}$ do espaço vetorial $P_3$.
\end{enumerate}

\item \calc Encontre uma base e a dimensão de cada uma das somas $U+V$ do exercício anterior.

\item \teoria Mostre que se $u$, $v$ e $w$ são vetores L. I. então os vetores $u+v$, $u-v$ e $u-w$ são L. I.

\item \teoria Mostre que se $B = \{u, v\}$ é uma base de $V$ então $V = \ger{u} \oplus \ger{v}$.

\item \calc Sejam $U = \ger{ (1,0,0), (2,1,1) }$ e $V = \ger{ (0,-1,0), (0,0,1) }$ subespaços de $\R^3$.
\begin{enumerate}
\item Encontre uma base e a dimensão de $U \cap V$.
\item Verifique se $\R^3 = U + V$ e se $\R^3 = U \oplus V$.
\end{enumerate}

\item \teoria Mostre que se $B = \{u, v\}$ é uma base de $V$ então $B^\prime = \{ u+v, u-v \}$ também é base.

\item \teoria Mostre que para qualquer espaço vetorial $V$, é verdade que $V=V+V$ e $V={0}\oplus V$.
\end{enumerate}

\newpage
\section*{Respostas}

\begin{enumerate}
\item Todos os axiomas de espaço vetorial devem ser checados individualmente. A comutatividade da adição, por exemplo, seria verificada como segue:
\begin{enumerate}
\item
$(x_1, \ldots, x_n) + (y_1, \ldots, y_n)
= (x_1 + y_1, \ldots, x_n + y_n) \\
= (y_1 + x_1, \ldots, y_n + x_n)
= (y_1, \ldots, y_n) + (x_1, \ldots, x_n)$.

O vetor nulo é a sequência com as $n$ coordenadas iguais a zero: $0 = (0, \ldots, 0)$.

\item
$(A + B)_{ij}
= (A)_{ij} + (B)_{ij}
= (B)_{ij} + (A)_{ij}
= (B + A)_{ij}$.

O vetor nulo é a matriz nula $0 = 0_{m \times n}$ em que $0_{ij} = 0, \forall i,j$.

\item
$(f + g)(x)
= f(x) + g(x)
= g(x) + f(x)
= (g + f)(x), \forall x \in \R$.

O vetor nulo é a função constante igual a zero: $f(x) = 0, \forall x \in \R$.

\item
$(a + b \ii)+ (c + d \ii)
= (a + c) + (b + d) \ii
= (c + a) + (d + b) \ii
= (c + d \ii) + (a + b \ii)$.

O vetor nulo é o número complexo (e, de fato, real) $0 = 0 + 0 \ii$.

\item
$(x_1, x_2, \ldots) + (y_1, y_2, \ldots)
= (x_1 + y_1, x_2 + y_2, \ldots) \\
= (y_1 + x_1, y_2 + x_2, \ldots)
= (y_1, y_2, \ldots) + (x_1, x_2, \ldots)$.

O vetor nulo é a sequência infinita com todas as posições iguais a zero: $0 = (0, 0, \ldots)$.

\end{enumerate}

\item
\begin{enumerate}
\item $\R$ é um espaço vetorial com as operações indicadas. O vetor nulo é $\vec{0} = -1$ e o oposto do vetor $v = x$ é o vetor $-v = -x-2$.
\item $\R^2$ não é espaço vetorial com as operações indicadas, pois
\[1 \cdot(x,y) = (-3x+y,5x-2y) \neq (x,y), \text{ para vários valores de }x \text{ e }y.\]
\item $\R^2$ é um espaço vetorial com as operações indicadas. O vetor nulo é $\vec{0} = (-1, 2)$ e o vetor oposto de $v = (x,y)$ é o vetor $-v = (-x-2, -y+4)$.
\end{enumerate}


\item \begin{enumerate}
\item Basta observar que pela propriedade comutativa da adição, tem-se $u + (-u) = -u + u$ e que por definição de elemento oposto, $-u + u = \vec{0}$. Logo, $u + (-u) = \vec{0}$ e $-(-u) = u$.
\item Como $((-a) u) + (a u)
= ((-a) + a) u
= 0 u
= \vec{0}$, conclui-se que $-(a u) = (-a) u$.
\item Isso resulta das seguintes igualdades: $(a (-u)) + (a u)
= a((-u) + u)
= a \vec{0}
= \vec{0}$.
\item Como o oposto do vetor $u+v$ é único, basta mostrar que $(-u) + (-v)$ satisfaz a propriedade que caracteriza o vetor oposto, ou seja, que
\begin{align*}
  ((-u) + (-v)) + (u+v)
& = -u + (-v + (u+v))
  = -u + (-v + (v+u))\\
& = -u + ((-v + v)+u)
  = -u + (\vec{0}+u)
  = -u + (u+\vec{0})\\
& = -u + u
  = \vec{0}.
\end{align*}
\end{enumerate}

\item
\begin{enumerate}
\item Se $u = (a,b,c,d) \in U$ e $v = (p,q,r,s) \in U$, então $a-b = c+d$ e $p-q = r+s$. Assim, como $u + v = (a+p,b+q,c+r,d+s)$ tem-se
\[
(a+p) - (b+q)
= (a-b) + (p-q)
= (c+d) + (r+s)
= (c+r) + (d+s),
\]
ou seja, $u + v \in \R$. Do mesmo modo, se $k \in \R$, tem-se $k u = (ka, kb, kc, kd)$ e
\[
ka - kb
= k(a-b)
= k(c+d)
= kc+kd,
\]
ou seja, $ku \in \R$. Note ainda que o vetor $0 = (0,0,0,0) \in U$.
\item $W = \{ (a,b,c,d) \in \R^4 \mid c = d = 0 \}$.
Dados $(a,b,0,0) \in W$ e $(c,d,0,0) \in W$, tem-se
\begin{align*}
(a,b,0,0) + (c,d,0,0)
& = (a+c,b+d,0+0,0+0)
  = (a+c,b+d,0,0)
\in W \text{ e} \\
 k( a,  b, 0, 0)
& = (ka, kb, k0, k0)
  = (ka, kb, 0, 0)
\in W
\end{align*}
\end{enumerate}
\item
\begin{enumerate}
\item Se $X$ e $Y$ são matrizes simétricas então $X^T = X$ e $Y^T = Y$. Consequentemente, $(X+Y)^T = X^T+Y^T = X + Y$, ou seja, $X+Y$ é simétrica. Dado $k \in \R$, $(kX)^T = k(X^T) = kX$, ou seja, $kX$ é simétrica. Note que a matriz nula também é simétrica.

\item Se $X$ e $Y$ são matrizes antissimétricas então $X^T = -X$ e $Y^T = -Y$. Consequentemente, $(X+Y)^T = X^T+Y^T = -X + (-Y) = -(X+Y)$, ou seja, $X+Y$ é antissimétrica. Dado $k \in \R$, $(kX)^T = k(X^T) = k(-X) = -(kX)$, ou seja, $kX$ é antissimétrica. Note que a matriz nula também é antissimétrica.
\item Se $X$ e $Y$ comutam com $B$, então $BX = XB$ e $BY = YB$. Consequentemente, $B(X+Y) = BX+BY = XB + YB = (X+Y)B$, ou seja, $X+Y$ comuta com $B$. Dado $k \in \R$, $B(kX) = k(BX) = k(XB) = (kX)B$, ou seja, $kX$ comuta com $B$. Note que $B0 = 0B = 0$, isto é, a matriz nula também comuta com $B$.

\item
$U = \{ A = (a_{ij}) \in M_{3 \times 3}(\R) \mid a_{ij} = 0 \text{ sempre que } i + j \neq 4 \}$.
A condição que define o conjunto $U$ indica que para toda matriz $A \in U$ é da forma $A = \begin{bmatrix}
     0 &      0 & a_{13}\\
     0 & a_{22} & 0     \\
a_{31} &      0 & 0
\end{bmatrix}$, pois $a_{11} = a_{12} = a_{21} = a_{23} = a_{32} = a_{33} = 0$ (em particular, a matriz nula $3 \times 3$ está em $U$). Além disso, ao somar $A \in U$ com $B \in U$, as entradas $[A+B]_{ij}$ em que $i+j \neq 4$ serão iguais a zero pois
\[[A+B]_{ij} = a_{ij} + b_{ij} = 0 + 0 = 0.\]
Logo, $A+B \in U$, e este é um conjunto fechado para a adição. Em relação à multiplicação por um escalar $k \in \R$, tem-se:
\[[kA]_{ij} = ka_{ij} = k0 = 0,\]
ou seja, $kA \in U$ (a multiplicação por escalar é fechada em $U$), e conclui-se que $U$ é um subespaço de $M_{3 \times 3} (\R)$.
\end{enumerate}
\item
\begin{enumerate}
\item
Se $p$ e $q$ são polinômios e $c \in \R$, a soma $p+q$ e o produto $c \cdot p$ também são polinômios, logo $p + q \in P_\infty$ e $cp \in P_\infty$. Mais precisamente, se $p(x) = a_n x^n + \ldots + a_1 x^1 + a_0$ e $q(x) = b_m x^m + \ldots + b_1 x^1 + b_0$, com $m, n \in \N$ e $a_i,b_i \in \R$ então $p + q$ é dado por
\begin{align*}
(p + q)(x)
= p(x)+q(x)
& = (a_n x^n + \ldots + a_1 x^1 + a_0)
  + (b_m x^m + \ldots + b_1 x^1 + b_0) \\
& = (a_k + b_k) x^k + \ldots + (a_1+b_1) x^1 + (a_0+b_0) \in P_\infty,
\end{align*}
onde $k$ é o maior dos graus $m$ e $n$. De forma análoga,
\begin{align*}
(c \cdot p)(x)
= c \cdot p(x)
& = c(a_n x^n + \ldots + a_1 x^1 + a_0) \\
& = (ca_n) x^n + \ldots + (ca_1) x^1 + (ca_0) \in P_\infty.
\end{align*}

Perceba que o polinômio nulo ($p(x) = 0$) é o vetor nulo neste espaço vetorial.

\item Dado um natural $n$, o conjunto $P_n$ de todos polinômios de grau menor ou igual a $n$:
\[
P_n = \{ p(x) \in P_\infty \mid \operatorname{grau}(p(x)) \leq n \}.
\]

Pelo exercício anterior, a soma de polinômios resulta em um polinômio, e a multiplicação de um polinômio por um escalar real qualquer também é um polinômio. Levando isso em conta, para ver que $P_n$ é um subespaço vetorial de $\mathcal{F}(\R)$ basta lembrar que se $p, q \in P_n$ então:
\begin{itemize}
\item $\operatorname{grau}(p+q) < \operatorname{grau}(p) + \operatorname{grau}(q)$, se os termos de maior grau são opostos
\item $\operatorname{grau}(p+q) = \operatorname{grau}(p) + \operatorname{grau}(q)$, nos demais casos
\item $\operatorname{grau}(c \cdot p) = \operatorname{grau}(p)$ se $c \neq 0$;
\item $\operatorname{grau}(c \cdot p) = \operatorname{grau}(0) < \operatorname{grau}(p), \text{ se } c = 0$
\end{itemize}
Este último item também mostra que o polinômio nulo é um elemento de $P_n$.
\item Se $f$ e $g$ são funções pares, então para todo $x \in \R$ tem-se
\[
(f+g)(-x)
= f(-x) + g(-x)
= f(x) + g(x)
= (f + g)(x),
\]
ou seja, $f+g$ também é par. Analogamente, dado qualquer $c \in \R$,
\[
(cf)(-x)
= cf(-x)
= cf(x)
= (cf)(x),
\]
e portanto $cf$ também é par, e o conjunto dado é um subespaço de $\mathcal{F}(\R)$. A função constante igual a zero, que é o vetor nulo deste espaço, é uma função par.

\item Se $f$ e $g$ são funções ímpares, então para todo $x \in \R$ tem-se
\[
(f+g)(-x)
= f(-x) + g(-x)
= (-f(x)) + (-g(x))
= -(f(x) + g(x))
= -(f + g)(x),
\]
ou seja, $f+g$ também é ímpar. Analogamente, dado qualquer $c \in \R$,
\[
(cf)(-x)
= cf(-x)
= c(-f(x))
= -c(f(x))
= -(cf)(x),
\]
e portanto $cf$ também é ímpar e o conjunto é fechado para ambas as operações do espaço vetorial, ou seja, é um subespaço de $\mathcal{F}(\R)$. A função constante igual a zero também é uma função ímpar.

\item Como a soma de funções contínuas é contínua e a multiplicação de funções contínuas por constantes resulta em funções contínuas, conclui-se que $C^0(\R)$ é fechado para ambas as operações do espaço vetorial, e consequentemente é um subespaço de $\mathcal{F}(\R)$.

\item Sejam $f,g \in C^1(\R)$ e $c \in \R$. Então:
\begin{itemize}
\item As funções $f$ e $g$ são deriváveis e suas derivadas são contínuas. Como a soma de funções deriváveis é uma função derivável, $f+g$ também é derivável e vale $(f+g)' = f^\prime + g^\prime$. Além disso, pelo exercício anterior $f^\prime+g^\prime$ é contínua, pois é soma de funções contínuas. Logo, $f+g$ é uma função derivável cuja derivada é contínua, ou seja, $f+g \in C^1(\R)$.
\item Sendo $f$ derivável, a função $cf$ também é derivável e $(cf)^\prime = c (f^\prime)$. Mas $f^\prime$ é contínua, então $c (f^\prime)$ também é (pelo exercício anterior), ou seja, $cf \in C^1(\R)$.
\end{itemize}
Como as duas operações são fechadas, segue que $C^\prime(\R)$ é um subespaço de $\mathcal{F}(\R)$.
\end{enumerate}

\item
\begin{enumerate}
\item Como $(0,0)$ é o vetor nulo de $\R^2$ e $(0,0) \not\in U$, pois $0+2\cdot 0 \neq 5$, este conjunto $U$ não é um subespaço vetorial de $\R^2$ (geometricamente, é uma reta que não passa pela origem). Note que a adição e a multiplicação por escalar não são fechadas em $U$. Por exemplo, $(0,5)+(3,-1)=(3,4) \not\in U$ pois $3+2\cdot (-1) \neq 5$.

\item Sejam $(a,b) \in U$, $(c,d) \in U$ e $k \in \R$. Então
\begin{itemize}
\item $(a,b)+(c,d) = (a+c,b+d) \in U$ pois $b+d=(-4a)+(-4c)=-4(a+c)$.
\item $k(a,b) = (ka,kb) \in U$ pois $kb=k(-4a)=-4(ka)$.
\end{itemize}
Portanto, $U$ é um subespaço vetorial de $\R^2$ (é a reta que passa pela origem e tem a direção do vetor $(1,-4)$)

\item $U = \{ X \in M_{3 \times 1}(\R) \mid AX = 0 \}$ é um subespaço vetorial de $V = M_{3 \times 1}(\R)$ pois
\begin{itemize}
\item $A0 = 0$, ou seja, a matriz nula $3 \times 1$ pertence a $U$.
\item Se $AX_1=0$ e $AX_2=0$ então $A(X_1+X_2)=AX_1 + AX_2 = 0 + 0$.
\item Se $AX=0$ e $t \in \R$ então $A(tX)=t(AX) = t0 = 0$.
\end{itemize}
Em outras palavras, $U$ é fechado para as operações do espaço vetorial $V$.

Note que as propriedades acima são válidas independentemente de quais sejam os números que aparecem na matriz $A$, então não é necessário expandir os produtos em termos das entradas das matrizes. De fato, $U = N(A)$, isto é, é o espaço nulo de $A$.

\item Note que $U = \left\{ \begin{bmatrix}
0 & z_{12}\\ z_{21} & 0
\end{bmatrix} \mid z_{12},z_{21} \in \R \right\}$ e que para todo $X, Y \in U$ e $m \in \R$ tem-se:
\begin{itemize}
\item $X + Y = \begin{bmatrix}
0 & x_{12}\\ x_{21} & 0
\end{bmatrix} + \begin{bmatrix}
0 & y_{12}\\ y_{21} & 0
\end{bmatrix}
=\begin{bmatrix}
0 & x_{12} + y_{12}\\ x_{21}+y_{21} & 0
\end{bmatrix} \in U$
\item $mX = m\begin{bmatrix}
0 & x_{12}\\ x_{21} & 0
\end{bmatrix}
=\begin{bmatrix}
0 & mx_{12} \\ mx_{21} & 0
\end{bmatrix} \in U$
\end{itemize}

Portanto, $U$ é um subespaço vetorial de $V$.

\item O conjunto $U$ consiste das matrizes $2 \times 2$ que são inversíveis. Como $I$ e $-I$ são matrizes inversíveis ($\det(I) = \det(-I) = 1 \neq 0$), e sua soma $I + (-I) = 0$ não é inversível (pois $\det(0) = 0$), conclui-se que $U$ não é fechado para a adição, e como tal não é um subespaço de $V$. A multiplicação por escalar também não é fechada em $U$, pois a ao multiplicar uma matriz inversível por zero o resultado é a matriz nula, que não pertence a $U$.
\item Os polinômios de grau 3 não formam um espalo vetorial pois, por exemplo, se $p(x) = 4x^3+5x$ e $q(x) = -4x^3$ então $p(x)+q(x) = 5x$ que  não tem grau 3.

\item Sejam $f,g \in U$. Então $f^\prime(-1) = f^\prime(1) = 0$ e $g^\prime(-1) = g^\prime(1) = 0$. Consequentemente:
\begin{itemize}
\item $(f+g)^\prime(1) = f^\prime(1) + g^\prime(1) = 0 + 0 = 0$
\item $(f+g)^\prime(-1) = f^\prime(-1) + g^\prime(-1) = 0 + 0 = 0$
\end{itemize}
Isto significa que a adição é fechada em $U$. De forma análoga, para todo $c \in \R$ vale:
\begin{itemize}
\item $(cf)^\prime(1) = c(f^\prime(1)) = c \cdot 0 = 0$
\item $(cf)^\prime(-1) = c(f^\prime(-1)) = c \cdot 0 = 0$
\end{itemize}
Isto significa que a multiplicação por escalar também é fechada, e assim $U$ é um subespaço vetorial de $\mathcal{F}(\R)$.
\end{enumerate}

\item Sabendo que $A,B$ e $X$ são elementos do espaço vetorial $M_{3 \times 2}$, pode-se utilizar as propriedades deste espaço (associatividade, comutatividade, etc) para ``isolar a incógnita $X$'', de forma análoga ao que seria feito se $A,B$ e $X$ fossem números reais:
\begin{align*}
\frac{A-X}{5}-\frac{X+B}{2} = C
& \Leftrightarrow
10\left(\frac{A-X}{5}-\frac{X+B}{2}\right) = 10 C
\Leftrightarrow
2(A-X)-5(X+B) = 10 C\\
& \Leftrightarrow
2A - 2X - 5X - 5B = 10 C
\Leftrightarrow
-7X = -2A + 10C + 5B\\
& \Leftrightarrow
X = \frac{1}{7}(2A - 10C - 5B)
\end{align*}
Assim, $X = \frac{1}{7}\left(2\begin{bmatrix}
11 & 15 & -1 \\
 3 &  7 & 16
\end{bmatrix} - 10\begin{bmatrix}
-3 & -6 & 6\\
-2 &  4 & 2
\end{bmatrix} - 5\begin{bmatrix}
2 & 11 & -18  \\
1 & -8 &  1
\end{bmatrix}\right)
=\begin{bmatrix}
6 & 5 & 4  \\
3 & 2 & 1
\end{bmatrix}$

\item
\begin{enumerate}
\item Considerando que, para qualquer conjunto $A$, as funções $f:A \to \R$ formam um espaço vetorial com as operações usuais, basta considerar $A = \R \setminus \{3\}$ e mostrar que $U$ é um subespaço vetorial desse espaço vetorial. Para isso, observe que
\begin{enumerate}
\item Se $q_1 \in U$ e $q_2 \in U$ são definidas por $q_1(x) = \dfrac{a_1x+b_1}{(x-3)^2}$ e $q_2(x) = \dfrac{a_2x+b_2}{(x-3)^2}$, para todo $x\neq 3$, então $q_1+q_2$ é dada por
\[
(q_1+q_2)(x)
= q_1(x) + q_2(x)
= \dfrac{a_1x+b_1}{(x-3)^2}
 +\dfrac{a_2x+b_2}{(x-3)^2}
= \dfrac{(a_1+a_2)x+(b_1+b_2)}{(x-3)^2}, x\neq 3
\]
\item Se $k \in \R$ e $q_1 \in U$ é definida por $q_1(x) = \dfrac{a_1x+b_1}{(x-3)^2}$, para todo $x\neq 3$, então $k q_1$ é dada por
\[
(kq_1)(x)
= kq_1(x)
= k \dfrac{a_1x+b_1}{(x-3)^2}
= \dfrac{(ka_1)x+(kb_1)}{(x-3)^2}, x\neq 3.
\]
\end{enumerate}
Portanto, $q_1+q_2 \in U$ e $k q_1\in U$ e o conjunto $U$ é um (sub)espaço vetorial.
\item Basta observar que
$
q_1(x) = \dfrac{1}{x-3}
= \dfrac{1x+(-3)}{(x-3)^2}
$
e
$
q_2(x) = \dfrac{1}{(x-3)^2}
=\dfrac{0x+1}{(x-3)^2}
$.
\item Para que $q_3$ esteja no espaço gerado por $\{q_1, q_2\}$, devem existir escalares $m_1$ e $m_2$ tais que $q_3 = m_1 q_1 + m_2q_2$, isto é,
\begin{align*}
\dfrac{-2x+11}{(x-3)^2}
& = m_1 \dfrac{1}{x-3}
  + m_2 \dfrac{1}{(x-3)^2}\\
& = \dfrac{m_1(x-3)+m_2}{(x-3)^2},\forall x\neq3
\end{align*}
Assim, deve ocorrer $-2x+11 = m_1x + (-3m_1+m_2)$, ou seja, $m_1 = -2$ e $-3m_1+m_2=11$, o que é possível para $m_1 = -2$ e $m_2=5$. Logo, $q_3 \in \ger{q_1,q_2}$.

\item Seja $q \in U$. Então $q(x) = \dfrac{ax+b}{(x-3)^2}$, para todo $x \neq 3$. Usando a decomposição em frações parciais, obtém-se:
\[
q(x)
= \dfrac{ax+b}{(x-3)^2}
= \dfrac{a}{x-3}
+ \dfrac{3a+b}{(x-3)^2}
= a\dfrac{1}{x-3}
+ (3a+b)\dfrac{1}{(x-3)^2}.
\]
Logo, $q = aq_1+(3a+b)q_2 \in \ger{q_1, q_2}$.
\item Seja $q \in \ger{q_1, q_2}$. Então existem $k_1 \in \R$ e $k_2 \in \R$ tais que $q = k_1 q_1 + k_2q_2$. Assim, para todo $x \neq 3$ tem-se:
\[
q(x)
= k_1\dfrac{1}{x-3}
+ k_2\dfrac{1}{(x-3)^2}
= \dfrac{k_1(x-3) + k_2}{(x-3)^2}
= \dfrac{k_1x+(-3k_1+ k_2)}{(x-3)^2}
= \dfrac{a^\prime x+b^\prime}{(x-3)^2}
\]
Logo, $q \in U$.

\item Se $c_1 \in \R$ e $c_2 \in \R$ são tais que  $c_1 q_1 + c_2q_2 = \vec{0}$ então, para todo $x\neq 3$, tem-se:
$0 = c_1 \dfrac{1}{x-3} + c_2 \dfrac{1}{(x-3)^2}
   = \dfrac{c_1x+(-3c_1+c_2)}{(x-3)^2}$.
Logo, $c_1x+(-3c_1 + c_2) = 0x+0$, o que só é possível se $c_1=0$ e $-3c_1 + c_2 = 0 + c_2 = 0$. Portanto, $\{q_1,q_2\}$ é linearmente independente.
\item Como $B$ gera $U$ e é linearmente independente, conclui-se que $B$ é uma base de $U$.
\end{enumerate}
\item Usando as operações de adição e multiplicação por escalar, é possível multiplicar os vetores de ambos os membros de uma equação por uma constante (não nula), e também somar o resultado com os vetores em cada membro de outra equação, obtendo equações equivalentes às que foram dadas. Isso é permite que a matriz ampliada seja escalonada:
\[
\begin{bmatrix}
 1 &  2 & -2 & b_1\\
 2 &  5 & -5 & b_2\\
-2 & -5 &  6 & b_3
\end{bmatrix}
\rightarrow
\cdots
\rightarrow
\begin{bmatrix}
 1 & 0 & 0 & 5b_1 - 2b_2\\
 0 & 1 & 0 & -2b_1+2b_2+b_3\\
 0 & 0 & 1 & b_2+b_3
\end{bmatrix}.
\]
Assim, as equações originais são equivalentes às seguintes:
\begin{align*}
u &= 5b_1 - 2b_2
   = 5(2,-3,1) - 2(4,0,0)
   = (2,-15,5),\\
v &= -2b_1+2b_2+b_3
   = -2(2,-3,1)+2(4,0,0)+(0,2,2)
   = (4,8,0),\\
w &= b_2+b_3
   = (4,0,0)+(0,2,2)
   = (4,2,2).
\end{align*}

\item Observe que a função nula satisfaz $0^{\prime\prime}-4\cdot 0 = 0 - 4\cdot 0 = 0$, logo, $S \neq \emptyset$. Além disso, se $y_1 \in S$, $y_2 \in S$ e $k \in \R$, então:
\[
  (y_1 + y_2)^{\prime\prime}-4(y_1+y_2)
= y_1^{\prime\prime} + y_2^{\prime\prime}-4y_1-4y_2
= (y_1^{\prime\prime}-4y_1)
 -(y_2^{\prime\prime}-4y_2)
= 0 + 0 = 0
\]
e
\[
  (k y_1)^{\prime\prime}-4(k y_1)
= k (y_1^{\prime\prime})-k(4y_1)
= k (y_1^{\prime\prime}-4y_1)
= k 0 = 0
\]
Assim, $y_1+y_2 \in S$ e $ky_1 \in S$, o que implica que $S$ é um subespaço vetorial.

\item Se $U$ é um subespaço de $\R$ e $U \neq \{0\}$ então $U$ tem algum elemento não nulo. Denote-o por $b$. Para ver que todo número real $x$ também tem que estar em $U$ basta escrever $x = (\frac{x}{b}) \cdot b$, pois sendo $U$ um subespaço vetorial de $\R$ ele é fechado para a multiplicação por escalar e portanto a multiplicação de $b \in U$ pelo escalar $\frac{x}{b}$ deve resultar em um elemento de $U$, isto é, $x \in U$. Portanto $U = \R$.

\item O conjunto $W = \{ x \in V \mid x \not \in U \}$ não é um subespaço de $V$ pois $0 \in U$ mas $0 \not \in W$.

\item
\begin{enumerate}
\item Primeiramente lembre-se que se $p(x) = ax^3 + bx^2 + cx + d$ então $p^\prime(x) = 3ax^2 + 2bx + c$ e $p^\prime(-1) = 3a - 2b + c$. Assim,
\[
U \cap V = \{ ax^3 + bx^2 + cx + d \in P_3(\R) \mid
3a -  b + c + d = 0 \text{ e }
3a - 2b + c     = 0 \}.
\]
Em outras palavras, os coeficientes de $p(x)$ são soluções do sistema linear homogêneo
\systeme{
3a -  b + c + d = 0,
3a - 2b + c     = 0
}

Escalonando a matriz associada ao sistema, obtem-se:
\begin{align*}
&
\begin{bmatrix}
3 & -1 & 1 & 1\\
3 & -2 & 1 & 0
\end{bmatrix}
\grstep{ \frac{1}{3} L_1 }
\begin{bmatrix}
1 & -1/3 & 1/3 & 1/3 \\
3 & -2 & 1 & 0
\end{bmatrix}
\grstep{ L_2 - 3L_1 }
\begin{bmatrix}
1 & -1/3 & 1/3 & 1/3 \\
0 & -1 & 0 & -1
\end{bmatrix}\\
\grstep{ -L_2 }
&
\begin{bmatrix}
1 & -1/3 & 1/3 & 1/3 \\
0 & 1 & 0 & 1
\end{bmatrix}
\grstep{ L_1 +\frac{1}{3}L_2 }
\begin{bmatrix}
1 & 0 & 1/3 & 2/3 \\
0 & 1 & 0 & 1
\end{bmatrix}
\end{align*}
ou seja, $a = \frac{1}{3} (-c-2 d)$ e $b = -d$, em que $c,d \in \R$ são variáveis livres. Assim,
\begin{align*}
U \cap V
& = \left\{ ax^3 + bx^2 + cx + d \in P_3(\R) \mid
a = \frac{-c-2 d}{3}
\text{ e }
b = -d, \text{ sendo } c,d \in \R \right\} \\
& = \left\{ \left(\frac{-c-2 d}{3}\right)x^3 + (-d)x^2 + cx + d \mid c,d \in \R \right\}.
\end{align*}

\item Se $g \in U$ então para todo $x \in \R$ ocorre $g(-x) = g(x)$ e se, além disso, $g \in V$ então para todo $x \in \R$ vale $g(-x) = -g(x)$. Neste caso, para todo $x \in \R$ tem-se $g(x) = g(-x) = -g(x)$, ou seja $2g(x) = 0$ e portanto $g(x) = 0$.

Assim, a única função que está simultaneamente em $U$ e em $V$ é a função constante igual a zero, que é o vetor nulo de $\mathcal{F}(\R)$, isto é, $U \cap V = \{ 0 \}$.

\item Se $M \in U$ então $M^T = M$ e se, além disso, $M \in V$ então $M^T = -M$. Neste caso, tem-se $M = M^T = -M$, ou seja $2M = 0$ e portanto $M = 0$.

Assim, a única matriz que é simétrica e antissimétrica é a matriz nula, ou seja, $U \cap V = \{ 0 \}$.
\end{enumerate}


\item Para quaisquer $x, y \in V$ tem-se $y = (y - x) + x$. Então se $x \in U$ e $y-x \in U$ e $U$ é um subespaço de $V$, então $U$ é fechado para a adição, e consequentemente $y = (y - x) + x \in U$.

\item
\begin{enumerate}
\item Sejam
\begin{itemize}
\item $V_1 = \{ (x,y,z) \in \R^3 \mid z=0 \}$
\item $V_2 = \{ (x,y,z) \in \R^3 \mid x=0 \}$
\end{itemize}
Então o conjunto $V_1 \cup V_2 = \{ (x,y,z) \in \R^3 \mid z=0 \text{ ou } x=0 \}$ não é um subespaço de $\R^3$ pois $(1,1,0) \in V_1 \cup V_2$ e $(0,1,1) \in V_1 \cup V_2$ mas $(1,1,0) + (0,1,1) = (1,2,1) \not \in V_1 \cup V_2$.

\item Sejam
\begin{itemize}
\item $V_1 = \R^3$
\item $V_2 = \{ (0,0,0) \}$
\end{itemize}
Então $V_1 \cup V_2 = \R^3 \cup \{ (0,0,0) \} = \R^3$, que é, naturalmente, um subespaço de $\R^3$.

\end{enumerate}


\item Suponha que $V_1$ e $V_2$ são subespaços de $W$ tais que $V_1 \cup V_2$ também é subespaço de $W$. Se $V_1$ não estiver contido em $V_2$, é porque existe algum vetor $x_1 \in V_1$ que não pertence a $V_2$, e pode-se deduzir que $V_2 \subseteq V_1$ da seguinte forma: dado um vetor $x_2 \in V_2$ arbitrário, tanto $x_1$ quanto $x_2$ são elementos de $V_1 \cup V_2$. Assumindo que esta união seja um subespaço, a soma $x_1 + x_2$ deve pertencer a $V_1 \cup V_2$. Ou seja, $x_1 + x_2$ deve ser elemento de $V_1$ ou de $V_2$:
\begin{enumerate}
\item Para que $x_1 + x_2$ fosse elemento de $V_2$ seria preciso que $x_1 = (x_1 + x_2) - x_2 \in V_2$ (pois $V_2$ é subespaço). Mas foi assumido que $x_1 \not \in V_2$, então só resta a outra possibilidade;
\item Sendo $x_1 + x_2$ um elemento de $V_1$, resulta que $x_2 = (x_1 + x_2) - x_1 \in V_1$.
\end{enumerate}
Em outras palavras, todo elemento $x_2$ de $V_2$ tem que pertencer a $V_1$, isto é, $V_2 \subseteq V_1$.

%ANTIGA LISTA 3, DE 2016

\item
\begin{enumerate}
\item \textbf{Verdadeira}. Como todo subespaço vetorial deve conter o vetor nulo, a interseção de dois subespaços vetoriais quaisquer tem pelo menos um elemento, ou seja, não é vazia.
\item \textbf{Falsa}. Mesmo que $V$ seja o espaço gerado por ${ u_1, u_2, u_3 }$, pode ser que um destes vetores seja combinação linear dos demais. Neste caso, os vetores são linearmente dependentes, e não são uma base de $V$. Por exemplo: $\R^2 = \ger{(1,0),(0,1),(2,3)}$, mas $(1,1) = 2(1,0)+3(0,1)$, ou seja, estes três vetores geram $\R^2$ mas não são uma base.
\item \textbf{Verdadeira}. Se $\{u, v\}$ é linearmente dependente, então existem $a,b \in \R$ tais que $au+bv = 0$, sem que ambos os coeficientes precisem ser nulos. Se, por exemplo, $a \neq 0$, então pode-se escrever $u = (-\frac{b}{a}) v$. E se $b \neq 0$, então $v = (-\frac{a}{b}) u$. Em ambos os casos, um dos vetores pode ser visto como um múltiplo do outro.

\item \textbf{Falso}. Os vetores $u$ e $-u$ nunca são linearmente independentes, pois um deles é múltiplo do outro, e pode-se escrever $au + b(-u) = 0$ com os escalares não nulos $a=1$ e $b=1$.

\item \textbf{Falso}. Mesmo que $\{u, v\}$ sejam L.I. e que $\{ v, w \}$ seja L.I., pode acontecer de $\{u, v, w \}$ ser L.D.. Por exemplo, em $\R^2$, se $u = (1,0)$, $v = (0,1)$ e $w = (1, 1)$ então $w = u + v$, ou seja, há uma combinação linear $1u+1v+(-1)w$ que resulta no vetor nulo sem que todos os coeficientes sejam nulos.
\item \textbf{Falsa}. Apesar de, separadamente, os vetores serem L.I., pode ocorrer que o conjunto contendo ambos seja L.D. Por exemplo, em $\R^3$, o conjunto $\{ (1,0,0) \}$ é linearmente independente, e o conjunto $\{ (5,0,0) \}$ também. No entanto, como $(5,0,0) = 5 (1,0,0)$, o conjunto $\{ (1,0,0), (5,0,0) \}$ é linearmente dependente.
\item \textbf{Verdadeira}. Em $\R^2$, considerando-se $u=(2,3)$, $v = (1,0)$ e $w = (0,1)$, observa-se que $u = 2(1,0)+3(0,1) = 2u+3v \in \ger{v,w}$, mas $u \not \in \ger{v}$ e $u \not \in \ger{w}$.
\end{enumerate}

\item Para que $\R^4 = \ger{u,v,x,z}$, com $u=(1,-1,0,0)$, $v=(0,0,1,-1)$, $x=(0,2,1,0)$ e $z=(0,0,1,1)$ é necessário e suficiente que qualquer $(a,b,c,d)$ possa ser escrito como uma combinação linear dos vetores $u,v,x,z$, isto é, que dado $(a,b,c,d) \in \R^4$ exista alguma solução para a equação
\[
(a,b,c,d) = t_1 (1,-1,0,0) + t_2 (0,0,1,-1)+ t_3 (0,2,1,0)+ t_4 (0,0,1,1),
\]
que é equivalente a um sistema linear nas variáveis $t_1, t_2, t_3$ e $t_4$:

\systeme[t_1,t_2,t_3,t_4]{
 t_1                    = a,
-t_1       + 2t_3       = b,
       t_2 +  t_3 + t_4 = c,
      -t_2        + t_4 = d
}

ou ainda, à equação matricial
\[
\begin{bmatrix}
 1 &  0 & 0 & 0 \\
-1 &  0 & 2 & 0 \\
 0 &  1 & 1 & 1 \\
 0 & -1 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
t_1 \\
t_2 \\
t_3 \\
t_4
\end{bmatrix}
=
\begin{bmatrix}
a \\
b \\
c \\
d
\end{bmatrix}.
\]
Note que esta matriz poderia ser construída diretamente a partir dos vetores, copiando-se as coordenadas de $u$, $v$, $x$ e $z$ para as colunas da matriz. Como a matriz é quadrada, o sistema terá solução se, e somente se, ela for inversível, o que será verdade se o determinante da matriz for diferente de zero. Calculando-o pela primeira linha, obtém-se:
\[
\begin{vmatrix}
 1 &  0 & 0 & 0 \\
-1 &  0 & 2 & 0 \\
 0 &  1 & 1 & 1 \\
 0 & -1 & 0 & 1
\end{vmatrix}
=
1 \cdot
\begin{vmatrix}
 0 & 2 & 0 \\
 1 & 1 & 1 \\
-1 & 0 & 1
\end{vmatrix}
=
-2 \cdot
\begin{vmatrix}
 1 & 1 \\
-1 & 1
\end{vmatrix}
= -2(1 \cdot 1 - (-1) \cdot 1)
=-4 \neq 0.
\]
Então, de fato, qualquer $(a,b,c,d)$ é combinação linear dos quatro vetores dados, isto é, $\R^4 = \ger{u,v,x,z}$.

\item Seja $v = (a,b,c,d) \in \R^4$. Para que $v$ seja combinação linear de $w_1$, $w_2$ e $w_3$, devem existir escalares $c_1$, $c_2$ e $c_3$ tais que $v = c_1w_1 + c_2w_2 + c_3w_3$, ou seja,
\[
(a,b,c,d) = c_1(1,-1,1,0) + c_2(2,-3,2,-1) + c_3(0,0,-1,0).
\]
Isso equivale ao seguinte sistema linear ser compatível:
\[
\systeme[c_1c_2c_3]{
 c_1 + 2c_2 = a,
-c_1 - 3c_2 = b,
 c_1 + 2c_2 -c_3 = c,
       -c_2      = d
}
\]
Escalonando a matriz ampliada desse sistema, resulta:
\[
\begin{bmatrix}
 1 &  2 &  0 & a\\
-1 & -3 &  0 & b\\
 1 &  2 & -1 & c\\
 0 & -1 &  0 & d\\
\end{bmatrix}
\rightarrow
\cdots
\rightarrow
\begin{bmatrix}
 1 & 0 & 0 & 3a+2b\\
 0 & 1 & 0 & -a-b \\
 0 & 0 & 1 &  a-c  \\
 0 & 0 & 0 & -a-b+d \\
\end{bmatrix}
\]
Assim, se $-a-b+d = 0$, o posto da matriz do sistema será igual ao posto da matriz ampliada, e o sistema será possível. Portanto, $v$ é combinação linear de $w_1$, $w_2$ e $w_3$ se, e somente se, $d = a+b$.

\item
\begin{enumerate}
\item Para toda matriz diagonal $X \in M_{3 \times 3}(\R)$ tem-se
\[
X
= \begin{bmatrix}
a & 0 & 0 \\
0 & b & 0 \\
0 & 0 & c
\end{bmatrix}
=
a
\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
+b
\begin{bmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{bmatrix}
+c
\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix},
\]
com $a,b,c \in \R$. Então estas três matrizes geram $D$.

\item Para toda matriz antissimétrica $A \in M_{3 \times 3}(\R)$ tem-se
\[
A
= \begin{bmatrix}
 0 & a & b \\
-a & 0 & c \\
-b & c & 0
\end{bmatrix}
=
a
\begin{bmatrix}
 0 & 1 & 0 \\
-1 & 0 & 0 \\
 0 & 0 & 0
\end{bmatrix}
+b
\begin{bmatrix}
 0 & 0 & 1 \\
 0 & 0 & 0 \\
-1 & 0 & 0
\end{bmatrix}
+c
\begin{bmatrix}
0 &  0 & 0 \\
0 &  0 & 1 \\
0 & -1 & 0
\end{bmatrix},
\]
com $a,b,c \in \R$. Então estas três matrizes geram $W$.

\item Para toda matriz simétrica $B \in M_{3 \times 3}(\R)$ tem-se
\begin{align*}
B
= \begin{bmatrix}
a & b & c \\
b & d & e \\
c & e & f
\end{bmatrix}
&=
a
\begin{bmatrix}
1 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
+b
\begin{bmatrix}
0 & 1 & 0 \\
1 & 0 & 0 \\
0 & 0 & 0
\end{bmatrix}
+c
\begin{bmatrix}
0 & 0 & 1 \\
0 & 0 & 0 \\
1 & 0 & 0
\end{bmatrix}
+d
\begin{bmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{bmatrix}\\
&+e
\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 1 \\
0 & 1 & 0
\end{bmatrix}
+f
\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
0 & 0 & 1
\end{bmatrix},
\end{align*}
com $a,b,c,d,e,f \in \R$. Então estas 6 matrizes geram $U$.
\end{enumerate}

\item
\begin{enumerate}
\item $v_1=(0,3,-9)$, $v_2=(1,4,-10)$, $v_3=(2,5,-11)$ em $\R^3$.

Suponha que $c_1$, $c_2$ e $c_3$ sejam escalares tais que $c_1v_1 + c_2v_2 + c_3v_3 = 0$, isto é, que
\[
c_1(0,3,-9) + c_2(1,4,-10) + c_3 (2,5,-11)
= (0, 0, 0)
\]
ou ainda,
\[(c_2 + 2c_3, 3c_1 + 4c_2 + 5c_3, -9c_1 - 10c_2 - 11c_3)
= (0, 0, 0)
\]
Então $c_1, c_2, c_3$ são soluções do sistema linear homogêneo
\systeme{
c_2 + 2c_3 =0,
3c_1 + 4c_2 + 5c_3=0,
-9c_1 - 10c_2 - 11c_3=0
}
Como o número de equações é o mesmo do número de incógnitas, pode-se determinar se $c_1=c_2=c_3=0$ é ou não a única solução calculando o determinante da matriz associada ao sistema:
\[
\begin{vmatrix}
 0 &   1 &   2 \\
 3 &   4 &   5 \\
-9 & -10 & -11
\end{vmatrix}
=
(-1)
\begin{vmatrix}
 3 &   5 \\
-9 & -11
\end{vmatrix}
+2
\begin{vmatrix}
 3 &   4 \\
-9 & -10
\end{vmatrix}
=
(-1) \cdot 12
+2 \cdot 6
= 0
\]
Como o determinante é zero, existem infinitas soluções não triviais e os vetores dados  são linearmente dependentes.




\item $v_1=(0,3,-9)$, $v_2=(1,4,-10)$ em $\R^3$.
Pelo exercício 1, se $v_1$ e $v_2$ fossem linearmente dependentes, um deles seria múltiplo do outro. Mas estes dois vetores não são múltiplos, logo, só podem ser linearmente independentes.


\item A condição $c_1 v_1 + c_2 v_2 + c_3 v_3 + c_4 v_4 + c_5 v_5 = 0$ equivale às seguintes condições:
\begin{align*}
& c_1
\begin{bmatrix}
0 & 2 \\
2 & 0
\end{bmatrix}
c_2
\begin{bmatrix}
1 & 0 \\
0 & 4
\end{bmatrix}
c_3
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix}
c_4
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix}
c_5
\begin{bmatrix}
1 & 2 \\
3 & 4
\end{bmatrix}
=\begin{bmatrix}
0 & 0 \\
0 & 0
\end{bmatrix} \\
&
\Leftrightarrow
\systeme{
c_2+c_3+c_5=0,
2c_1+c_4+2c_5=0,
2c_1+3c_5=0,
4c_2+4c_5=0
}
\Leftrightarrow
\begin{bmatrix}
0 & 1 & 1 & 0 & 1 \\
2 & 0 & 0 & 1 & 2 \\
2 & 0 & 0 & 0 & 3 \\
0 & 4 & 0 & 0 & 4
\end{bmatrix}
\cdot
\begin{bmatrix}
c_1\\
c_2\\
c_3\\
c_4\\
c_5
\end{bmatrix}
=
\begin{bmatrix}
0\\
0\\
0\\
0\\
0
\end{bmatrix}
\end{align*}
Escalonando a matriz deste sistema (ou encontrando as suas soluções por qualquer outro método) obtém-se:
\begin{align*}
& \begin{bmatrix}
0 & 1 & 1 & 0 & 1 \\
2 & 0 & 0 & 1 & 2 \\
2 & 0 & 0 & 0 & 3 \\
0 & 4 & 0 & 0 & 4
\end{bmatrix}
\grstep{ L_1 \swap L_2 }
\begin{bmatrix}
2 & 0 & 0 & 1 & 2 \\
0 & 1 & 1 & 0 & 1 \\
2 & 0 & 0 & 0 & 3 \\
0 & 4 & 0 & 0 & 4
\end{bmatrix}
\grstep[ \frac{1}{4}L_4 ]{ \frac{1}{2}L_1 }
\begin{bmatrix}
1 & 0 & 0 & 1/2 & 1 \\
0 & 1 & 1 & 0 & 1 \\
2 & 0 & 0 & 0 & 3 \\
0 & 1 & 0 & 0 & 1
\end{bmatrix}\\
\grstep[ L_4 - L_2 ]{ L_3 - 2L_1 }
& \begin{bmatrix}
1 & 0 & 0 & 1/2 & 1 \\
0 & 1 & 1 &  0 & 1 \\
0 & 0 & 0 & -1 & 1 \\
0 & 0 & -1 & 0 & 0
\end{bmatrix}
\grstep{ L_3 \swap L_4 }
\begin{bmatrix}
1 & 0 & 0 & 1/2 & 1 \\
0 & 1 & 1 &  0 & 1 \\
0 & 0 & -1 & 0 & 0 \\
0 & 0 & 0 & -1 & 1
\end{bmatrix}
\grstep[ -L_4 ]{ -L_3 }
\begin{bmatrix}
1 & 0 & 0 & 1/2 & 1 \\
0 & 1 & 1 &  0 & 1 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & -1
\end{bmatrix}\\
\grstep[ L_1-\frac{1}{2}L_4 ]{ L_2-L_3 }
&\begin{bmatrix}
1 & 0 & 0 & 0 & 3/2 \\
0 & 1 & 0 & 0 & 1 \\
0 & 0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 & -1
\end{bmatrix}
\Rightarrow
\begin{cases}
c_1 =-c_5/2\\
c_2 = -c_5\\
c_3 = 0\\
c_4 = c_5
\end{cases}
\end{align*}
Como o sistema tem $c_5$ como uma variável livre, é possível obter o vetor nulo como combinação linear das matrizes $v_1,\ldots,v_5$ sem que todos os coeficientes sejam nulos (exemplo: $-1v_1 - 2v_2 + 0v_3 + 5v_4 + 5v_5 = 0$). Isto significa que estes vetores são linearmente dependentes.

\item A condição $a v_1 + b v_2 + c v_3 + d v_4 = 0$ equivale às seguintes condições:
\begin{align*}
& a ( x^3 + x^2 + x + 1 ) + b ( x^2 + x + 1 ) + c (x + 1) + d
= 0 x^3 + 0 x^2 + 0 x + 0 \\
& \Leftrightarrow
a x^3 + (a+b) x^2 + (a+b+c)x + (a+b+c+d) = 0 x^3 + 0 x^2 + 0 x + 0 \\
\Leftrightarrow
&
\systeme{
a=0,
a+b=0,
a+b+c=0,
a+b+c+d=0
}
\Leftrightarrow
\begin{cases}
a=0\\
b=0\\
c=0\\
d=0
\end{cases}
\end{align*}
\end{enumerate}
Como a única forma de obter o vetor nulo como combinação linear dos polinômios dados é usar todos os coeficientes iguais a zero, resulta que eles são linearmente independentes.


\item
\begin{enumerate}
\item Pode-se verificar que $B = \{ (1,0,0,0), (0,1,0,0), (0,0,1,0), (0,0,0,1) \}$ é uma base de $\R^4$ e disto resulta que $\dim( \R^4 ) = 4$.
\item $B = \left\{
\begin{bmatrix}
1 & 0 & 0 \\0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & 1 & 0 \\0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & 0 & 1 \\0 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & 0 & 0 \\1 & 0 & 0
\end{bmatrix},
\begin{bmatrix}
0 & 0 & 0 \\0 & 1 & 0
\end{bmatrix},
\begin{bmatrix}
0 & 0 & 0 \\0 & 0 & 1
\end{bmatrix}
\right\}$ é uma base de $M_{2 \times 3}(\R)$ (por que?) e consequentemente $\dim( M_{2 \times 3}(\R) ) = 6$.

\item $B = \left\{1,x,x^2,x^3,x^4,x^5 \right\}$ é uma base de $P_5$ (verifique!) e portanto $\dim( P_5 ) = 6$.
\end{enumerate}

\item
\begin{enumerate}
\item Tem-se $u \in W$ se e somente se $u = (x,y,0)$, para algm $x,y, \in \R$. Neste caso, $u = x(1,0,0) + y(0,1,0)$. Portanto os vetores $v_1 = (1,0,0)$ e $v_2 = (0,1,0)$ geram $W$. Mas estes vetores são linearmente independentes, pois se $k_1 v_1 + k_2 v_2 = 0$ então
\[
k_1(1,0,0) + k_2(1,0,0) = (k_1,k_1,0) = (0,0,0)
\]
o que significa que $k_1 = k_2 = 0$. Em outras palavras, o vetor nulo só pode ser obtido como combinação linear de $v_1$ e $v_2$ se todos os coeficientes forem nulos. Portanto, $B = \{(1,0,0),(0,1,0)\}$ é uma base de $W$, e conclui-se que $\dim(W)=2$.

\item Um vetor $u \in V$ pertence a $W$ se, e somente se
\[
u
=
\begin{bmatrix}
a & b \\ b & c
\end{bmatrix}
=
a\begin{bmatrix}
1 & 0 \\ 0 & 0
\end{bmatrix}
+b\begin{bmatrix}
0 & 1 \\ 1 & 0
\end{bmatrix}
+c\begin{bmatrix}
0 & 0 \\ 0 & 1
\end{bmatrix},
= a v_1 + bv_2 + cv_3
\]
em que $v_1
=
\begin{bmatrix}
1 & 0 \\ 0 & 0
\end{bmatrix}$,
$v_2=
\begin{bmatrix}
0 & 1 \\ 1 & 0
\end{bmatrix}$
e
$v_3=
\begin{bmatrix}
0 & 0 \\ 0 & 1
\end{bmatrix}$ e $a,b,c \in \R$. Isto significa que $W = \ger{v_1,v_2,v_3}$. Além disso, se $m_1 v_1 + m_2 v_2 + m_3v_3 = 0$, tem-se
\[
m_1
\begin{bmatrix}
1 & 0 \\ 0 & 0
\end{bmatrix}
+m_2
\begin{bmatrix}
0 & 1 \\ 1 & 0
\end{bmatrix}
+m_3
\begin{bmatrix}
0 & 0 \\ 0 & 1
\end{bmatrix}
=
\begin{bmatrix}
m_1 & m_2 \\ m_2 & m_3
\end{bmatrix}
=\begin{bmatrix}
0 & 0 \\ 0 & 0
\end{bmatrix},
\]
isto é, cada $m_i = 0$, o que significa que os vetores $v_1, v_2, v_3$ são  L.I. e $B = \{v_1, v_2, v_3\}$ é uma base de $W$. Consequentemente, $\dim(W) = 3$.

\item $V = \mathcal{F}(\R)$ e $W = \{ p(x) = ax^2+bx+c \in P_2 \mid p^\prime(6) = 0 \}$ (geometricamente, estas funções correspondem a parábolas com vértice em $(6, f(6))$).

Dado $p(x) = ax^2+bx+c \in P_2$ tem-se $p^\prime(x) = 2ax + b$. Então $p(x) \in W$ se, e somente se $2a(6) + b = 0$, ou seja, $b = -12a$. Assim, todo elemento de $W$ é da forma
\[
p(x)
= ax^2 + (-12a)x + c
= a \cdot (x^2 -12x) + c \cdot 1,
\]
com $a,c \in \R$. Em outras palavras, os polinômios $q_1 = x^2 -12x$ e $q_2 = 1$ geram $W$. Estes vetores também são linearmente independentes pois

$
c_1(x^2 -12x) + c_2(1) = 0
\Leftrightarrow
c_1x^2 -12 c_1 x + c_2 = 0
\Leftrightarrow
\begin{cases}
    c_1 = 0\\
-12 c_1 = 0\\
    c_2 = 0
\end{cases}
\Leftrightarrow
\begin{cases}
c_1 = 0\\
c_2 = 0
\end{cases}
$
Portanto, $B = \ger{1, x^2 - 12x}$ é uma base de $W$ e $\dim(W) = 2$.

\item Como
\[
\begin{cases}
x +  2w = 0\\
  -y    = -2w\\
      w = z/2
\end{cases}
\Leftrightarrow
\begin{cases}
x = -2w\\
y = 2w\\
z = 2w
\end{cases}
\]
resulta que $u \in W$ se, e somente se, $u = (-2w,2w,2w,w) = w (-2,2,2,1)$, com $w \in \R$. Em outras palavras, $W = \ger{ (-2,2,2,1) }$. Como este vetor é diferente de zero, $B = \{ (-2,2,2,1) \}$ é L.I. pois
\[
k (-2,2,2,1) = (0,0,0,0)
\Rightarrow
\begin{cases}
-2k = 0\\
2k = 0\\
2k = 0\\
k = 0
\end{cases}
\Rightarrow k = 0.
\]
Portanto $B$ é uma base para $W$ e $\dim(W) = 1$.
\end{enumerate}

\item Como $(1,1,1) = \frac{1}{2} \left( (1,1,0) + (1,0,1) + (0,1,1) \right)$, pode-se dizer que $S$ é L.D. (pois um dos vetores é combinação linear dos demais, ou porque o sistema linear associado tem soluções não nulas) e portanto não forma uma base de $\R^3$. No entanto, se forem considerados apenas os vetores $v_1 = (1,1,0)$, $v_2 = (1,0,1)$ e $v_3 = (0,1,1)$, então o subconjunto $B = \{ v_1, v_2, v_3 \}$ será uma base de $\R^3$ pois $R^3 = \ger{ v_1, v_2, v_3 }$ e estes três vetores são L.I. De fato, qualquer $(a,b,c) \in \R^3$ pode ser escrito como combinação linear de $v_1, v_2$ e $v_3$ pois
\begin{align*}
& (a,b,c) = x(1,1,0) + y(1,0,1) + z(0,1,1)
\Leftrightarrow
\begin{cases}
x+y=a\\
x+z=b\\
y+z=c\\
\end{cases}
\Leftrightarrow
\begin{cases}
x=\frac{1}{2}( a+b-c)\\
y=\frac{1}{2}( a-b+c)\\
z=\frac{1}{2}(-a+b+c)
\end{cases}
%\\
%& \Leftrightarrow
%(a,b,c) =
%  \left(\frac{1}{2}( a+b-c)\right)(1,1,0)
%+ \left(\frac{1}{2}( a-b+c)\right)(1,0,1)
%+ \left(\frac{1}{2}(-a+b+c)\right)(0,1,1)
\end{align*}
Além disso, com base no cálculo acima, o único modo de obter $(a,b,c) = (0,0,0)$ como combinação linear de $v_1$, $v_2$ e $v_3$ é considerando todos os coeficientes $x=y=z=0$.

\item Para que um conjunto de vetores seja L.D. deve ser possível expressar o vetor nulo como combinação linear deles sem que todos os coeficientes desta combinação linear sejam zero. No caso dos vetores $0, v_1, \ldots, v_n$, pode-se usar, por exemplo, os escalares $c_0 =7, c_1 = 0, c_2 = 0, \ldots, c_n = 0$ pois $7 \cdot 0 + 0 \cdot v_1 + \ldots + 0 \cdot v_n = 0$. Como pelo menos o escalar $c_0 \neq 0$, conclui-se que os vetores são linearmente dependentes.

\item Sejam $W = \ger{ v_1, \ldots, v_n }$ e $z \in W$, isto é, $z = k_1 v_1 + \ldots + k_n v_n$, com $k_1, \ldots, k_n \in \R$. Sendo $U = \ger{ v_1, \ldots, v_n, z }$, pode-se dizer que os conjuntos $U$ e $W$ são iguais se for explicado por que todo elemento de $W$ pertence a $U$ e vice-versa ($W \subseteq U$ e $U \subseteq W$).
\begin{itemize}
\item
Se $u \in W$, então $u$ é uma combinação linear dos vetores $v_i$, isto é, $u = c_1 v_1 + \ldots + c_n v_n$. Mas então é claro que $u$ também é combinação linear dos vetores $v_i$ e de $z$ pois basta considerar um coeficiente zero para $z$, como em $u = c_1 v_1 + \ldots + c_n v_n + 0 \cdot z$. Isto quer dizer que $z \in \ger{ v_1, \ldots, v_n, z } = U$, ou seja, que $W \subseteq U$.
\item Por outro lado, todo $u \in W$ é combinação linear de $v_1, \ldots, v_n, z$, isto é,
\begin{align*}
u
& = d_1 v_1 + \ldots + d_n v_n + d_{n+1} z \\
& = d_1 v_1 + \ldots + d_n v_n + d_{n+1}(k_1 v_1 + \ldots + k_n v_n) \\
& = (d_1 + d_{n+1}k_1) v_1 + \ldots + ( d_n + d_{n+1}k_n) v_n.
\end{align*}
Portanto $u$ é uma combinação linear dos vetores $v_1, \ldots, v_n$, o que quer dizer que $u \in \ger{ v_1, \ldots, v_n } = W$, ou seja, que $U \subseteq W$.
\end{itemize}
Em resumo, ao adicionar a uma lista de vetores $v_i$ um vetor $z$ que já esteja no espaço gerado por eles, o espaço gerado por todos estes vetores continuará sendo o mesmo.

\item Suponha que $a u_1 + bu_2 = 0$. Então $a u_1 + b u_2 + 0 u_3= 0$, ou seja, tem-se uma combinação de $u_1, u_2, u_3$ que resulta no vetor nulo. Mas por hipótese, $u_1, u_2, u_3$ são L.I., então esta combinação linear precisa ter todos os coeficientes iguais a zero, isto é, $a=b=0$. Assim, mostrou-se que não há outra forma de obter o vetor nulo como combinação linear de $u_1$ e $u_2$, isto é, $u_1$ e $u_2$ são L.I.

\item Para que os conjuntos $U$ e $\ger{U}$ sejam iguais, cada um deve estar contido no outro.
\begin{enumerate}
\item $\ger{U} \subseteq U$, pois dado $u \in \ger{U}$, existem vetores $v_1, \ldots v_n \in U$ e escalares $c_1, \ldots, c_n \in \R$ tais que $u = c_1v_1 + \cdots + c_nv_n \in U$. Logo, $u \in U$ pois $U$ é subespaço vetorial, e como tal, é fechado para as operações de adição e multiplicação por escalar.
\item $U \subseteq \ger{U}$, pois dado $u \in U$, tem-se $u = 1 \cdot u$, o que significa que $u$ é uma combinação linear de vetores de $U$ e, portanto, um elemento de $\ger{U}$.
\end{enumerate}

\item
\begin{enumerate}
\item
\begin{enumerate}
\item Como o espaço linha da matriz $A$ e o espaço linha de sua forma escalonada reduzida são os mesmos, pode-se utilizar as linhas não nulas da forma reduzida como vetores geradores de $L(A)$. Para a matriz dada, o escalonamento consistirá destes passos:
\begin{align*}
\begin{bmatrix}
1 & -1 \\
1 & 1
\end{bmatrix}
\grstep{ L_2 - L_1 }
\begin{bmatrix}
1 & -1 \\
0 & 2
\end{bmatrix}
\grstep{ \frac{1}{2}L_2 }
\begin{bmatrix}
1 & -1 \\
0 & 1
\end{bmatrix}
\grstep{ L_1 + L_2 }
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\end{align*}
Então $L(A) = \ger{
\begin{bmatrix}
1 & 0
\end{bmatrix},
\begin{bmatrix}
0 & 1
\end{bmatrix}
} = \{
\begin{bmatrix}
a & b
\end{bmatrix} \mid a,b, \in \R
\} = M_{1 \times 2} (\R)$. Além disso, é claro que estes dois vetores linha são L.I. pois $a
\begin{bmatrix}
1 & 0
\end{bmatrix}
+b
\begin{bmatrix}
0 & 1
\end{bmatrix}
=
\begin{bmatrix}
a & b
\end{bmatrix}$ só pode ser igual a $
\begin{bmatrix}
0 & 0
\end{bmatrix}$ se $a$ e $b$ forem ambos nulos. Em outras palavras, estes dois vetores formam uma base de $L(A)$, que portanto é um espaço vetorial de dimensão dois.

\item
Para o espaço coluna de $A$, pode-se usar como geradores as colunas de $A$ que correspondem às posições dos pivôs da forma reduzida. Especificamente, tem-se $C(A)
= \ger{\begin{bmatrix}
1\\
1
\end{bmatrix},
\begin{bmatrix}
-1\\
1
\end{bmatrix}}$. Estes dois vetores coluna são $L.I.$ pois nenhum deles é múltiplo do outro. Então eles formam uma base de $C(A)$ e $\dim(C(A)) = 2$.
\item O núcleo $N(A)$ consiste dos vetores $\begin{bmatrix}
x\\y
\end{bmatrix}$ tais que $\begin{bmatrix}
1 & -1\\
1 & 1
\end{bmatrix}\begin{bmatrix}
x\\
y
\end{bmatrix}
=
\begin{bmatrix}
0\\
0
\end{bmatrix}$, isto é, as coordenadas $x,y$ são soluções do sistema linear
\systeme{
x-y=0,
x+y=0.
}

Isto implica que $x=y=0$ e que $N(A) = \left\{ \begin{bmatrix}
0\\
0
\end{bmatrix} \right\}$. Portanto, o conjunto vazio $\emptyset$ é uma base de $N(A)$. Perceba que não faria sentido que a base tivesse algum vetor não nulo, pois senão ela geraria um espaço maior do que o espaço nulo; e também não faz sentido considerar apenas o vetor nulo como uma base, já que ele não seria L.I.. Logo a dimensão de $N(A)$ é zero (não há vetores na base).
\end{enumerate}

\item Note que $A = \begin{bmatrix}
1 & 2 & 3 & 4
\end{bmatrix}$ está na forma escalonada reduzida por linhas, então:
\begin{enumerate}
\item $L(A) = \ger{ \begin{bmatrix}
1 & 2 & 3 & 4
\end{bmatrix} }$ e como este conjunto é L.I., $\dim(L(A)) = 1$

\item
$C(A) = \ger{ \begin{bmatrix}
1
\end{bmatrix} }$ e como o conjunto é L.I. $\dim(C(A)) = 1$

\item $N(A) = \left\{ \begin{bmatrix}
-2y-3z-4w\\y\\z\\w
\end{bmatrix} \mid y,z,w\in \R
\right\}
=
\ger{ \begin{bmatrix}
-2\\1\\0\\0
\end{bmatrix},
\begin{bmatrix}
-3\\0\\1\\0
\end{bmatrix},
\begin{bmatrix}
-4\\0\\0\\1
\end{bmatrix}  }$ e como os três vetores são L.I., $\dim(L(A)) = 3$

\end{enumerate}

\item A matriz $A = \begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & 0 & -1
\end{bmatrix}$ está na forma escalonada reduzida então:
\begin{enumerate}
\item $L(A) = \ger{ \begin{bmatrix}
1 & 0 & 1 & 0
\end{bmatrix}, \begin{bmatrix}
0 & 1 & 0 & -1
\end{bmatrix} }$ e $\dim(L(A)) = 2$ (os vetores são L.I.)

\item $C(A) = \ger{ \begin{bmatrix}
1 \\ 0
\end{bmatrix}, \begin{bmatrix}
0 \\ 1
\end{bmatrix} }$ e $\dim(C(A)) = 2$.

\item $N(A) = \left\{ \begin{bmatrix}
-z\\w\\z\\w
\end{bmatrix} \mid y,z,w\in \R
\right\}
=
\ger{ \begin{bmatrix}
-1\\0\\1\\0
\end{bmatrix},
\begin{bmatrix}
0\\1\\0\\1
\end{bmatrix} }$ e $\dim(L(A)) = 2$.
\end{enumerate}


\item A forma escalonada reduzida de $A = \begin{bmatrix}
1 & 2\\
0 & 0\\
2 & 4\\
0 & 0\\
3 & 6
\end{bmatrix}$ é $\begin{bmatrix}
1 & 2\\
0 & 0\\
0 & 0\\
0 & 0\\
0 & 0
\end{bmatrix}$, então
\begin{enumerate}
\item $L(A) = \ger{ \begin{bmatrix}
1 & 2
\end{bmatrix} }$ e $\dim(L(A)) = 1$.

\item $C(A) = \ger{ \begin{bmatrix}
1\\
0\\
0\\
0\\
0
\end{bmatrix} }$ e $\dim(C(A)) = 1$.

\item $N(A) = \left\{ \begin{bmatrix}
-2y\\y
\end{bmatrix} \mid y\in \R
\right\}
=
\ger{ \begin{bmatrix}
-2\\1
\end{bmatrix} }$ e $\dim(L(A)) = 1$.
\end{enumerate}
\end{enumerate}

\item \begin{enumerate}
\item Como $v=(x,y,z,w) \in U$ se, e somente se, $x + y = 0 = z + w$, isto é, $x=-y$ e $z = -w$, ou seja, $v = (-y,y,-w,w) = y(-1,1,0,0) + w(0,0,-1,1)$. Assim,
\[
U = \ger{ (-1,1,0,0), (0,0,-1,1) }.
\]

Além disso, $v = (a,b,c,d) \in V$ se, e somente se, $b=2c$, ou seja, $v = (a,2c,c,d) = a(1,0,0,0) + c(0, 2,1,0)+c(0,0,0,1)$. Logo,
\[
V = \ger{ (1,0,0,0), (0, 2,1,0), (0,0,0,1) }
\]
e, consequentemente,
\[
U+V = \ger{(-1,1,0,0), (0,0,-1,1), (1,0,0,0), (0,2,1,0), (0,0,0,1)}
    = \R^4.
\]
A soma não é direta pois, por exemplo, $(2,-2,-1,1) \in U \cap V$.

\item Os elementos de $U$ são da forma
\begin{align*}
M =
\begin{bmatrix}
 0 &  x & y\\
-x &  0 & z\\
-y & -z & 0
\end{bmatrix}
& =x
\underbrace{
\begin{bmatrix}
 0 & 1 & 0\\
-1 & 0 & 0\\
 0 & 0 & 0
\end{bmatrix}
}_{M_1}
+y
\underbrace{
\begin{bmatrix}
 0 & 0 & 1\\
 0 & 0 & 0\\
-1 & 0 & 0
\end{bmatrix}
}_{M_2}
+z
\underbrace{
\begin{bmatrix}
0 &  0 & 0\\
0 &  0 & 1\\
0 & -1 & 0
\end{bmatrix}
}_{M_3}\\
& = xM_1+yM_2+zM_3.
\end{align*}
Logo $U = \ger{
M_1, M_2, M_3
}$.
Por outro lado, os elementos de $V$ são da forma
\begin{footnotesize}
\begin{align*}
N =
\begin{bmatrix}
a & b & c\\
b & d & e\\
c & e & f
\end{bmatrix}
& =a
\underbrace{
\begin{bmatrix}
1 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 0
\end{bmatrix}
}_{N_1}
+b
\underbrace{
\begin{bmatrix}
0 & 1 & 0\\
1 & 0 & 0\\
0 & 0 & 0
\end{bmatrix}
}_{N_2}
+c
\underbrace{
\begin{bmatrix}
0 & 0 & 1\\
0 & 0 & 0\\
1 & 0 & 0
\end{bmatrix}
}_{N_3}
+d
\underbrace{
\begin{bmatrix}
0 & 0 & 0\\
0 & 1 & 0\\
0 & 0 & 0
\end{bmatrix}
}_{N_4}
+e
\underbrace{
\begin{bmatrix}
0 & 0 & 0\\
0 & 0 & 1\\
0 & 1 & 0
\end{bmatrix}
+f
}_{N_5}
\underbrace{
\begin{bmatrix}
0 & 0 & 0\\
0 & 0 & 0\\
0 & 0 & 1
\end{bmatrix}
}_{N_6}\\
&=aN_1 + bN_2+cM_3+dN_4+eN_5+fN_6.
\end{align*}
\end{footnotesize}

Logo $V = \ger{N_1, N_2, N_3, N_4, N_5, N_6}$ e
\[
U+V= \ger{M_1, M_2, M_3, N_1, N_2, N_3, N_4, N_5, N_6}
= \R^{3 \times 3}.
\]
Além disso $U \cap V = \{ 0_{3 \times 3} \}$ pois se $A \in U \cap V$ então $A = A^T = -A$, ou seja, $2A = 0_{3\times3}$ e $A=0_{3\times3}$. Logo, $\R^{3 \times 3} = U \oplus V$.

\item Se $q(x)=b_0 + b_1x+b_2x^2+b_3x^3$, então $q(-x)=b_0 - b_1x + b_2x^2 - b_3x^3$. Assim:
\begin{itemize}
\item $q \in U$ se, e somente se, $\forall x \in \R, q(-x) = -q(x)$, isto é, $\forall x \in \R, b_0 - b_1x + b_2x^2 - b_3x^3 = -b_0 - b_1x - b_2x^2 - b_3x^3$, o que equivale a $b_0 = -b_0$ e $b_2 = -b_2$, ou seja, $b_0 = b_2 = 0$. Portanto, $\forall x \in \R, q(x) = b_1 x + b_3 x^3$, ou seja $U = \ger{x, x^3}$.
\item $q \in V$ se, e somente se, $\forall x \in \R, q(-x) = q(x)$, isto é, $\forall x \in \R, b_0 - b_1x + b_2x^2 - b_3x^3 = b_0 + b_1x + b_2x^2 + b_3x^3$, o que equivale a $b_1 = -b_1$ e $b_3 = -b_3$, ou seja, $b_1 = b_3 = 0$. Portanto, $\forall x \in \R, q(x) = b_0 + b_3 x^2$, ou seja $U = \ger{1, x^2}$.
\end{itemize}
Com base nos itens anteriores, $U + V = \ger{1,x,x^2,x^3} = P_3$. Além disso, $U \cap V = \{ \vec{0} \}$, pois se $\forall x \in \R, p(x) = p(-x) = -p(x)$, então $\forall x \in \R, 2p(x)=0$, ou seja, $p = \vec{0}$. Logo, $P_3 = U \oplus V$.

\end{enumerate}

\item \begin{enumerate}
\item O conjunto $B_1 = \{ (-1,1,0,0), (0,0,-1,1), (1,0,0,0), (0,2,1,0) \}$ é uma base de $U+V$ e assim, $\dim{U+V} = 4$.
\item O conjunto $B_2 = \{ M_1, M_2, M_3, N_1, N_2, N_3, N_4, N_5, N_6 \}$ é uma base de $U+V$ e assim, $\dim{U+V} = 9$.
\item O conjunto $B_3 = \{ 1, x, x^2, x^3 \}$ é uma base de $U+V$ e assim, $\dim{U+V} = 4$.
\end{enumerate}


\item Suponha que $a(u+v) + b(u-v) + c(u-w) = 0$. Então $(a+b+c)u+(a-b)v + (-c)w = 0$ e como $u,v,w$ são L.I. esta combinação linear só resulta no vetor nulo se os três coeficientes forem zero, isto é, se
\[
\begin{cases}
a+b+c=0\\
a-b=0\\
-c=0\\
\end{cases}
\]
Mas a única solução deste sistema é $a=b=c=0$, o que implica que vetores $u+v$, $u-v$ e $u-w$ são L.I..

\item Se $B = \{u, v\}$ é uma base de $V$ então $V = \ger{u} \oplus \ger{v}$, pois:
\begin{itemize}
\item $V = \ger{u} + \ger{v}$. De fato, como $B$ é base, seus elementos geram $V$, ou seja todo $w \in V$ pode ser escrito na forma $w = \alpha u + \beta v$, com $\alpha, \beta \in \R$. Mas $\alpha u \in \ger{u}$ e $\beta v \in \ger{v}$, então $w \in \ger{u} + \ger{v}$, o que significa que $V \subseteq \ger{u} + \ger{v}$. Reciprocamente, dado $w \in \ger{u} + \ger{v}$, tem-se $w \in V$, pois a soma de subespaços de $V$ também é um subespaço de $V$. Assim, $\ger{u} + \ger{v} \subseteq V$.
\item $\ger{u} \cap \ger{v} = \{ 0 \}$. De fato, se $w \in \ger{u} \cap \ger{v}$ então $w = \alpha u$ e $w = \beta v$, para algum $\alpha, \beta \in \R$. Consequentemente, $\alpha u - \beta v = 0$ e o fato de $u$ e $v$ serem linearmente independentes implica que $\alpha = 0$ e $-\beta = 0$, ou seja, $w = 0 u = 0 w = \vec{0}$.
Assim, $\ger{u} \cap \ger{v} \subseteq \{ 0 \}$. Por outro lado, é imediato que $\{ 0 \} \subseteq \ger{u} \cap \ger{v}$, pois a interseção de subespaços é um subespaço vetorial, e como tal deve conter o vetor nulo.
\end{itemize}

\item Sejam $U = \ger{ (1,0,0), (2,1,1) }$ e $V = \ger{ (0,-1,0), (0,0,1) }$ subespaços de $\R^3$.
\begin{enumerate}
\item Um vetor $v \in \R^3$ está em $U \cap V$ se, e somente se, existirem $x_1$, $x_2$, $x_3$ e $x_4$ tais que $v = x_1(1,0,0) + x_2(2,1,1) = x_3(0,-1,0) + x_4(0,0,1)$, o que equivale a:
\begin{footnotesize}
\[
\systeme[x_1x_2x_3x_4]{
 x_1+2x_2=0x_3+0x_4,
 0x_1+x_2=-x_3+0x_4,
 0x_1+x_2=0x_3+x_4
}
\Leftrightarrow
\systeme[x_1x_2x_3x_4]{
 1x_1+2x_2+0x_3+0x_4=0,
 0x_1+1x_2+1x_3+0x_4=0,
 0x_1+1x_2+0x_3-1x_4=0
}
\Leftrightarrow
\begin{bmatrix}
1 & 2 & 0 & 0 \\
0 & 1 & 1 & 0 \\
0 & 1 & 0 & -1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2\\x_3\\x_4
\end{bmatrix}
=
\begin{bmatrix}
0\\0\\0
\end{bmatrix}.
\]
\end{footnotesize}
Como a solução deste sistema é $S = \{ (-2x_4, x_4, -x_4, x_4) \mid x_4 \in \R \}$, conclui-se que $v = x_1(1,0,0) + x_2(2,1,1) = -2x_4(1,0,0) + x_4(2,1,1)
= x_4(0, 1, 1)$. Logo $U \cap V = \ger{ (0,1,1) }$ e como $B=\{(0,1,1)\}$ é linearmente independente, conclui-se que $B$ é uma base de $U\cap V$ e que $\dim{U \cap V} = 1$.
\item  Um vetor $w = (a,b,c) \in \R^3$ está em $U + V$ se, e somente se, existirem $u \in U$ e $v \in V$ tais que $w=u+v$, ou seja, se existirem $x_1$, $x_2$, $x_3$ e $x_4$ tais que
\[
w = x_1(1,0,0) + x_2(2,1,1) + x_3(0,-1,0) + x_4(0,0,1),
\]
o que equivale a:
\[
\systeme[x_1x_2x_3x_4]{
 1x_1+2x_2+0x_3+0x_4=a,
 0x_1+1x_2-1x_3+0x_4=b,
 0x_1+1x_2+0x_3+1x_4=c
}
\Leftrightarrow
\begin{bmatrix}
1 & 2 & 0 & 0 \\
0 & 1 & -1 & 0 \\
0 & 1 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2\\x_3\\x_4
\end{bmatrix}
=
\begin{bmatrix}
a\\b\\c
\end{bmatrix}.
\]
Realizando a eliminação de Gauss-Jordan, obtém-se:
\[
\begin{bmatrix}
1 & 2 & 0 & 0 & a \\
0 & 1 & -1 & 0 & b\\
0 & 1 & 0 & 1 & c
\end{bmatrix}
\rightarrow
\cdots
\rightarrow
\begin{bmatrix}
1 & 0 & 0 & -2 & a -2c \\
0 & 1 & 0 &  1 & b \\
0 & 0 & 1 &  1 & c-b
\end{bmatrix},
\]
Como o posto da matriz de coeficientes é igual ao da matriz ampliada do sistema, ele é possível (e indeterminado), ou seja, todo $w \in \R^3$ está em $U+V$, isto é, $\R^3 = U+V$. No entanto, como $U \cap V = \ger{ (0,1,1) } \neq \{ (0,0,0) \}$, a soma não é direta.
\end{enumerate}

\item Se $B = \{u, v\}$ é uma base de $V$, então $\dim{V} = 2$ e qualquer conjunto linearmente independente com dois vetores gera (e é base de) $V$. Em particular, isso pode ser usado para mostrar que  $B^\prime = \{ u+v, u-v \}$  é base: supondo que $\alpha_1 (u+v) + \alpha_2 (u-v) = \vec{0}$, então $(\alpha_1 +\alpha_2)u + (\alpha_1 -\alpha_2)v = \vec{0}$. Como $u$ e $v$ são linearmente independentes, então 
\[
\systeme[\alpha_1\alpha_2]{
\alpha_1 + \alpha_2 = 0,
\alpha_1 - \alpha_2 = 0
}
\]
o que implica que $\alpha_1 = \alpha_2 = 0$, pois este sistema homogêneo é possível e determinado. Logo, $B^\prime$ é linearmente independente e, portanto, uma base de $V$.

\item
\begin{itemize}
\item Dado $v \in V$, tem-se $v = 0 + v \in V + V$. Então $V \subseteq V + V$. Reciprocamente, dado $w \in V + V$, existem vetores $v_1 \in V$ e $v_2 \in V$ tais que $w = v_1 + v_2$. Como $V$ é um espaço vetorial, a soma de vetores de $V$ pertence a $V$ e, em particular, $w \in V$. Logo $V+V \subseteq V$, e portanto, $V = V + V$.
\item Por um argumento bastante similar ao do item anterior, demonstra-se que $V = 0 + V$. Além disso, como $0 \cap V = 0$, conclui-se que a soma é direta, isto é, $V = 0 \oplus V$.
\end{itemize}
\end{enumerate}

\end{document}
